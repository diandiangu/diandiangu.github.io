<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>论文与代码阅读笔记</title>
  
  
  <link href="https://diandiangu.github.io/atom.xml" rel="self"/>
  
  <link href="https://diandiangu.github.io/"/>
  <updated>2023-09-26T13:12:48.115Z</updated>
  <id>https://diandiangu.github.io/</id>
  
  <author>
    <name>GDD</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BPipe: 面向大语言模型训练的内存均衡的流水线并行</title>
    <link href="https://diandiangu.github.io/2023/09/25/BPipe/"/>
    <id>https://diandiangu.github.io/2023/09/25/BPipe/</id>
    <published>2023-09-25T10:52:07.000Z</published>
    <updated>2023-09-26T13:12:48.115Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BPIPE-Memory-Balanced-Pipeline-Parallelism-for-Training-Large-Language-Models"><a href="#BPIPE-Memory-Balanced-Pipeline-Parallelism-for-Training-Large-Language-Models" class="headerlink" title="BPIPE: Memory-Balanced Pipeline Parallelism for Training Large Language Models"></a>BPIPE: Memory-Balanced Pipeline Parallelism for Training Large Language Models</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>流水线并行为每个GPU分配一个单独的流水线阶段（stage），用于计算模型中的不同层。因此，每个流水线阶段都具有对其他阶段的数据依赖性，并导致计算暂停直到所需的数据到达，通常称为“流水线气泡”（bubble）。为了最大限度地减少气泡，1F1B（一个正向和一个反向）流水线将输入micro-batch，并交替处理正向计算和反向计算。为了使所有流水线阶段饱和，早期阶段比后期阶段保留更多的内存用于计算更多的前向微批。因此，流水线阶段之间存在内存不平衡，如果早期阶段内存不足，则执行模型失败，如图1所示。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/1.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：BPIPE如何处理内存不足问题的说明</div></center><br><p>作者观察到，后期阶段无法使用与早期阶段相同大小的GPU内存来预计算前向micro batch。因此，如果我们能够利用后期的空闲内存作为前期的额外内存，那么均衡的内存负载将缓解内存压力。此外，降低的内存压力使我们能够利用更多的内存来加速训练，避免冗余的重新计算、增加微批量大小或降低模型并行度。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="激活值平衡"><a href="#激活值平衡" class="headerlink" title="激活值平衡"></a>激活值平衡</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/2.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：8个微批次的4路1F1B流水线并行</div></center><br><p>在具有m个微批次的p路流水线并行中，用A(s)表示第s个阶段最多存储的激活值的内存量，W(s)为包括优化器状态在内的模型参数大小，则第s个阶段的内存使用量M(s) = A(s) + W(s)。假如没有一个阶段的模型层数相同，则W(s)为一个恒定的值W_0。μ(s)为第s个阶段最多能保存的微批次数量，A(s) = A0μ(s)，M(s) = W0 + A0μ(s)。</p><p>根据图2，1F1B流水线中的每一个阶段最多通信μ(s) = min(p − s, m)次。实际上，为了让所有流水线的阶段饱和，m远大于p。因此，μ(s) = p − s。</p><p>作者将阶段s和阶段p−s−1配对，每一对中的内存不均衡可以被写为：M(s) − M(p − s − 1) = A_0(p − 2s − 1)。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/3.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：激活平衡的图示</div></center><br><h3 id="传输调度"><a href="#传输调度" class="headerlink" title="传输调度"></a>传输调度</h3><p>为了达到内存均衡的目标，不同阶段之间数据的传输需要调度，调度算法决定何时传输或者加载什么数据。算法的核心思想是：（1）保证μ(s)不超过优化的最优值；（2）反向传播计算需要某个激活值，但是该激活值不在相应的GPU上的时候，去其配对的GPU上加载激活值；（3）如果某次激活值的加载会使得μ(s)超过优化的最优值，则算法会提前传输出去一个激活值，以此预留出足够的内存。</p><h3 id="“阶段对”的GPU分配"><a href="#“阶段对”的GPU分配" class="headerlink" title="“阶段对”的GPU分配"></a>“阶段对”的GPU分配</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/4.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：标准GPU分配和BPipe的配对分配</div></center><br><p>如图4所示，BPipe把配对的一对GPU分配在相邻的位置，这样可以减小激活值传输的时间，从而让激活值传输的时间被计算的时间完全overlap，从而减小BPipe内存优化对计算性能的影响。这样虽然会让训练时不同stage之间的通信变慢，但是内存平衡的通信时间比训练时不同stage数据依赖的通信量更大。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>作者通过分别的CUDA stream进行激活值的传输和加载，为激活值手动管理CUDA内存。作者在6台具有8个A100 GPU的服务器上进行了实验，实验中训练了GPT-3模型。</p><p>实验结果表明，BPipe按照Megatron-LM无法执行的配置进行GPT-3的训练，比Megatron-LM执行最快的配置快1.25倍。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/5.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：标准化后的训练吞吐量</div></center><br><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文提出了BPipe，通过在流水线阶段之间传输中间激活的激活平衡，BPIPE解决了流水线并行的内存不平衡问题。实验评估表明，BPipe通过执行更快的训练配置来加快大规模GPT-3模型的训练，并且如果没有BPipe，这样的配置无法运行起来。</p><hr><p><strong>原文作者：</strong>Taebum Kim, Hyoungjoo Kim, Gyeong-In Yu, Byung-Gon Chun<br><strong>原文链接：</strong><a href="https://openreview.net/pdf?id=HVKmLi1iR4">https://openreview.net/pdf?id=HVKmLi1iR4</a></p>]]></content>
    
    
    <summary type="html">流水线并行是在GPU集群中训练大型语言模型的关键技术。然而，流水线并行通常会导致内存不平衡问题，某些GPU的内存压力很大，而其他GPU则没有充分利用其内存。这种不平衡会影响训练性能。为了解决这种低效性，BPIPE在流水线并行中实现内存平衡。BPIPE在训练期间在GPU之间传输中间激活值，使所有GPU都能利用相当大小的内存。通过平衡内存利用率，BPIPE消除冗余重新计算或增加微批大小，从而提高GPT-3等大型语言模型的训练效率。</summary>
    
    
    
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="ICML" scheme="https://diandiangu.github.io/tags/ICML/"/>
    
    <category term="大模型" scheme="https://diandiangu.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Lucid：一个可扩展、可解释的实用型深度学习作业调度器</title>
    <link href="https://diandiangu.github.io/2023/04/25/Lucid/"/>
    <id>https://diandiangu.github.io/2023/04/25/Lucid/</id>
    <published>2023-04-25T03:12:39.000Z</published>
    <updated>2023-04-25T12:09:15.296Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lucid-A-Non-intrusive-Scalable-and-Interpretable-Scheduler-for-Deep-Learning-Training-Jobs"><a href="#Lucid-A-Non-intrusive-Scalable-and-Interpretable-Scheduler-for-Deep-Learning-Training-Jobs" class="headerlink" title="Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs"></a>Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs</h1><p>本文由南洋理工大学与上海人工智能实验室合作，发表于ASPLOS 2023。</p><p>现有的面向深度学习的调度器存在两个方面、五点问题：</p><ul><li>为了获得更好的系统性能，大多数现有的方法依赖于支持抢占的调度：<ul><li>有些调度器要求用户导入特定的库并修改代码来实现这些机制</li><li>有些调度器需要修改底层DL框架</li><li>很多调度器需要适应性地调整batch size和learning rate，影响训练的质量</li></ul></li><li>许多调度器在实际部署中也存在较大问题：<ul><li>可扩展性有限</li><li>决策不透明，难以调整</li></ul></li></ul><h2 id="机会"><a href="#机会" class="headerlink" title="机会"></a>机会</h2><ol><li>对于大部分情况来说，如果把两个job打包放到一个GPU上执行，互相的影响不大；如果考虑混合精度训练中的AMP，packing对训练速度的影响更小：<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"  src="/img/Lucid/1.png" width="55%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图1：Motivation</div></center></li></ol><br><ol start="2"><li><p>无需修改DL框架进行packing，训练速度也足够好了。</p></li><li><p>很多job都是重复提交的。可以通过profiling寻找和其相似的以前提交过的job。</p></li></ol><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>Lucid 遵循三个设计原则:</p><ul><li>非侵入性。整个调度工作流遵循无抢占方式，不需要用户操作和 DL 框架修改。</li><li>可伸缩性。该系统可以快速获得大量复杂工作负载的调度策略。</li><li>可解释性。所有的模块都是透明的，可以由集群运营商进行清晰的调整。</li></ul><p>此外，Lucid 还提高了资源利用率，并提供了及时的调试反馈。我们未来的工作旨在为更多的调度目标服务，比如公平性和服务水平保证。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Lucid/cover.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：System Overview</div></center><br><p>蓝色块为关键调度器模块，紫色块为用于性能增强和维护的两个系统优化器。工作流：分析作业，记录作业资源使用情况等信息，并把作业分类；绑定器判断是否以及如何打包作业；资源编排器根据概要分析和用户提供的特性，为每个作业分配一个优先级值，并选择要分配的作业。</p><h3 id="非侵入的Job-Profiler"><a href="#非侵入的Job-Profiler" class="headerlink" title="非侵入的Job Profiler"></a>非侵入的Job Profiler</h3><h3 id="Affine-作业对-绑定器"><a href="#Affine-作业对-绑定器" class="headerlink" title="Affine 作业对 绑定器"></a>Affine 作业对 绑定器</h3><h3 id="资源编排器"><a href="#资源编排器" class="headerlink" title="资源编排器"></a>资源编排器</h3><h3 id="可解释模型"><a href="#可解释模型" class="headerlink" title="可解释模型"></a>可解释模型</h3><h3 id="系统优化"><a href="#系统优化" class="headerlink" title="系统优化"></a>系统优化</h3><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>32GPU的tesebed，商汤的trace。可以降低JCT，提高scalability。</p><hr><p><strong>原文作者：</strong>Qinghao Hu, Meng Zhang, Peng Sun, Yonggang Wen, Tianwei Zhang<br><strong>原文链接：</strong><a href="https://dl.acm.org/doi/pdf/10.1145/3575693.3575705">https://dl.acm.org/doi/pdf/10.1145/3575693.3575705</a><br><strong>Code：</strong><a href="https://github.com/S-Lab-System-Group/Lucid">https://github.com/S-Lab-System-Group/Lucid</a><br><strong>参考链接：</strong>[1] 【论文笔记】Lucid论文阅读笔记 <a href="https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0lucid%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">https://tweakzx.github.io/p/论文笔记lucid论文阅读笔记/</a><br>[2] Lucid总结：一个可扩展和可解释的实用型深度学习任务调度器 <a href="https://zhuanlan.zhihu.com/p/612076435">https://zhuanlan.zhihu.com/p/612076435</a></p>]]></content>
    
    
    <summary type="html">现有的深度学习作业调度器存在可扩展性有限、决策过程不透明等等问题。本文引入了一个二维优化的探查器用于高效收集作业度量和及时调试作业反馈，利用惰性包装策略来规避干扰，并根据估计的作业优先级值和共享分数来协调资源，以实现高效的调度。Lucid实现了较好的模型性能维护和系统透明调整。可以减少JCT，并为实际部署提供了明确的系统解释和出色的可扩展性。</summary>
    
    
    
    
    <category term="集群调度" scheme="https://diandiangu.github.io/tags/%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="ASPLOS" scheme="https://diandiangu.github.io/tags/ASPLOS/"/>
    
  </entry>
  
  <entry>
    <title>Pheromone：服务器无感知计算平台中以数据中为心的函数编排</title>
    <link href="https://diandiangu.github.io/2022/11/13/Pheromone/"/>
    <id>https://diandiangu.github.io/2022/11/13/Pheromone/</id>
    <published>2022-11-13T15:19:48.000Z</published>
    <updated>2022-12-23T13:06:35.925Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Following-the-Data-Not-the-Function-Rethinking-Function-Orchestration-in-Serverless-Computing"><a href="#Following-the-Data-Not-the-Function-Rethinking-Function-Orchestration-in-Serverless-Computing" class="headerlink" title="Following the Data, Not the Function: Rethinking Function Orchestration in Serverless Computing"></a>Following the Data, Not the Function: Rethinking Function Orchestration in Serverless Computing</h1><p>本文是对即将发表于NSDI 2023会议的论文《Following the Data, Not the Function: Rethinking Function Orchestration in Serverless Computing》的解读。服务器无感知（serverless）应用程序通常由函数（function）工作流组成，交换数据需要通过触发多个短期函数以响应事件或更改状态。当前的服务器无感知计算平台通过高级调用依赖关系来协调和触发函数，但忽略了函数之间的底层数据交换。这种设计在编排复杂工作流时既不高效，也不易使用。本文作者认为函数编排应该遵循以数据为中心的方法。本文的Pheromone平台提供了一个数据桶抽象来保存函数生成的中间数据。开发人员可以使用数据触发器原语来控制每个函数的输出何时以及如何传递给工作流中的下一个函数。通过明确数据消耗并允许其触发函数和驱动工作流，可以轻松高效地支持复杂的函数交互。与商业平台和开源平台相比，Phromone将函数交互和数据交换的延迟降低了一个数量级，可扩展到大型工作流，并可轻松实现复杂应用程序。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>当前的服务器无感知计算平台使用“面向函数的方法”来编排和激活服务器无感知工作流的函数：每个函数被视为一个单独的单元，函数之间的交互在工作流中单独表达。这样的方式存在以下三个缺点：</p><ul><li><p>表达能力有限<br>当前的函数编排假定数据流与在工作流中调用函数的流相同，并且一个函数会立刻直接调用其他函数，将全部输出传递出去。 这个假设并不适用于很多应用程序。</p></li><li><p>可用性有限<br>当前的服务器无感知计算平台为函数之间的数据交换提供了各种选项：函数可以通过消息代理或共享存储同步或异步地交换数据，还可以处理来自各种来源的数据，例如嵌套函数调用、消息队列或其他云服务。因此，当前服务器无感知计算平台缺乏一种在函数之间交换数据的最佳方法，开发人员为了有效地在函数之间传递数据必须找到自己的方法，这些方法可能是动态的、琐碎的，这导致服务器无感知应用程序的开发和部署变得非常复杂。</p></li><li><p>适用性有限</p></li></ul><h2 id="以数据为中心的函数编排"><a href="#以数据为中心的函数编排" class="headerlink" title="以数据为中心的函数编排"></a>以数据为中心的函数编排</h2><p>本文作者注意到中间数据（即函数返回的结果）通常是短暂的且不可变的：在数据生成后会等待被消耗，然后过时。由于中间数据一旦生成就不会更新，因此使用它们来触发函数不会导致一致性问题。作者将数据消耗显式化，并使其能够触发目标函数；开发人员可以指定何时以及如何将中间数据传递给目标函数，并触发它们的激活，从而驱动整个工作流的执行。</p><p>为了方便以数据为中心的函数编排，作者设计了一个数据桶（bucket）抽象和一个触发器原语列表。 图1概述了如何触发函数。 服务器无感知应用程序创建一个或多个保存中间数据的数据桶。 开发人员可以用触发器配置每个桶，这些触发器指定数据何时以及如何调用目标函数并由它们使用。 执行工作流时，源函数直接将其结果发送到指定的桶。 每个桶检查配置的触发条件是否满足（例如，所需的数据已经完成并准备好被消耗）。 如果是，则桶自动触发目标函数并将所需数据传递给它们。 这个过程发生在所有桶中， 这些桶共同驱动整个工作流的执行。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/1.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：数据桶</div></center><br><p>作者为桶设计了各种触发原语，以指定如何触发函数。 函数间的交互模式大致可分为直接触发器原语、条件触发原语、动态触发器原语三类，如表1所示。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/table1.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表1：原语</div></center><p>Pheromone平台目前支持用C++编写的函数，并能够支持更多语言。 Pheromone还提供了一个Python客户端，开发人员可以通过该客户端编写函数交互。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/table2.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表2：API</div></center><h2 id="Pheromone系统设计"><a href="#Pheromone系统设计" class="headerlink" title="Pheromone系统设计"></a>Pheromone系统设计</h2><p>图2展示了Pheromone的系统架构。 每个工作节点接收来自本地调度器的指令，并运行多个执行器，这些执行器根据需要加载和执行用户函数代码。 工作节点还维护一个共享内存对象存储，该存储保存由函数生成的中间数据。 对象存储提供了一个数据桶接口，通过该接口，函数可以在一个节点内以及与其他节点有效地交换数据。Pheromone还会通过可持续的键值存储同步需要长期保存的数据，当新数据添加到对象存储时，本地调度器检查关联的桶触发器。 如果满足触发条件，本地调度器将在全局协调器的帮助下本地或远程调用目标函数，全局协调器在工作节点以外的单独机器上运行。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/2.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：Architecture</div></center><br><p>本文设计了一个两层的分布式调度方案来将Pheromone扩展到一个大的集群中。 具体地说，外部请求首先到达全局协调器，全局协调器将请求发送到工作节点上的本地调度器。 本地调度器尽可能通过调度使得后续函数在本地执行工作流，从而减少调用延迟。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/3.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：两层调度设计</div></center><br><p>Pheromone使用节点上的共享内存对象存储来维护数据对象，这样函数就可以通过指针直接访问数据。当源函数通过send_object()将数据对象放入bucket时，就标记它为“就绪”。 桶可以分布在其负责的协调器和多个工作节点上，其中每个工作节点跟踪本地数据状态，而协调器持有全局视图。桶状态同步只需要在负责的协调器和工作节点之间进行，因为不同工作节点的本地状态只跟踪他们的本地对象。</p><p>由于中间数据是短暂且不可变的，本文用它们的持久性来换取快速的数据共享和低资源占用。通过节点上的共享内存对象存储，Peromone可以通过将数据对象的指针传递到目标函数，实现本地函数之间的“零复制”数据共享，从而避免了大量的数据复制和序列化开销。</p><p>为了有效地将数据传递给远程函数，Pheromone可以在节点之间直接传递数据对象。 一个函数将数据对象的元数据（例如，locator）打包成发送到远程节点的函数请求。远程节点上的目标函数使用这样的元数据直接检索所需的数据对象。与使用远程存储进行跨节点数据共享相比，这种直接数据传输避免了不必要的数据复制，从而减少了网络和存储开销。虽然远程存储方法可以确保更好的数据持久性和一致性，但对于中间数据来说这些是不必要的。</p><p>Pheromone支持各种类型的容错。如果执行器宕机或工作节点上的数据对象丢失，Pheromone会重新启动失败的函数以复现丢失的数据并恢复中断的工作流。 如果在指定的超时时间内没有收到预期的输出，桶触发器就会重新执行源函数。</p><h2 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h2><p>本文评估了在没有任何有效负载的情况下函数的延迟。 本文考虑了三种常见的调用模式：顺序执行（例如，两个函数链）、并行调用（扇出）和聚合调用（扇入）。 作者在实验中改变了并行和聚合调用的函数的数量，以控制并行度。 图4显示了在这三种模式下调用的延迟。 每个延迟数据被进一步拆分为外部调用和内部调用的开销。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/4.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：三种workflow下函数的延迟</div></center><br><p>左图比较了在五个平台上测量的两个函数的函数链的调用延迟。 Pheromone明显优于其他方法。 其中Pheromone的基于共享内存的消息传递只带来不到20微秒的开销，将本地调用延迟降低到40微秒，比CloudBurst快10倍。 与其他平台相比，延迟的改善变得更加显著。 当调用远程函数时，Feromone和CloudBurst都需要网络传输，从而导致类似的内部调用延迟。 然而，由于CloudBurst需要在服务请求之前调度整个工作流的功能，CloudBurst对外部调用的开销比Pheromone高，从而导致整体性能更差。</p><p>中间和右图分别显示了并行调用和聚合调用下的调用延迟。 本文在每个worker上配置12个执行器，从而在运行16个函数时强制远程调用。 结果显示，Pheromone在各种情况下都具有最佳性能，并且在所有情况下只产生亚毫秒级的延迟，即使对于跨节点函数调用也是如此。 </p><hr><p><strong>原文作者：</strong>Minchen Yu, Tingjia Cao, Wei Wang, Ruichuan Chen<br><strong>原文链接：</strong><a href="https://arxiv.org/pdf/2109.13492.pdf">https://arxiv.org/pdf/2109.13492.pdf</a></p>]]></content>
    
    
    <summary type="html">本文的Pheromone平台提供了一个数据桶抽象来保存函数生成的中间数据。开发人员可以使用数据触发器原语来控制每个函数的输出何时以及如何传递给工作流中的下一个函数。通过明确数据消耗并允许其触发函数和驱动工作流，可以轻松高效地支持复杂的函数交互。与商业平台和开源平台相比，Phromone将函数交互和数据交换的延迟降低了一个数量级，可扩展到大型工作流，并可轻松实现复杂应用程序。</summary>
    
    
    
    
    <category term="NSDI" scheme="https://diandiangu.github.io/tags/NSDI/"/>
    
    <category term="serverless" scheme="https://diandiangu.github.io/tags/serverless/"/>
    
    <category term="函数编排" scheme="https://diandiangu.github.io/tags/%E5%87%BD%E6%95%B0%E7%BC%96%E6%8E%92/"/>
    
  </entry>
  
  <entry>
    <title>TOPOOPT：面向分布式训练作业的网络拓扑与并行策略协同优化</title>
    <link href="https://diandiangu.github.io/2022/11/04/TOPOOPT/"/>
    <id>https://diandiangu.github.io/2022/11/04/TOPOOPT/</id>
    <published>2022-11-04T07:52:35.000Z</published>
    <updated>2022-11-05T05:11:29.329Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TOPOOPT-Co-optimizing-Network-Topology-and-Parallelization-Strategy-for-Distributed-Training-Jobs"><a href="#TOPOOPT-Co-optimizing-Network-Topology-and-Parallelization-Strategy-for-Distributed-Training-Jobs" class="headerlink" title="TOPOOPT: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs"></a>TOPOOPT: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs</h1><p>如今的DNN训练系统建立在传统数据中心集群的基础上，交换机以多层胖树拓扑结构（Fat-tree）排列。胖树拓扑忽略流量的特征，服务器对之间的带宽和延迟一致。当工作负载不可预测且主要由短传输组成时，是一种理想的网络拓扑结构；但是胖树网络是分布式DNN训练工作负载的瓶颈：根据Meta的生产环境中，网络overhead占据训练时间的60%。由于DNN训练的通信流量特征是可预测的，可以根据这个特点协同优化网络拓扑和并行策略。</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><h3 id="DNN训练的特点"><a href="#DNN训练的特点" class="headerlink" title="DNN训练的特点"></a>DNN训练的特点</h3><p>数据并行时分布式训练中的一种常见做法，进行数据并行时不同工作节点之间通常以参数服务器、ring-AllReduce 、tree-AllReduce等拓扑结构进行通信。此外，单个GPU的内存无法容纳大型DNN模型，因此需要使用模型并行性将模型划分到多个GPU上。此外，由于跨GPU同步模型参数的成本越来越高，纯数据并行性对于大型训练作业来说很不理想，因此，人们使用数据和模型并行的混合来分布大型DNN，其中DNN的不同部分及其数据集在不同的GPU上并行处理。</p><p>每个训练迭代都包括两种主要类型的数据依赖关系。（1）在正向和反向传播期间计算的激活和梯度数据；（2）一旦处理了一批样本，就通过AllReduce步骤在加速器之间同步模型权重。根据并行化策略，这些数据依赖性可能产生本地内存访问或跨GPU流量。下文将类型（1）产生的网络流量称为MP传输，将类型（2）产生的网络流量称为AllReduce传输。</p><p>以深度学习推荐模型（DLRM）为例，如果模型有4个embedding表，embedding具有512列、10^7行，只采用数据并行的时候，会产生44GB的AllReduce数据传输，但是如果采用混合并行，最大的数据传输量只有4GB。因此，数据并行和模型并行结合对于大型模型来说十分必要。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：不同并行策略下的DLRM模型通信heatmap</div></center><br><h3 id="生产环境数据分析"><a href="#生产环境数据分析" class="headerlink" title="生产环境数据分析"></a>生产环境数据分析</h3><p>图2展示了Meta产业环境中的网络overhead，随着GPU数量的增加，网络很快占据了训练迭代时间的很大一部分。网络开销占Meta生产环境中DNN训练迭代时间的60%。这个瓶颈表明，现有的数据中心网络不足以满足新兴的DNN训练的工作负载。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/3.png" width="35%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：Meta产业环境中的网络overhead</div></center><br><p>图3展示了Meta生产GPU集群中运行的四个训练作业的服务器到服务器流量的heatmap。图中的所有heatmap都包含对角线上的正方形（深蓝色），这表示服务器之间的环形通信模式。但是MP传输（浅蓝色和绿色方块）取决于模型，因为MP传输取决于训练作业的并行化策略和设备放置。在整个训练过程中，不同的迭代之间，训练作业的流量模式是一样的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/4.png" width="85%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：Meta生产GPU集群中运行的四个训练作业的服务器到服务器流量的heatmap</div></center><br><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>本文提出的TOPOOPT是一种基于光学设备的新型系统，它联合优化DNN并行策略和拓扑结构，以加速训练作业。</p><p>TOPOOPT集群是一种可分片的直接连接结构，其中每台服务器都有d个接口连接到d个光交换机的核心层，如图4所示。光交换机使TOPOOPT能够将集群分片，产生每个训练作业的专用分区。每个分片的大小取决于作业申请的服务器数量。给定DNN训练工作和一组服务器，TOPOOPT首先离线在服务器之间找到最佳并行化策略和拓扑结构；然后，TOPOOPT重新配置光交换机以实现作业的目标拓扑。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/5.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：TOPOOPT的连接</div></center><br><p>此外，本文使用了Host-based forwarding等现有工作使用了的优化方法优化网络。</p><h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><p>单独寻找最优并行策略是一个NP-complete的问题，网络拓扑和路由使得问题更加困难。</p><p>TOPOOPT将搜索空间分成两个平面：Comp.×Comm. 和Comm.×Topo.。本文交替优化，在一个平面中迭代搜索，同时保持另一个平面的结果不变。图5说明了交替优化框架：TOPOOPT使用FlexFlow的MCMC（马尔可夫链蒙特卡罗）搜索算法，在考虑通信成本的同时，为给定网络拓扑找到最佳并行化策略。如果并行化策略提高了训练迭代时间，TOPOOPT将其提供给Comm.×Topo.平面，使用TOPOOPT的TOPOLOGY FINDER算法找到有效的网络拓扑和路由。然后将发现的拓扑反馈到Comp.×Comm.。这进一步优化了基于新拓扑的并行化策略和设备布局。该优化循环重复直到收敛或k次迭代之后，其中k是可配置的超参数。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/1.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：TOPOOPT算法流程</div></center><br><p>TOPOOPT有两个目标：（i）为AllReduce传输分配充足的带宽（因为大部分流量属于AllReduce）；（ii）确保MP传输的跳数较小。这两个目标是可以同时实现的，因为DNN训练流量具有一种独特的特性——AllReduce流量是可变的。</p><p>例如，图6展示了三种不同的AllReduce链接方式，它们对应的traffic heatmap分别如图7所示。蓝色的AllReduce通信的位置是可以变化的，但是MP传输的heatmap保持不变。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/6.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图6：Ring-AllReduce permutations</div></center><br><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/7.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图7：DLRM traffic heatmaps</div></center><br>流量易变性意味着，如果一组服务器按特定顺序连接，只需重新排列服务器的标签，就可以得到另一种顺序，以相同的延迟完成AllReduce操作，同时可能为MP传输提供较小的跳数。TOPOOPT不是只选择一个AllReduce顺序，而是为每个AllReducer组找到多个排列，并与它们对应的子拓扑重叠。这样，TOPOOPT可以有效地服务AllReduce流量，同时减少MP传输的跳数。<p>为了减少所有可能排列的搜索空间，本文设计了TotientPerms算法，以基于群论找到所有规则环（连续服务器的索引之间的距离相等的环）的生成法则。然后，SelectPermutations算法可以减少集群的直径，以利于MP传输。</p><h2 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h2><p>本文在6种不同的DNN模型上进行了模拟实验比较，其中DLRM和NCF模型的MP传输量更大。与成本相似的胖树拓扑相比，TOPOOPT将DNN训练时间减少了3.4倍。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/8.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图8：在128台服务器上的效果</div></center><br><hr><p><strong>原文作者：</strong>Weiyang Wang, Moein Khazraee, Zhizhen Zhong, Manya Ghobadi, Zhihao Jia, Dheevatsa Mudigere, Ying Zhang, Anthony Kewitsch<br><strong>原文链接：</strong><a href="https://arxiv.org/abs/2202.00433">https://arxiv.org/abs/2202.00433</a></p>]]></content>
    
    
    <summary type="html">本文发表于NSDI2023。在本文中，作者对从阿里巴巴 PAI 6000多个GPU的生产MLaaS集群收集的两个月工作负载跟踪进行了表征研究，解释了集群调度面临的挑战，描述了目前的解决方案，并公开了数据集。</summary>
    
    
    
    
    <category term="NSDI" scheme="https://diandiangu.github.io/tags/NSDI/"/>
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="网络" scheme="https://diandiangu.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>大规模异构GPU集群中的作业负载分析与调度</title>
    <link href="https://diandiangu.github.io/2022/02/20/MLaaS/"/>
    <id>https://diandiangu.github.io/2022/02/20/MLaaS/</id>
    <published>2022-02-20T09:56:46.000Z</published>
    <updated>2022-11-05T05:11:16.622Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MLaaS-in-the-Wild-Workload-Analysis-and-Scheduling-in-Large-Scale-Heterogeneous-GPU-Clusters"><a href="#MLaaS-in-the-Wild-Workload-Analysis-and-Scheduling-in-Large-Scale-Heterogeneous-GPU-Clusters" class="headerlink" title="MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters"></a>MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters</h1><p>随着机器学习技术的持续进步和海量数据集的使用，科技公司正在部署大型ML-as-a-Service（MLaaS）云，通常带有异构GPU，以提供大量ML应用程序。然而，在异构GPU集群中运行不同的ML工作负载带来了许多挑战。为了适应快速增长的ML工作负载计算需求，阿里云提供了人工智能机器学习平台（PAI），这是一个多功能的MLaaS平台，使开发人员能够高效、灵活、简化地使用ML技术。其架构如图1所示。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/MLaaS/1.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：PAI的架构</div></center><br>在本文中，作者对从阿里巴巴 PAI 6000多个GPU的生产MLaaS集群收集的两个月工作负载跟踪进行了表征研究，解释了集群调度面临的挑战，包括GPU利用率低、排队延迟长、难以调度的任务需要高端GPU、具有苛刻的调度要求、异构机器之间的负载不平衡，以及CPU上的潜在瓶颈。作者描述了目前的解决方案，并呼吁进一步调查仍有待解决的挑战。作者公开了相关数据集。<h2 id="数据集概览"><a href="#数据集概览" class="headerlink" title="数据集概览"></a>数据集概览</h2><p>PAI的数据集中，大多数作业要求多个GPU。数据集包括不同级别（如作业、任务和实例）的工作负载的到达时间、完成时间、资源请求和GPU、CPU、GPU内存和主内存中的使用情况。数据集还提供了机器级信息，包括守护进程代理收集的硬件规格、资源利用率等。</p><p>具有以下特点：</p><p><strong>严重倾斜的实例分布。</strong>PAI跟踪包含超过1300名用户提交的120万个任务的750万个实例。用户运行的任务实例的分布是严重倾斜的。大约77%的任务实例由前5%的用户提交，每个用户运行的实例超过17.5k个，而后50%的用户每个用户运行的实例不到180个。</p><p><strong>组调度（gang scheduling）占主流。</strong>在所有任务实例中，大约85%的作业要求组调度，其中20%必须在100多个GPU上调度，有些甚至要求超过1000个。加在一起，带有组调度实例的作业占GPU总需求的79%。这类任务的普遍性使得很难实现高利用率。</p><p><strong>GPU局部性。</strong>除了组调度，作业可能会请求在同一台机器上的多个GPU上运行其所有实例，这一要求称为GPU的局部性。虽然这种要求通常会导致长时间的调度延迟，但它允许在单个节点（例如NVLink和NVSwitch）内使用高速GPU到GPU互连，从而显著加快分布式训练。在PAI中，对某些训练任务实施GPU局部性可以有超过10倍的加速比。</p><p><strong>GPU共享。</strong>PAI支持GPU共享，允许多个任务实例以低成本时间共享一个GPU。使用此功能，用户可以在（0，1）区间内指定GPU请求，并使用部分GPU运行其任务实例。GPU共享可以节约大量的GPU资源。</p><p>在时间维度上，作业提交频率具有日间模式，工作日提交的作业略多于周末，其中除了白天，午夜也是任务提交的高峰时间；实例运行时间的变化范围很大（第90百分位的运行时间为4.5小时，比Philly的25小时短），作业排队的延迟时间分布很不均匀。</p><p>在空间维度上，资源请求的分布呈重尾分布（一小部分的实例请求了大量的资源），GPU使用率很低但CPU使用率较高。</p><h2 id="GPU机器利用率"><a href="#GPU机器利用率" class="headerlink" title="GPU机器利用率"></a>GPU机器利用率</h2><h3 id="计算资源利用率"><a href="#计算资源利用率" class="headerlink" title="计算资源利用率"></a>计算资源利用率</h3><p>如图2所示，与内存（包括主存和GPU内存）相比，GPU和CPU的利用率更高。在8-GPU机器（左上角）中，GPU（红色虚线）和CPU（蓝色实线）的平均P90利用率分别达到82%和77%。在2-GPU机器中（右上角），P90 GPU的利用率仍然很高（平均77%），而P90 CPU的利用率在平均值上下降到42%，这是因为CPU与GPU的比率很大（每个GPU配有32或48个CPU）。在这两种类型的机器中，主存和GPU内存的P90利用率几乎一直低于60%，这说明数据集中的作业的内存密集度较低。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/MLaaS/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：资源利用率的CDF</div></center><br>与其他资源相比，在GPU上测量的利用率变化更大。如图2所示，P90 GPU利用率的分布范围很广，在40%到100%之间变化；GPU上的尾部和中值利用率之间的差异也比其他资源上的差异更大。这种巨大的差异是由于在ML工作负载中发现的突发性GPU使用模式。PAI使用的调度器中packing的优先级高于负载平衡也是到这支个现象的原因。<h3 id="网络与I-O利用率"><a href="#网络与I-O利用率" class="headerlink" title="网络与I/O利用率"></a>网络与I/O利用率</h3><p>网络：P100（或Misc）、T4和V100机器中P95网络输入速率分别为带宽的54%、48%和34%。<br>I/O：I/O等待上的CPU时间比usr比内核模式下的CPU时间小三个数量级，这说明CPU主要忙于处理数据，而不是等待I/O完成。</p><h2 id="集群管理的机遇"><a href="#集群管理的机遇" class="headerlink" title="集群管理的机遇"></a>集群管理的机遇</h2><p><strong>GPU共享：</strong>GPU独享导致GPU利用率很低。为了避免这一问题，PAI支持GPU共享，且GPU共享在及群众补货导致激烈的资源竞争。<br><strong>重复性任务的可预测执行时间：</strong>已知任务执行时间是作出更好调度结果的关键。现有调度器根据训练进度（例如，操作次数、损失曲线和目标精度）和任务速度来预测执行时间。获取此类信息需要特定的框架支持（例如TensorFlow和PyTorch），这在实际集群中并不总是可能的，因为用户可能运行各种标准或版本的框架，而且他们提交的任务可能不会执行迭代训练）。然而，作者发现大多数任务都是重复性的：大约65%的任务至少重复运行5次。作者通过任务定期运行、平均实例运行时间稳定的特征来预测人物运行时间。经过实验评估，PAI使用的预测方法的准确率对于做出调度决定来说是足够的。</p><h2 id="集群管理的挑战"><a href="#集群管理的挑战" class="headerlink" title="集群管理的挑战"></a>集群管理的挑战</h2><p>挑战包括：机器规格和实例请求之间不匹配、老旧GPU机器中的任务过多、高端机器中负载不均衡、CPU成为瓶颈等</p><hr><p><strong>原文作者：</strong>Qizhen Weng, Wencong Xiao, Yinghao Yu, Wei Wang, Cheng Wang, Jian He, Yong Li, Liping Zhang, Wei Lin, Yu Ding<br><strong>原文链接：</strong><a href="https://qzweng.github.io/files/2022NSDI-MLaaS-Weng.pdf">https://qzweng.github.io/files/2022NSDI-MLaaS-Weng.pdf</a><br><strong>公开数据集：</strong><a href="https://github.com/alibaba/clusterdata">https://github.com/alibaba/clusterdata</a></p>]]></content>
    
    
    <summary type="html">本文发表于NSDI2022。在本文中，作者对从阿里巴巴 PAI 6000多个GPU的生产MLaaS集群收集的两个月工作负载跟踪进行了表征研究，解释了集群调度面临的挑战，描述了目前的解决方案，并公开了数据集。</summary>
    
    
    
    
    <category term="NSDI" scheme="https://diandiangu.github.io/tags/NSDI/"/>
    
    <category term="集群调度" scheme="https://diandiangu.github.io/tags/%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6/"/>
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>每天一个没用的代码小技巧</title>
    <link href="https://diandiangu.github.io/2022/01/10/techniques/"/>
    <id>https://diandiangu.github.io/2022/01/10/techniques/</id>
    <published>2022-01-10T04:18:31.000Z</published>
    <updated>2023-07-12T08:38:54.798Z</updated>
    
    <content type="html"><![CDATA[<h2 id="服务器使用"><a href="#服务器使用" class="headerlink" title="服务器使用"></a>服务器使用</h2><ol><li><p>可以使用使NVMeNon-Volatile Memory express）SSD加速disk上文件的读写。<br>首先查看是否有NVMe SSD</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /dev/nvme*</span><br></pre></td></tr></table></figure><p>然后将NVMe mount到需要的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.ext4 /dev/nvme1n1 <span class="comment"># 格式化硬盘 </span></span><br><span class="line">sudo mkdir /mnt/data1 </span><br><span class="line">sudo mount /dev/nvme1n1 /mnt/data1 </span><br></pre></td></tr></table></figure></li><li><p>RAM disk: 不真正地使用disk读写，而是把文件放进内存。[<a href="https://www.linuxbabe.com/command-line/create-ramdisk-linux">参考链接</a>]</p></li><li><p>如果使用ssh登录服务器遇到问题，可以通过<code>ssh -vv</code>查看verbose log。</p></li><li><p><code>parallel-ssh</code> 和<code>parallel-scp</code>命令可以快速登陆多个服务器/向多个服务器同时传输文件。<br>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install pssh </span><br></pre></td></tr></table></figure><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parallel-ssh -i -t 0 -h hostfile <span class="string">&quot;hostname&quot;</span> </span><br><span class="line">parallel-scp –h hostfile -l &lt;user&gt; src dst </span><br></pre></td></tr></table></figure><p>如果出现</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Usage: parallel-scp [OPTIONS] <span class="built_in">local</span> remote</span><br><span class="line">parallel-scp: error: Hosts not specified.</span><br></pre></td></tr></table></figure><p>那就改用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parallel-scp --hosts=hostfile --user=&lt;user&gt; src dst</span><br></pre></td></tr></table></figure></li><li><p>如果服务器不能连到外网，而自己有ClashX代理，可以配置反向代理。<br>在本地：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -NfR 7820:127.0.0.1:7890 username@server_ip</span><br></pre></td></tr></table></figure><p>注意第一个端口号需要和代理的端口号不同，而第二个端口号7890是ClashX的默认端口号，根据实际情况替换。<br>然后正常ssh到服务器。<br>打开ClashX，command+C复制，然后粘贴到服务器的命令行里，把端口号都改成刚才设置的第一个端口号（7820），然后就可以上网了。另外可以注意一下需要按照<a href="https://www.zhang-hb.com/2022/02/02/%E7%94%A8ssh%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E8%AE%A9%E4%B8%8D%E8%81%94%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%BD%91/">这篇博客</a> 设置<code>/etc/ssh/sshd_config</code>。</p></li><li><p>使用NFS共享存储<br>服务器端安装NFS服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nfs-kernel-server</span><br></pre></td></tr></table></figure><p>服务器端在<code>/etc/exports</code>里添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/dir *(rw,sync,no_subtree_check,no_root_squash)</span><br></pre></td></tr></table></figure><p>然后命令行执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nfs-kernel-server restart</span><br></pre></td></tr></table></figure><p>客户端命令行运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nfs-common --force-yes</span><br><span class="line">sudo mount -t nfs <span class="variable">$1</span>:/dir /dir</span><br></pre></td></tr></table></figure><p>不用的时候需要在服务器端stop service，并在客户端unmount。</p></li><li><p>查看端口使用情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -tunlp | grep &lt;port_number&gt;</span><br></pre></td></tr></table></figure></li><li><p>不要轻易使用<code>conda install python==x.x</code>更换已有环境的python版本，以前安装的pip包会被覆盖。</p></li><li><p>查看物理CPU个数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep <span class="string">&quot;physical id&quot;</span>| sort| uniq| wc -l</span><br></pre></td></tr></table></figure><p>查看逻辑CPU个数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep <span class="string">&quot;processor&quot;</span>| wc -l</span><br></pre></td></tr></table></figure><p>查看每个CPU的核数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep <span class="string">&quot;cpu cores&quot;</span>| uniq</span><br></pre></td></tr></table></figure><p>查看CPU型号：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c</span><br></pre></td></tr></table></figure><p>查看Linux内核：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -a</span><br></pre></td></tr></table></figure><p>查看内存使用情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">free -m</span><br></pre></td></tr></table></figure></li></ol><h2 id="GPU-使用"><a href="#GPU-使用" class="headerlink" title="GPU 使用"></a>GPU 使用</h2><ol><li><p>Persistent mode 可以加速GPU上的计算和内存操作 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-smi -pm 1</span><br></pre></td></tr></table></figure></li><li><p><code>dmesg</code>命令可以查看硬件上的历史报错信息。</p></li><li><p>检测不到GPU device：可能是fabric manager版本不匹配的问题。<br>检测方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nvidia-fabricmanager status</span><br></pre></td></tr></table></figure><p>如果检测出版本不匹配的问题，则重新安装fabric manager：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/nvidia-fabricmanager-450_450.80.02-1_amd64.deb </span><br><span class="line">sudo apt install ./nvidia-fabricmanager-450_450.80.02-1_amd64.deb </span><br><span class="line">sudo systemctl <span class="built_in">enable</span> nvidia-fabricmanager </span><br><span class="line">sudo systemctl restart nvidia-fabricmanager </span><br></pre></td></tr></table></figure><p>在有些机器上，fabric manager会自动更新其版本，需要停止自动跟新更新才能保证版本始终可以匹配。[<a href="https://github.com/Azure/azhpc-images/blob/master/ubuntu/common/disable_auto_upgrade.sh">停止自动更新的脚本</a>]</p></li><li><p>改变GPU的power上限：<code>nvidia-smi -i %s -pl [power_upper_limit]</code></p></li><li><p>改变GPU的frequency：<code>&quot;nvidia-smi -i %s -ac %s,%s&quot; % (gid, mem, gra)</code><br>查看GPU的frequency：<code>nvidia-smi --query-gpu=clocks.applications.graphics,clocks.applications.mem -i %s --format=csv,noheader,nounits</code><br>恢复默认值：<code>nvidia-smi -rac</code><br>锁住固定的frequency：<code>nvidia-smi -lgc $frequency -i $i</code></p></li><li><p>AWS EC2服务器暂时不能直接使用 GPU，可能是因为NVIDIA更新了公钥，但是AWS系统更新的时候没暂时更新驱动，因此需要自己装一遍驱动才能使用GPU<br>修改环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BASE_URL&#x3D;https:&#x2F;&#x2F;us.download.nvidia.com&#x2F;tesla</span><br><span class="line">DRIVER_VERSION&#x3D;450.80.02 # 510.47.03 for A100</span><br></pre></td></tr></table></figure><p>然后执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -fSsl -O <span class="variable">$BASE_URL</span>/<span class="variable">$DRIVER_VERSION</span>/NVIDIA-Linux-x86_64-<span class="variable">$DRIVER_VERSION</span>.run</span><br><span class="line">sudo sh NVIDIA-Linux-x86_64-<span class="variable">$DRIVER_VERSION</span>.run</span><br></pre></td></tr></table></figure><p>或者修复apt-get之后通过<a href="https://blog.csdn.net/qq_28256407/article/details/115548675">https://blog.csdn.net/qq_28256407/article/details/115548675</a> 安装</p></li></ol><h2 id="Deep-Learning-开发-主PyTorch-NCCL分布式训练"><a href="#Deep-Learning-开发-主PyTorch-NCCL分布式训练" class="headerlink" title="Deep Learning 开发 (主PyTorch+NCCL分布式训练)"></a>Deep Learning 开发 (主PyTorch+NCCL分布式训练)</h2><ol><li><p>使用pip install安装的PyTorch使用静态链接的NCCL。如果想要更新NCCL版本，则可以使用一下命令（适用于A100 GPU）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b v2.11.4-1 https://github.com/NVIDIA/nccl /nccl-2.11.4 </span><br><span class="line"><span class="built_in">cd</span> /nccl-2.11.4 </span><br><span class="line">make -j src.build TRACE=1 NVCC_GENCODE=<span class="string">&quot;-gencode=arch=compute_80,code=sm_80&quot;</span> </span><br><span class="line">make install </span><br></pre></td></tr></table></figure></li><li><p>使用Pytorch DDP后清理GPU内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.synchronize()</span><br><span class="line"><span class="keyword">del</span> ddp_model</span><br><span class="line"><span class="keyword">del</span> dataset</span><br><span class="line"><span class="keyword">del</span> optimizer</span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line">dist.destroy_process_group() </span><br></pre></td></tr></table></figure><p>但是目前版本的PyTorch中，<code>dist.destroy_process_group(group=subgroup)</code>并不能真正清理掉subgroup使用的内存，且这个内存泄漏无法被常用的内存泄漏检测方法检测到。<br>（<a href="https://gist.github.com/MInner/8968b3b120c95d3f50b8a22a74bf66bc">内存泄漏检测方法1</a>，<a href="https://discuss.pytorch.org/t/how-to-debug-causes-of-gpu-memory-leaks/6741/3">内存泄漏检测方法2</a>）</p></li><li><p>使用<code>NCCL_DEBUG=TRACE</code>需要使用<code>TRACE=1</code>这一flag重新编译NCCL，然后运行<code>make install</code>。<br>还需要保证NCCL的安装路径在LD_LIBRARY_PATH里。打出来的trace前面有时间戳，单位是ms。 </p></li><li><p>如果使用NCCL时遇到了奇怪的bug（如segmentation fault），可以检查环境变量的设置是否正确。即使通过<code>NCCL_DEBUG=INFO</code>打印出来的log一切正常，也需要再设置一次环境变量。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> NCCL_IB_PCI_RELAXED_ORDERING=1</span><br><span class="line"><span class="built_in">export</span> NCCL_SOCKET_IFNAME=eth0</span><br><span class="line"><span class="built_in">export</span> CUDA_DEVICE_ORDER=PCI_BUS_ID</span><br><span class="line"><span class="built_in">export</span> NCCL_NET_GDR_LEVEL=5</span><br><span class="line"><span class="built_in">export</span> LD_PRELOAD=/opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so <span class="comment"># path to ibnccl-net.so</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/opt/hpcx/nccl_rdma_sharp_plugin/lib:<span class="variable">$LD_LIBRARY_PATH</span> <span class="comment"># the dir where ibnccl-net.so is in</span></span><br></pre></td></tr></table></figure><p>其中，如果不设置NCCL_IB_PCI_RELAXED_ORDERING、CUDA_DEVICE_ORDER和NCCL_NET_GDR_LEVEL，（单机）多卡之间的通信可能会非常慢。</p></li><li><p>如果使用NCCL时能建立起来进程组，但是在通信时报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: NCCL error <span class="keyword">in</span>: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:957, internal error, NCCL version 21.1.4</span><br><span class="line">ncclInternalError: Internal check failed. This is either a bug <span class="keyword">in</span> NCCL or due to memory corruption</span><br></pre></td></tr></table></figure><p>检查master IP、port的设置是否正确，如果是多机环境，检查不同机器上环境变量的设置是否一致、NCCL版本是否一致。</p><p>如果在卡住一段时间之后报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;test.py&quot;</span>, line 112, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    ddp_model = DDP(model, device_ids=[args.rank% torch.cuda.device_count()], output_device=args.rank% torch.cuda.device_count())</span><br><span class="line">  File <span class="string">&quot;/data/gdd/software/miniconda3/envs/env/lib/python3.7/site-packages/torch/nn/parallel/distributed.py&quot;</span>, line 578, <span class="keyword">in</span> __init__</span><br><span class="line">    dist._verify_model_across_ranks(self.process_group, parameters)</span><br><span class="line">RuntimeError: NCCL error <span class="keyword">in</span>: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:957, unhandled system error, NCCL version 21.0.3</span><br><span class="line">ncclSystemError: System call (socket, malloc, munmap, etc) failed.</span><br></pre></td></tr></table></figure><p>则需要设置NCCL_SOCKET_IFNAME环境变量！</p></li><li><p>如果使用NCCL时无法建立进程组，并报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ProcessGroupNCCL.cpp:957, invalid usage, NCCL version 21.0.3</span><br><span class="line">ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams <span class="keyword">in</span> a group, etc).</span><br></pre></td></tr></table></figure><p>检查同一台机器上不同进程（rank）的<code>CUDA_VISIBLE_DEVICES</code>环境变量是否不同。</p></li><li><p>使用PyTorch训练时报错</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, xxxx])</span><br></pre></td></tr></table></figure><p>检查<code>DataLoader</code>是否设置了<code>drop_last=True</code>，并检查batch size是否大于1（如果模型中有batch Normalization层）。</p></li><li><p><a href="https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile">PyTorch Profiler</a>，分析性能的好帮手</p></li><li><p>如果PyTorch dataloader报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Exception ignored <span class="keyword">in</span>: &lt;<span class="keyword">function</span> _MultiProcessingDataLoaderIter.__del__ at 0x7f733cbf6f70&gt;</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;</span>, line 1510, <span class="keyword">in</span> __del__</span><br><span class="line">    self._shutdown_workers()</span><br><span class="line">  File <span class="string">&quot;/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;</span>, line 1493, <span class="keyword">in</span> _shutdown_workers</span><br><span class="line">    <span class="keyword">if</span> w.is_alive():</span><br><span class="line">  File <span class="string">&quot;/opt/conda/lib/python3.8/multiprocessing/process.py&quot;</span>, line 160, <span class="keyword">in</span> is_alive</span><br><span class="line">    assert self._parent_pid == os.getpid(), <span class="string">&#x27;can only test a child process&#x27;</span></span><br><span class="line">AssertionError: can only <span class="built_in">test</span> a child process</span><br><span class="line">terminate called after throwing an instance of <span class="string">&#x27;c10::CUDAError&#x27;</span></span><br><span class="line">  what():  CUDA error: initialization error</span><br><span class="line">CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.</span><br><span class="line">For debugging consider passing CUDA_LAUNCH_BLOCKING=1.</span><br></pre></td></tr></table></figure><p>可能是数据集corrupt了。</p></li></ol><h2 id="Graceful-Python-Projects"><a href="#Graceful-Python-Projects" class="headerlink" title="Graceful Python Projects"></a>Graceful Python Projects</h2><ol><li>使用logging包是，如果在一个文件里add handler了，就无需在这个包所import的其他py文件里再次add handler了，只需：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOG = logging.getLogger(__name__)</span><br><span class="line">LOG.setLevel(logging.INFO)</span><br></pre></td></tr></table></figure></li><li>删除<code>__pycache__</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find . -name <span class="string">&quot;*.pyc&quot;</span> -<span class="built_in">type</span> f -<span class="built_in">print</span> -<span class="built_in">exec</span> rm -rf &#123;&#125; \;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h2 id="Docker使用"><a href="#Docker使用" class="headerlink" title="Docker使用"></a>Docker使用</h2><ol><li><p>从远程镜像启动容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it -d --name=&lt;name&gt; --privileged --net=host --ipc=host --gpus=all -v /opt:/opt2 repo/tag</span><br></pre></td></tr></table></figure></li><li><p>进入容器的bash命令行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker <span class="built_in">exec</span> -it &lt;name&gt; bash</span><br></pre></td></tr></table></figure></li><li><p>make and run</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker build -t &lt;name&gt; -f Dockerfile .</span><br></pre></td></tr></table></figure></li><li><p>将当前容器的镜像push到远程repo<br>首先确保自己登录了docker，没有登录的话先登录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u &lt;用户名&gt; -p &lt;密码&gt;</span><br></pre></td></tr></table></figure></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo docker commit &lt;container_id&gt; &lt;repo&gt;/&lt;tag&gt;</span><br><span class="line">sudo docker push &lt;repo&gt;/&lt;tag&gt;</span><br></pre></td></tr></table></figure><ol start="5"><li><p>删除image之前需要先删除容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo docker ps</span><br><span class="line">sudo docker rm &lt;container&gt;</span><br><span class="line">sudo docker image ls</span><br><span class="line">sudo docker rmi &lt;image&gt;</span><br></pre></td></tr></table></figure></li><li><p>重启docker服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure></li><li><p>如果无法pull镜像，但能正常联网，可能是docker代理配置的问题，参考：<br><a href="https://forums.docker.com/t/docker-pull-results-in-request-canceled-while-waiting-for-connection-client-timeout-exceeded-while-awaiting-headers/73064/26">https://forums.docker.com/t/docker-pull-results-in-request-canceled-while-waiting-for-connection-client-timeout-exceeded-while-awaiting-headers/73064/26</a><br>两种常见的解决方案：</p></li></ol><ul><li>设置系统DNS：<a href="https://docs.docker.com/config/daemon/systemd/#httphttps-proxy">https://docs.docker.com/config/daemon/systemd/#httphttps-proxy</a></li><li>Docker代理配置：<a href="https://datawookie.dev/blog/2018/10/dns-on-ubuntu/">https://datawookie.dev/blog/2018/10/dns-on-ubuntu/</a></li></ul><p>可以通过<code>docker info</code>查看是否有默认的代理配置。如果编辑了<code>/etc/systemd/system/docker.service.d/</code>并重启Docker不能覆盖这些代理的配置，可能是因为这些配置被写在了<code>/lib/systemd/system/docker.service</code>文件里，需要去编辑<code>/lib/systemd/system/docker.service</code>。</p><h2 id="Kubernetes使用"><a href="#Kubernetes使用" class="headerlink" title="Kubernetes使用"></a>Kubernetes使用</h2><ol><li><p>初始化：<code>sudo kubeadm init --pod-network-cidr=192.168.0.0/16</code></p></li><li><p>让master节点也成为worker参与到调度中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line">kubectl get nodes -o wide <span class="comment"># 查看节点</span></span><br></pre></td></tr></table></figure><p>其中第一条命令会去掉所有node的node-role.kubernetes.io/master相关taint（有些taint后面会带有NoSchedule的标记，具体有哪些taint可以通过<code>kubectl describe &lt;node-name&gt;</code>查看）。</p></li><li><p>查看资源详细描述：<code>kubectl describe</code><br>查看提交任务的状态：<code>kubectl get pods</code><br>查看具体pod的log：<code>kubectl logs &lt;pod-name&gt;</code></p></li><li><p>删除pod之前需要先删除deployment/stateful set/daemonset等，不然pod被删除后会一直被重启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deployment -n &lt;namespace&gt;</span><br><span class="line">kubectl delete deployment &lt;deployment&gt; -n &lt;namespace&gt;</span><br><span class="line">kubectl delete pod &lt;pod-name&gt; -n &lt;namespace&gt;</span><br></pre></td></tr></table></figure></li><li><p>如果<code>/</code>目录下已经用了超过80%的空间，会有disk pressure的问题，需要清理磁盘。</p></li><li><p>Kubernetes inter-pod networking test：<a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/hardway/test-networking">https://projectcalico.docs.tigera.io/getting-started/kubernetes/hardway/test-networking</a></p></li><li><p>使用Kubernetes时让DL job内使用InfiniBand通信：<a href="https://github.com/gudiandian/k8s-rdma-sriov-dev-plugin">https://github.com/gudiandian/k8s-rdma-sriov-dev-plugin</a></p></li></ol><h2 id="Latex"><a href="#Latex" class="headerlink" title="Latex"></a>Latex</h2><ol><li><p>去掉行号：<code>\documentclass</code>里面去掉<code>review</code></p></li><li><p>表格中用对号和叉，使其更美观。对号：\ding{52}；叉：\ding{56}</p></li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol><li>用命令行下载Google drive上的大文件（例如数据集）：</li></ol><p>进入链接：<a href="https://developers.google.com/oauthplayground/">https://developers.google.com/oauthplayground/</a></p><p>在 Select the Scope 栏复制 <a href="https://www.googleapis.com/auth/drive.readonly">https://www.googleapis.com/auth/drive.readonly</a></p><p>点击Authorize APIs 然后 Exchange authorization code for tokens</p><p>复制 Access token</p><p>在命令行运行 <code>curl -H &quot;Authorization: Bearer ACCESS_TOKEN&quot; https://www.googleapis.com/drive/v3/files/FILE_ID?alt=media -o FILE_NAME</code></p><p>参考链接：<a href="https://stackoverflow.com/questions/65312867/how-to-download-large-file-from-google-drive-from-terminal-gdown-doesnt-work">https://stackoverflow.com/questions/65312867/how-to-download-large-file-from-google-drive-from-terminal-gdown-doesnt-work</a></p><ol start="2"><li>统计项目代码行数: <a href="https://github.com/AlDanial/cloc">cloc</a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cloc [options] &lt;file(s)/dir(s)/git <span class="built_in">hash</span>(es)&gt;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">一些小技巧、小方法的合集。每天一个小技巧，效率翻倍！</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>碳排放与大规模神经网络训练</title>
    <link href="https://diandiangu.github.io/2021/11/14/CO2e/"/>
    <id>https://diandiangu.github.io/2021/11/14/CO2e/</id>
    <published>2021-11-14T09:02:49.000Z</published>
    <updated>2022-11-05T05:12:33.662Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Carbon-Emissions-and-Large-Neural-Network-Training"><a href="#Carbon-Emissions-and-Large-Neural-Network-Training" class="headerlink" title="Carbon Emissions and Large Neural Network Training"></a>Carbon Emissions and Large Neural Network Training</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文调查并分享这些最新和大型NLP模型的能耗和CO<sub>2</sub>e（CO<sub>2</sub> equivalance emissions）估算值。通过描述硬件和数据中心上的实际搜索过程，本文还将Evolved Transformer的神经架构搜索（neural architecture search，NAS）的CO<sub>2</sub>e早期估计值减少了88倍。</p><p>本文对CO<sub>2</sub>e的调查揭示了对DNN生命周期、运行它们的数据中心和硬件、能量组合的变化以及准确评估CO<sub>2</sub>e的困难的不同于以往认知之处和误解。值得注意的是，本文测评了运行中的计算机和数据中心的CO<sub>2</sub>e，而不是制造或者回收它们CO<sub>2</sub>e。</p><p>为了让ML社区更容易理解训练的实际影响以及如何减少训练的影响，本文呼吁：</p><ol><li>本文呼吁更多的研究者测量能源的消耗并公开相关数据；</li><li>除了准确率和相关的指标以外，能源使用效率应该是发表计算密集型模型的ML研究的评估标准，因为最可持续的能源是你不使用的能源；</li><li>即使我们可以在云数据中心将CO<sub>2</sub>e降至零，减少训练时间也很重要，这既因为“时间就是金钱”，也因为更低的训练成本可以让更多人参与进来。因此，本文也支持让更多的研究人员公布训练时使用的加速器的数量及其训练时间，以鼓励在降低训练成本方面取得进展。本文作者相信，这些新的激励措施可能会导致一个良性循环，让ML从业者竞相增加收入。</li></ol><h2 id="NLP模型的资源消耗和碳足迹"><a href="#NLP模型的资源消耗和碳足迹" class="headerlink" title="NLP模型的资源消耗和碳足迹"></a>NLP模型的资源消耗和碳足迹</h2><p>跑一个机器学习所需要的电量是一个关于算法、实现算法的程序、运行程序所需处理器的数量、这些处理器的速度和耗电量、数据中心传递能量和冷却处理器的效率、能源供应组合（新能源、燃气、煤等）。</p><p>很多公司在DNN模型inference上小号的能量比训练要多。比如NVIDIA估计80-90%的机器学习workload是在进行inference。Amazon Web 90%的云端ML服务是inference。因此很多公司专门为inference设计了加速器。本文主要关注training（training的耗能更独立且方便于探究），但是inference中的耗能也是不可忽略的。</p><p>表1为CO<sub>2</sub>e分解，本文之后将进一步解释这些改进的业务原理，展示了提高ML效率的交叉激励。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/CO2e/1.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表1：基于能耗测量，估计CO<sub>2</sub>e for Transformer and Evolved Transformer for P100 and TPU v2</div></center><br><p>图1说明了每一步的收益：CO<sub>2</sub>e的总体改善为57倍。这一巨大的收益说明了为什么选择DNN模型、处理器、数据中心和地理位置对于改善二氧化碳排放至关重要。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/CO2e/2.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Improvement in CO<sub>2</sub>e over Transformer (Big) on P100 GPUs in an average US datacenter versus Evolved Transformer (Medium) on TPU v2s in the Google Iowa datacenter</div></center><br><h3 id="算法-程序的提升"><a href="#算法-程序的提升" class="headerlink" title="算法/程序的提升"></a>算法/程序的提升</h3><p>和Transformer比起来，使用了NAS的Evolved Transformer训练更快，准确率略高。<br><em>商业原则：</em>更快的训练不仅可以节省ML科研工作者的时间，还可以为他们的组织节省资金，减少CO<sub>2</sub>e排放。</p><h3 id="处理器的提升"><a href="#处理器的提升" class="headerlink" title="处理器的提升"></a>处理器的提升</h3><p>TPU比P100 GPU训练更快，且消耗的能源更少。<br><em>商业原则：</em>在过去十几年中，深度学习的范围和规模大幅增加，这为构建定制硬件创造了机会，该硬件可根据DNN模型训练和inference所涉及的计算类型进行定制。在过去的七年中，谷歌没有像许多其他组织那样使用GPU，而是为DNN设计、构建和部署了四代定制张量处理单元（TPU）硬件，以加速模型训练和inference。为了获得更好的投资回报，云计算公司的目标实际上是提高性价比，而不仅仅是性能。这里的成本是指总拥有成本（TCO），包括年度运营成本，如耗电量和计算机、冷却、配电和建筑的资本支出摊销。功耗与TCO几乎完全线性相关，因此提高性能/TCO也有助于性能/瓦特，节省资金并减少CO<sub>2</sub>e排放。</p><h3 id="数据中心的提升"><a href="#数据中心的提升" class="headerlink" title="数据中心的提升"></a>数据中心的提升</h3><p>衡量数据中心效率的一个指标是它超出了数据中心内计算设备的直接动力的能量开销。由于服务器利用率等其他因素的影响，云数据中心的能效大约是典型企业数据中心的2倍。更广泛地说，由于云数据中心比典型的企业的数据中心的能效要高得多，数据中心能源使用量并没有激增。最近发表在《科学》杂志上的一篇论文发现，尽管计算能力在同一时期增加了550%，但全球数据中心能耗与2010年相比仅增加了6%。<br><em>商业原则：</em>云计算公司致力于节能数据中心，因为它可以节省资金并降低排放。“能量就是金钱。”</p><h3 id="能源供应组合的提升"><a href="#能源供应组合的提升" class="headerlink" title="能源供应组合的提升"></a>能源供应组合的提升</h3><p>根据美国平均能源供应组合，能源的总碳强度为0.429kg CO<sub>2</sub>e/千瓦时。在按照谷歌的24/7无碳能源框架匹配谷歌的清洁能源购买后，作者在爱荷华州数据中心运行Evolved Transformer的净CO<sub>2</sub>e下降到0.080，比原来的好5.4倍。<br><em>商业原则：</em>与通过光纤以光子形式发送信息相比，远距离传输电力成本更高，效率更低。云计算允许像谷歌这样的公司拥有一个全球数据中心组合，其中许多位于电网更清洁的地方（如芬兰）或公司可以直接购买清洁能源的地方（如爱荷华州）。2020年，谷歌在其能源战略中宣布了一个新目标：到2030年，它的目标是让所有谷歌数据中心和办公室全天候使用无碳能源。对于这个24/7无碳能源核算，作者从每小时消耗量中扣除在同一地理位置的本地电网和同一小时购买的所有清洁能源，从而得出净CO<sub>2</sub>e/KWh值。由于爱荷华州夜间风力强劲，谷歌的风力发电组合将爱荷华州数据中心2020年12月的CO<sub>2</sub>e/KWh总平均值降低了6倍，从当地电网的0.478千克降至净平均值0.080千克。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>$$<br>KWh = (Hours\ to\ train) \times (Number\ of\ Processers) \times (Average \  Power \ per \ Processor) \times PUE \div 1000<br>$$<br>$$<br>tCO_2e = KWh \times kg \  CO_2e \  per \  KWh  \div 1000<br>$$</p><h2 id="5种大型NLP模型的C能源使用和CO2e排放"><a href="#5种大型NLP模型的C能源使用和CO2e排放" class="headerlink" title="5种大型NLP模型的C能源使用和CO2e排放"></a>5种大型NLP模型的C能源使用和CO<sub>2</sub>e排放</h2><ul><li>T5: 86MWh, 47tCO<sub>2</sub>e。</li><li>Meena：232MWh, 96tCO<sub>2</sub>e。</li><li>GShard：24MWh, 4.3tCO<sub>2</sub>e。</li><li>Switch Transformer：179MWh, 59tCO<sub>2</sub>e。</li><li>GPT-3：552MWh, 1287tCO<sub>2</sub>e。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/CO2e/3.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表2：CO<sub>2</sub>e for NLP models</div></center><br><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/CO2e/4.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：训练五个大型DNN模型所需要的加速器年、能量消耗和CO<sub>2</sub>e排放量</div></center><br><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>在本节中，作者讨论因训练NLP模型而产生的与碳排放相关的其他因素。</p><h3 id="估计NAS的能量消耗"><a href="#估计NAS的能量消耗" class="headerlink" title="估计NAS的能量消耗"></a>估计NAS的能量消耗</h3><p>使用NAS后，模型参数减少37%，训练收敛所需时间减少25%，CO<sub>2</sub>e排放量减少48.5t。</p><h3 id="整个训练过程中消耗的能源远远不止最后一次训练所消耗的能源"><a href="#整个训练过程中消耗的能源远远不止最后一次训练所消耗的能源" class="headerlink" title="整个训练过程中消耗的能源远远不止最后一次训练所消耗的能源"></a>整个训练过程中消耗的能源远远不止最后一次训练所消耗的能源</h3><p>作者认为ML从业者可以研究整个训练模型的生命周期，制定经验法则，根据最终训练成本估算总体碳足迹。</p><h3 id="测量比推测更有趣"><a href="#测量比推测更有趣" class="headerlink" title="测量比推测更有趣"></a>测量比推测更有趣</h3><p>虽然对碳排放量做出估计相对容易，但我们应更多地关注已经进行的实际实验，而不是假设的案例研究。无论是处理器还是DNN，实际测量情况和理论情况都有很大不同。</p><h3 id="标准的ML算法技术可以提高能源效率"><a href="#标准的ML算法技术可以提高能源效率" class="headerlink" title="标准的ML算法技术可以提高能源效率"></a>标准的ML算法技术可以提高能源效率</h3><p>有许多算法技术可以提高机器学习模型的能量效率。有些技术可以用较少的总体计算达到相同的精度。另外有的技术可以使用一个已经训练过的大型模型作为起点，生成一个重量更轻、计算效率更高、精度几乎相同的模型。这些技术都有助于减少计算成本，从而减少模型的能源和碳排放。这些技术包括知识蒸馏、剪枝量化高效编程、迁移学习与微调、稀疏激活的专家风格混合的模型等。</p><p><font size=3.5 color=Red><strong>即使在同一个组织里，使用哪一个数据中心很重要</strong></font><br>作者发现在何时何地训练DNN非常重要。此外，该方案可能是ML从业者减少CO<sub>2</sub>e排放的最简单途径。</p><h3 id="许多人都可以访问能源优化的数据中心"><a href="#许多人都可以访问能源优化的数据中心" class="headerlink" title="许多人都可以访问能源优化的数据中心"></a>许多人都可以访问能源优化的数据中心</h3><p>自2010年以来，云计算的使用量不断增加，数据中心的能源强度每年降低20%。访问能源优化、低成本的云数据中心并不局限于少数公司的员工；世界各地的人们可以使用阿里云、亚马逊网络服务、谷歌云平台和Microsoft Azure等服务租用服务器。此外，阿里巴巴、亚马逊和谷歌还通过云服务提供对其自定义DNN处理器的访问。自2010年以来，公共云的业务年增长率高达50%，这表明公共云非常受欢迎。许多人认为，云在成本和能源方面的效率意味着它是所有数据中心的最终未来。</p><h3 id="训练成本也很重要"><a href="#训练成本也很重要" class="headerlink" title="训练成本也很重要"></a>训练成本也很重要</h3><p>尽管许多人可以使用这些相对高效的计算资源，云公司在未来可能会大幅减少碳足迹，但降低训练的经济成本仍然很重要。显然，节约资金对每个人都很重要，但NLP模型的昂贵的训练成本也使许多研究人员无法实现这种研究。与环境问题一样，获取最先进模型的不平等性是另一个强有力的动机，它可以激励研究者开发高效节能的ML模型，且该模型的计算能力要与其他模型一样强。</p><h3 id="训练大型NLP模型与其他活动相比如何？"><a href="#训练大型NLP模型与其他活动相比如何？" class="headerlink" title="训练大型NLP模型与其他活动相比如何？"></a>训练大型NLP模型与其他活动相比如何？</h3><p>作者将训练NLP模型的能量消耗与航空飞行、比特币做了比较。<br>训练这四个大型NLP模型并不是谷歌能源消耗的重要部分。</p><h3 id="NLP模型带来的效益值得所需的能量消耗吗？"><a href="#NLP模型带来的效益值得所需的能量消耗吗？" class="headerlink" title="NLP模型带来的效益值得所需的能量消耗吗？"></a>NLP模型带来的效益值得所需的能量消耗吗？</h3><p>MoE模型越大，BLEU分数的获益越大。<br>训练模型的分摊人均CO<sub>2</sub>e影响小于发送一条短信的CO<sub>2</sub>e。</p><hr><p><strong>原文作者：</strong>David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean<br><strong>原文链接：</strong><a href="https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf">https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf</a></p>]]></content>
    
    
    <summary type="html">机器学习的计算需求近年来迅速增长，导致成本很高。估算能源成本有助于衡量其对环境的影响并找到更环保的战略。本文计算了几个大型模型的能源使用和碳足迹，并寻找提高能源效率和二氧化碳当量排放（CO&lt;sub&gt;2&lt;/sub&gt;e）的机会。</summary>
    
    
    
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="绿色机器学习" scheme="https://diandiangu.github.io/tags/%E7%BB%BF%E8%89%B2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="能耗" scheme="https://diandiangu.github.io/tags/%E8%83%BD%E8%80%97/"/>
    
  </entry>
  
  <entry>
    <title>Blink：面向分布式机器学习的快速、通用的通信原语</title>
    <link href="https://diandiangu.github.io/2021/09/19/Blink/"/>
    <id>https://diandiangu.github.io/2021/09/19/Blink/</id>
    <published>2021-09-19T12:53:53.000Z</published>
    <updated>2021-11-14T09:06:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Blink-Fast-and-Generic-Collectives-for-Distributed-ML"><a href="#Blink-Fast-and-Generic-Collectives-for-Distributed-ML" class="headerlink" title="Blink: Fast and Generic Collectives for Distributed ML"></a>Blink: Fast and Generic Collectives for Distributed ML</h1><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>目前用于深度学习的数据集规模越来越大，在训练效果得到提升的同时，与之同时，漫长的训练时间却成为了另一个让人头疼的问题。基于多个 GPU 的数据并行训练（Data-Parallel Training）大大缓解了大规模深度学习模型训练耗时长的问题，但是数据并行需要同步模型参数，跨 GPU 的参数同步在大规模训练时产生了较大开销，这一开销成为了限制训练速度的瓶颈。</p><p>为了缓解通信上的这一瓶颈，这几年在软件和硬件上都有很大的提升。在硬件方面，NVIDIA提供了多种先进的服务器，例如DGX-1、DGX-2等。在这些服务器中，有GPU用于计算的P100或者V100，有用于通信的PCIe和NCLink。在软件层面，很多公司提供了自己的通信库，例如NVIDIA的NCCL，Facebook的gloo，还有Uber的Horovod。</p><p>作者发现，即使使用了当前很先进的的硬件，通信开销仍然很高。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Blink/1.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：不同服务器上通信开销占训练时间的比例</div></center><br><p>而且，随着GPU的计算能力的增强，通信开销的带来的影响会越来越大。因此，作者认为，在现有的硬件条件下，我们需要可以更快进行通信的protocol。</p><p>为了减少通信开销，作者设计了Blink这一集合通信库。</p><h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><ol><li>Different server configuration。图2中有两种DGX-1服务器link的拓扑和带宽不同。 目前的通信库通常会忽略这个问题，从而导致很多的link不能被高效利用。<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"  src="/img/Blink/2.png" width="60%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图2：两种DGX-1服务器的拓扑结构</div></center></li></ol><br><ol start="2"><li><p>Link heterogeneoty。SOTA通信库的通信原语都是基于“环”结构的。那么什么是基于“环”的原语呢？以broadcast为例，左边有四块GPU，他们通过环形的拓扑连接起来。（broadcast的过程）而一个server中会有多种不同种类的link。（PPT图有pcie和nvlink）如果使用这种基于“环”结构的通信原语的话，他只能利用同种类型的link。如果使用了不同类型的link，通信过程中的bottleneck在于这个ring中bandwidth最小的那一个link（例如，pcie bottleneck）。</p></li><li><p>Fragmentation in multi-tenant clusters。 在多租户的集群中，由调度器把GPU分配给不同的job。有时候一个机器里会有多个job，这几个job之间有可能会共用PCIe或者某种link，从而互相影响。<br>在从微软收集到的的40,000多个job的trace中，可以看到，即使用户一般都会申请试用2的n次方块GPU，但是，在每个server中分配的GPU数量可能是3，5，6，7这种数字。比如一个使用8块GPU的job如果被分在2个server上，可能是2+6或者3+5这种数字的组合。为什么会有这种碎片化的问题呢？因为集群的调度器不是topology-aware的。目前还没有通用的、高效进行migration的方法，为了避免过长的queuing delay，这些DNN的任务必须面对碎片化的问题。在这种情况下，会有一些GPU的分配中，同一个job使用的GPU没法直接形成“环”（ppt图）。在NCCL中如果遇到这种情况，会使放弃使用贷款更高的NVLink，而去使用PCIe。</p></li></ol><h3 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Blink/3.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：Blink Workflow</div></center><br>（1）给定深度学习任务，一旦安排并分配了一组 GPU，Blink 就能够探测机器的拓扑结构，并通过分配的 GPU 推断互连拓扑结构。<p>（2）给定拓扑，将集体通信操作建模为有向图上的流，并计算生成树的最大分数填充（TreeGen）此步骤输出一组生成树和对应于应通过它们发送多少数据的权重。</p><p>（3）CodeGen 解析生成树并生成 CUDA 代码。生成的代码与 NCCL 提供的 API 匹配，并打包到共享库 libblink.so 中。</p><p>（4）设置 LD_PRELOAD 标志，以便在调用主程序时动态加载 Blink 实现。这确保了现有程序可以在没有任何修改的情况下运行。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="TreeGen"><a href="#TreeGen" class="headerlink" title="TreeGen"></a>TreeGen</h3><h4 id="packing-max-spanning-trees"><a href="#packing-max-spanning-trees" class="headerlink" title="packing max. spanning trees"></a>packing max. spanning trees</h4><p>将从分配的资源推断出的拓扑建模为一个有向图，其中每个 GPU 是顶点 V，每个链路（NVLink 或 PCIe）标记为有向边缘 E。每个有向边缘还具有带宽比例容量。通过找到图中的有向生成树或树状图的最大填充可以达到最优速率。每个树状图 Ti 从根涡生成，沿着有向链接扩展到其它涡点。通过确定满足能力限制的最大权重树状图，可以解决在广播中确定最佳时间表的问题。</p><p>由上式，给定一个图 G，顶点为 V，边为 E，根为 r，生成树为 T1, T2, T3, … Ti，希望找到权重 w_i，使得通过任何边的权重树的总和不超过特定边的容量。</p><p>如何生成这些树？MWU算法+一个新提出的算法[5]，接近线性时间。</p><p>这样做的问题是，最优解种可能包含了太多的树。而且每个树中需要传输的数据量太小了，不能充分利用每一个link的带宽。</p><p>为了解决这个问题，新增一个限制条件：对于每一个link，要么不用这个link，要么把这个link的带宽用满。（181-&gt;6）</p><h4 id="推广到多对多的情况"><a href="#推广到多对多的情况" class="headerlink" title="推广到多对多的情况"></a>推广到多对多的情况</h4><p>为了处理多对多的操作，作者利用了这样一个事实：在这些机器中发现的所有链接本质上都是双向的，因此可以创建一个无向图，用链接的一个方向运行多对一原语，并相应地在另一个方向运行一对多原语。allreduce = reduce+broadcast<br>(这种使用两个无向树的策略也与 AllReduce 操作所需的消息数下限相匹配。</p><p>AllReduce 的进程需要发送的最小消息数是 2x|(N-1)/N|。N 个顶点上的生成树包含 N-1 条边，并考虑了两个方向上的树（一个用于 Reduce，一个用于 Broadcast），类似的，本文方法同样有 2x(N-1）条消息。)</p><h3 id="CodeGen-implementation"><a href="#CodeGen-implementation" class="headerlink" title="CodeGen(implementation)"></a>CodeGen(implementation)</h3><p>通过pipeline减小数据传输的延迟。并通过前几个iter的profile选取合适的chunck size。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>在实验评估环节，作者将Blink与目前最先进的、基于环的集体通信协议（如 NCCL2）相比较。实验结果表明，Blink 可以实现高达8倍的模型同步速度，并将端到端DNN模型训练时间减少 40%。</p><hr><p><strong>原文作者：</strong>Guanhua Wang, Shivaram Venkataraman, Amar Phanishayee, Jorgen Thelin, Nikhil R. Devanur, Ion Stoica<br><strong>原文链接：</strong><a href="https://arxiv.org/pdf/1910.04940.pdf">https://arxiv.org/pdf/1910.04940.pdf</a><br><strong>参考文献：</strong><br>[1] MLSys提前看 | 机器学习的分布式优化方法, <a href="https://zhuanlan.zhihu.com/p/108289809">https://zhuanlan.zhihu.com/p/108289809</a></p>]]></content>
    
    
    <summary type="html">本文发表于MLSys 2020。目前用于深度学习的数据集规模越来越大，基于多个 GPU 的数据并行训练（Data-Parallel Training）大大缓解了大规模深度学习模型训练耗时长的问题，但是跨 GPU 的参数同步在大规模训练时产生了较大开销。本文提出了 Blink——一个通过包装生成树动态生成最佳通信原语的集合通信库。在处理硬件生成中的拓扑异质性问题或集群调度程序中的分配问题时，Blink 能够动态生成给定拓扑的最佳通信原语。</summary>
    
    
    
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="MLSys" scheme="https://diandiangu.github.io/tags/MLSys/"/>
    
  </entry>
  
  <entry>
    <title>Pollux：面向深度学习有效吞吐量优化的协同适应集群调度</title>
    <link href="https://diandiangu.github.io/2021/05/14/Pollux/"/>
    <id>https://diandiangu.github.io/2021/05/14/Pollux/</id>
    <published>2021-05-14T11:15:28.000Z</published>
    <updated>2023-02-21T09:48:26.652Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pollux-Co-adaptive-Cluster-Scheduling-for-Goodput-Optimized-Deep-Learning"><a href="#Pollux-Co-adaptive-Cluster-Scheduling-for-Goodput-Optimized-Deep-Learning" class="headerlink" title="Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning"></a>Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning</h1><p>现有的深度学习调度器程序要求提交作业的用户指定各种超参数，如果设置得不正确，这些参数会大大降低作业性能和资源效率。在这些参数中，batch size和learning rate非常依赖于其资源分配（GPU数量），这使得在共享资源环境中很难预先确定这些参数。此外，深度学习任务能否有效利用的资源分配不仅取决于所训练模型的结构，而且还取决于batch size和learning rate。资源数量、batch size、learning rate之间的这种相互依赖性，使得用户必须同时考虑到这些因素才能配置其作业以实现高效执行和资源利用。</p><p>从根本上讲，一个有效配置的深度学习任务需要在两种期望之间取得trade off：（1）系统吞吐量，即每个单位时间处理的训练实例的数量，以及（2）统计效率，即每个处理的训练实例所取得的进展量。如图1a所示，可以通过增加batch size来增加系统吞吐量。较大的batch size可以提高更多资源的利用率（例如，较大数量的GPU）。但是，当batch size增加时，必须重新调整learning rate。否则，统计效率将会降低，使得总训练时间不会更短，从而浪费额外分配的GPU。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/1.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Batch size、resource scalability、stage of training之间的trade-off（ResNet18 on CIFAR-10，为每一个batch size调整了learning rate）</div></center><br><p>本文提出了一种混合资源调度器Pollux，可以在为共享集群中的每个深度学习任务调整batch size和learning rate的同时协同适应地分配资源。这篇工作由CMU和磐腾科技合作完成，获得OSDI’21会议的best paper。</p><p>然而，调度器或许不应该任意调整用户指定的batch size，尤其是在调整超参数的任务中，这一行为是不被允许的。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>深度训练任务有两种衡量指标：System Throughput和Statistical Efficiency。<br>系统吞吐量（System Throughput）是每单位时间处理的训练样本数。当一个DL任务分布在多个节点上时，其系统吞吐量由几个因素决定，包括（1）分配给该作业的资源的分配和放置，（2）分布式执行和同步的方法，以及（3）SGD算法使用的batch size大小。由于深度学习任务的scalibility是次线性的（sublinear），使用太多数量的GPU不一定会提升系统吞吐量。通常我们可以使用更大的batch size来提升系统的吞吐量。</p><p>然而，我们也不能一味地增加batch size，因为在增大batch size的同时，统计效率（Statistical Efficiency）会下降。统计效率是每处理一单位数据所取得的训练进度，当统计效率下降时，训练所需要的epoch数量会增加。最佳的训练效果需要在系统吞吐量和统计效率之间权衡。</p><h2 id="DL训练的Goodput"><a href="#DL训练的Goodput" class="headerlink" title="DL训练的Goodput"></a>DL训练的Goodput</h2><p>本文定义了goodput：在第t个iteration的goodput为系统吞吐量和统计效率的乘积。</p><p>$$<br>GOODPUT_t(a,m) = THROUGHPUT(a,m) * EFFICIENCY_t(m)<br>$$</p><p>其中，$a \in R^N$是allocation vector，$a_n$是从节点n分配的GPU数量，$m$是batch size。</p><p>一个DL任务在使用batch size m ≥ m0时的统计效率是相对于用m0来说，每用m个训练样本所获得的训练进度。<br>This quantity can be framed in terms of the gradient noise scale φt .<br>统计效率可以被计算为：</p><p>$$<br>EFFICIENCY_t(m) = r_tm_0/m = (\phi_t+m_0)/(\phi_t+m)<br>$$</p><h2 id="Pollux架构与设计"><a href="#Pollux架构与设计" class="headerlink" title="Pollux架构与设计"></a>Pollux架构与设计</h2><ol><li>PolluxAgent对每个任务所给定的GPU资源分配结果，找到能够最大化有效吞吐量的batch size和梯度累积步数。Pollux使用用户指定的learning rate调整规则来根据batch size调整learning rate。</li><li>PolluxSched根据集群中每个任务的有效吞吐量，为每一个任务找到合适的GPU资源分配方案。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/Pollux.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：Pollux的Co-adaptive scheduling架构</div></center><br><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>作者在16个节点、没个节点有四块NVIDIA T4 GPU的集群上进行了实验评估，使用的job trace包含微软公开的Philly集群job trace中最繁忙的8个小时内的160个job。Pollux可以将训练所需时间缩短37-50%。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/2.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表1：实验结果总结</div></center><br><p>通过图3中（A）处可以看出，在集群中的资源较紧张的时候，Pollux会使用更大的batch size以提升系统吞吐量，此时统计效率较低。在此之后，通过（B）可以看出，Pollux会给任务减少GPU资源的分配，使用较小的batch size，从而提升统计效率。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/3.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：ImageNet训练任务中Pollux与baseline的实验结果比较</div></center><br><p>除了集群调度之外，Pollux还可以应用于云端弹性训练、超参数搜索等场景。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Pollux通过优化Goodput，协同调整集群和任务的参数。Pollux 基于对有用作业完成进度提出更有意义的衡量指标，来提升深度学习作业竞争资源的公平性，并揭示了在云环境下降低深度学习成本具有新机会。</p><hr><h1 id="代码阅读笔记"><a href="#代码阅读笔记" class="headerlink" title="代码阅读笔记"></a>代码阅读笔记</h1><h2 id="Pollux-AdaptDL-部署-该Section内容主要来自我的好伙伴xxt"><a href="#Pollux-AdaptDL-部署-该Section内容主要来自我的好伙伴xxt" class="headerlink" title="Pollux / AdaptDL 部署 (该Section内容主要来自我的好伙伴xxt)"></a>Pollux / AdaptDL 部署 (该Section内容主要来自我的好伙伴xxt)</h2><blockquote><p>文档：<a href="https://adaptdl.readthedocs.io/en/latest/index.html">https://adaptdl.readthedocs.io/en/latest/index.html</a></p></blockquote><h3 id="Step-0：Deploying-MicroK8s-for-AdaptDL"><a href="#Step-0：Deploying-MicroK8s-for-AdaptDL" class="headerlink" title="Step 0：Deploying MicroK8s for AdaptDL"></a>Step 0：Deploying MicroK8s for AdaptDL</h3><p>首先安装snap，之后用snap安装microk8s，这里大概率需要用到代理</p><pre><code>$ sudo snap set system proxy.http=&quot;代理地址&quot;$ sudo snap set system proxy.https=&quot;代理地址&quot;$ sudo snap install microk8s --classic --channel=1.18/stable</code></pre><p>microk8s装好后，依次按文档执行都没有什么问题</p><pre><code>$ sudo microk8s enable dns$ sudo microk8s enable gpu storage$ sudo microk8s enable helm$ sudo microk8s helm init --stable-repo-url=https://charts.helm.sh/stable$ sudo helm repo add stable https://charts.helm.sh/stable   </code></pre><p>接下来运行</p><pre><code>$ sudo microk8s.kubectl get nodesNAME       STATUS   ROLES    AGE   VERSIONxinjin-1   Ready    &lt;none&gt;   20d   v1.18.20     </code></pre><p>之后按文档可以为了省去sudo，以及用kubectl代替microk8s.kubectl，需要执行如下命令。</p><pre><code>$ sudo usermod -a -G microk8s $USER$ mkdir -p $HOME/.kube$ sudo microk8s kubectl config view --raw &gt; $HOME/.kube/config$ sudo chown -f -R $USER ~/.kube    </code></pre><p>这些由于服务器上的$HOME路径改动过，需执行以下命令后生效</p><pre><code>sudo dpkg-reconfigure apparmorsudo rm -f /etc/apparmor.d/cache/* /var/cache/apparmor/snap.*sudo reboot</code></pre><h3 id="Step-1：Installing-the-AdaptDL-Scheduler"><a href="#Step-1：Installing-the-AdaptDL-Scheduler" class="headerlink" title="Step 1：Installing the AdaptDL Scheduler"></a>Step 1：Installing the AdaptDL Scheduler</h3><p>按文档运行</p><pre><code>$ helm install adaptdl adaptdl-sched --repo https://github.com/petuum/adaptdl/raw/helm-repo --namespace adaptdl --create-namespace --set docker-registry.enabled=true</code></pre><p>随后执行</p><pre><code>$ kubectl get pods -n adaptdl</code></pre><p>发现STATUS并非Running，而是ContainerCreating，查看日志发现是缺少一些镜像，可以手动拉取，可以参考:<a href="https://www.cnbugs.com/post-3276.html">https://www.cnbugs.com/post-3276.html</a>   </p><p>补全pause和tiller镜像后，问题自动解决，状态变为STATUS。</p><pre><code>$ kubectl get pods -n adaptdlNAME                                     READY   STATUS    RESTARTS   AGEadaptdl-adaptdl-sched-67c7bf5b59-597bj   3/3     Running   71         15dadaptdl-registry-864758d854-757s6        1/1     Running   23         15dadaptdl-validator-66ff95d99c-wsnkd       1/1     Running   23         15d</code></pre><h3 id="Step-2-Submitting-a-Simple-Job"><a href="#Step-2-Submitting-a-Simple-Job" class="headerlink" title="Step 2: Submitting a Simple Job"></a>Step 2: Submitting a Simple Job</h3><p>接下来尝试用命令行工具运行adaptdl job。</p><p>首先，运行<code>python3 -m pip install adaptdl-cli</code>，接着创建文件夹hello_world并将以下内容复制到<code>hello_world/hello_world.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> adaptdl.env</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello, world!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(adaptdl.env.share_path(), <span class="string">&quot;foo.txt&quot;</span>), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&quot;Hello, world!&quot;</span>)</span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>接着，创建<code>hello_world/Dockerfile</code>，内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FROM python:3.7-slim</span><br><span class="line">RUN python3 -m pip install adaptdl</span><br><span class="line"></span><br><span class="line">COPY hello_world.py &#x2F;root&#x2F;hello_world.py</span><br><span class="line"></span><br><span class="line">ENV PYTHONUNBUFFERED&#x3D;true</span><br></pre></td></tr></table></figure><p>因为在提交步骤中出现</p><pre><code>python3: error while loading shared libraries: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory</code></pre><p>经排查发现是docker中缺少动态链接地址的原因，在Dockerfile中加上了<code>ENV LD_LIBRARY_PATH=:/usr/local/lib</code>   后恢复正常.</p><p>最后，拷贝以下内容至<code>hello_world/adaptdljob.yaml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: adaptdl.petuum.com&#x2F;v1</span><br><span class="line">kind: AdaptDLJob</span><br><span class="line">metadata:</span><br><span class="line">  generateName: hello-world-</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: main</span><br><span class="line">        command:</span><br><span class="line">        - python3</span><br><span class="line">        - &#x2F;root&#x2F;hello_world.py</span><br></pre></td></tr></table></figure><p>运行<code>adaptdl submit hello_world</code>后，任务提交成功，可以通过文档后续的命令查看输出并观察执行情况。</p><p>如果提交后出现<code>ModuleNotFoundError</code>，可以尝试运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install mitmproxy==6.0.2</span><br><span class="line">pip install markupsafe==2.0.1</span><br></pre></td></tr></table></figure><h2 id="Pollux-Testbed-Benchmark复现"><a href="#Pollux-Testbed-Benchmark复现" class="headerlink" title="Pollux Testbed Benchmark复现"></a>Pollux Testbed Benchmark复现</h2><p>因为我没有用aws的机器，所以复现的过程极其坎坷，在此记录一下。</p><h3 id="环境配置与调度器的安装"><a href="#环境配置与调度器的安装" class="headerlink" title="环境配置与调度器的安装"></a>环境配置与调度器的安装</h3><p>最开始以为按照<a href="https://adaptdl.readthedocs.io/en/latest/index.html">文档</a>配好调度器就可以了，但是调度的时候遇到PVC claim的问题。</p><p>然后发现了<code>benchmark</code>目录下的<code>main.tf</code>文件，应该是master和worker都需要按照文件里面的remote-exec配置好环境。所以卸载了所有安装的东西重新配环境。</p><p>在执行一些<code>kubectl apply -f</code>的时候遇到了很多：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unable to retrieve Cluster Version or Type: resource does not exist: XXX(default) with error: the server </span><br></pre></td></tr></table></figure><p>经搜索发现可能是kubernetes的版本太高了导致的，把kubernetes的版本降低到v1.21.0就解决了这些问题了。</p><p>配置好环境后执行<code>python run_workload.py pollux workloads/workload-6.csv</code>，遇到问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The push refers to repository [localhost:32000/pollux]</span><br><span class="line">Get <span class="string">&quot;http://localhost:32000/v2/&quot;</span>: dial tcp 127.0.0.1:32000: connect: connection refused</span><br></pre></td></tr></table></figure><p>经查看，端口32000是scheduler使用的，所以应该还是要先安装上scheduler才能跑benchmark。目前我还不知道如何用源码安装scheduler，所以暂时用了helm安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo helm repo add stable https://charts.helm.sh/stable</span><br><span class="line">sudo helm install adaptdl adaptdl-sched --repo https://github.com/petuum/adaptdl/raw/helm-repo --namespace default --<span class="built_in">set</span> docker-registry.enabled=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>在执行了安装scheduler的命令后，发现scheduler不能正常启动，都是pending状态：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/4.png" width="80%"></center><p>这是因为当前master节点带有taint（可以通过<code>kubectl describe node &lt;node&gt;</code>查看taint），这些tain后带有NoSchedule的标记，所以调度器找不到可以调度的资源了。我当时看到了两个taint，一个taint是说当前节点为master，另一个是Disk  pressure。可以用<code>kubectl taint nodes --all &lt;taint&gt;-</code>命令来去掉所有taint。但是有disk pressure的话后续还是会失败，清理磁盘到<code>/</code>目录的使用率在80%以下就没问题了。</p><p>调度器终于是running的状态之后，发现还是有失败的pod：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod copy-7pc4z</span><br><span class="line">Warning  FailedScheduling  115s  default-scheduler  0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims.</span><br><span class="line">$ kubectl get pvc <span class="comment"># the status of the PVC is pending</span></span><br><span class="line">$ kubectl describe pvc pollux</span><br><span class="line">Normal  Provisioning          3m35s (x27 over 92m)  rook-ceph.cephfs.csi.ceph.com_csi-cephfsplugin-provisioner-64d4468bbf-rwrfl_9d4dda0e-3e39-47e5-a5b5-b66b886b22d8  External provisioner is provisioning volume <span class="keyword">for</span> claim <span class="string">&quot;default/pollux&quot;</span></span><br><span class="line">  Normal  ExternalProvisioning  3m1s (x361 over 93m)  persistentvolume-controller                                                                                       waiting <span class="keyword">for</span> a volume to be created, either by external provisioner <span class="string">&quot;rook-ceph.cephfs.csi.ceph.com&quot;</span> or manually created by system administrator</span><br></pre></td></tr></table></figure><p>这居然是因为开源的artifact里面没有为PVC创建相应的PV！我只好把所有代码里涉及到PVC的目录全部改成hardcode的目录（<code>/mnt</code>），删掉创建PVC的pod和已经创建了的PVC，这样PVC就没有问题了。</p><p>然后发现提交了的adaptdljobs一直在pending，没有跑这些job的pod，但是可以通过kubectl get adaptdljobs看到这些job。kube describe没有报错信息。只能去查看scheduler pod的log。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs adaptdl-adaptdl-sched-cbc794b8f-br79v -c allocator </span><br></pre></td></tr></table></figure><p>发现log里有好多的信息，而且node上居然没有GPU这种资源类型：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/5.png" width="100%"></center><p>可能是GPU没有被scheduler发现。但是之前已经安装过nvidia的k8s plugin了。这种情况下只能去kube-system namespace下看看pod有什么问题。这个namespace下的所有pod都是正常运行的状态，所以要去看这些pod的log。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs nvidia-device-plugin-daemonset-n7f7q -n kube-system</span><br><span class="line">2022/05/27 07:25:44 Loading NVML</span><br><span class="line">2022/05/27 07:25:44 Failed to initialize NVML: could not load NVML library.</span><br><span class="line">2022/05/27 07:25:44 If this is a GPU node, did you <span class="built_in">set</span> the docker default runtime to `nvidia`?</span><br><span class="line">2022/05/27 07:25:44 You can check the prerequisites at: https://github.com/NVIDIA/k8s-device-plugin<span class="comment">#prerequisites</span></span><br><span class="line">2022/05/27 07:25:44 You can learn how to <span class="built_in">set</span> the runtime at: https://github.com/NVIDIA/k8s-device-plugin<span class="comment">#quick-start</span></span><br></pre></td></tr></table></figure><p>按照最后一行的link修改了<code>/etc/docker/daemon.json</code>文件并重启了docker service（<code>systemctl restart docker</code>），job终于是running状态了！</p><p>后来，在全新的环境里配置kubernetes的时候遇到问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">$ sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --v=5</span><br><span class="line">I0615 07:02:56.149065   39512 initconfiguration.go:115] detected and using CRI socket: /var/run/dockershim.sock</span><br><span class="line">I0615 07:02:56.149317   39512 interface.go:431] Looking <span class="keyword">for</span> default routes with IPv4 addresses</span><br><span class="line">I0615 07:02:56.149328   39512 interface.go:436] Default route transits interface <span class="string">&quot;eth0&quot;</span></span><br><span class="line">I0615 07:02:56.149899   39512 interface.go:208] Interface eth0 is up</span><br><span class="line">I0615 07:02:56.149966   39512 interface.go:256] Interface <span class="string">&quot;eth0&quot;</span> has 2 addresses :[10.5.0.7/24 fe80::20d:3aff:fe49:1b40/64].</span><br><span class="line">I0615 07:02:56.149982   39512 interface.go:223] Checking addr  10.5.0.7/24.</span><br><span class="line">I0615 07:02:56.149987   39512 interface.go:230] IP found 10.5.0.7</span><br><span class="line">I0615 07:02:56.150002   39512 interface.go:262] Found valid IPv4 address 10.5.0.7 <span class="keyword">for</span> interface <span class="string">&quot;eth0&quot;</span>.</span><br><span class="line">I0615 07:02:56.150007   39512 interface.go:442] Found active IP 10.5.0.7</span><br><span class="line">I0615 07:02:56.245405   39512 version.go:185] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.txt</span><br><span class="line">I0615 07:02:56.587172   39512 version.go:254] remote version is much newer: v1.24.1; falling back to: stable-1.21</span><br><span class="line">I0615 07:02:56.587215   39512 version.go:185] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.21.txt</span><br><span class="line">[init] Using Kubernetes version: v1.21.13</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">I0615 07:02:56.929034   39512 checks.go:582] validating Kubernetes and kubeadm version</span><br><span class="line">I0615 07:02:56.929090   39512 checks.go:167] validating <span class="keyword">if</span> the firewall is enabled and active</span><br><span class="line">I0615 07:02:56.937774   39512 checks.go:202] validating availability of port 6443</span><br><span class="line">I0615 07:02:56.937886   39512 checks.go:202] validating availability of port 10259</span><br><span class="line">I0615 07:02:56.937905   39512 checks.go:202] validating availability of port 10257</span><br><span class="line">I0615 07:02:56.937926   39512 checks.go:287] validating the existence of file /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">I0615 07:02:56.937939   39512 checks.go:287] validating the existence of file /etc/kubernetes/manifests/kube-controller-manager.yaml</span><br><span class="line">I0615 07:02:56.937949   39512 checks.go:287] validating the existence of file /etc/kubernetes/manifests/kube-scheduler.yaml</span><br><span class="line">I0615 07:02:56.937954   39512 checks.go:287] validating the existence of file /etc/kubernetes/manifests/etcd.yaml</span><br><span class="line">I0615 07:02:56.937964   39512 checks.go:437] validating <span class="keyword">if</span> the connectivity <span class="built_in">type</span> is via proxy or direct</span><br><span class="line">I0615 07:02:56.937981   39512 checks.go:476] validating http connectivity to first IP address <span class="keyword">in</span> the CIDR</span><br><span class="line">I0615 07:02:56.937999   39512 checks.go:476] validating http connectivity to first IP address <span class="keyword">in</span> the CIDR</span><br><span class="line">I0615 07:02:56.938010   39512 checks.go:103] validating the container runtime</span><br><span class="line">I0615 07:02:57.019188   39512 checks.go:129] validating <span class="keyword">if</span> the <span class="string">&quot;docker&quot;</span> service is enabled and active</span><br><span class="line">    [WARNING IsDockerSystemdCheck]: detected <span class="string">&quot;cgroupfs&quot;</span> as the Docker cgroup driver. The recommended driver is <span class="string">&quot;systemd&quot;</span>. Please follow the guide at https://kubernetes.io/docs/setup/cri/</span><br><span class="line">I0615 07:02:57.117386   39512 checks.go:336] validating the contents of file /proc/sys/net/bridge/bridge-nf-call-iptables</span><br><span class="line">I0615 07:02:57.117456   39512 checks.go:336] validating the contents of file /proc/sys/net/ipv4/ip_forward</span><br><span class="line">I0615 07:02:57.117489   39512 checks.go:654] validating whether swap is enabled or not</span><br><span class="line">I0615 07:02:57.117530   39512 checks.go:377] validating the presence of executable conntrack</span><br><span class="line">I0615 07:02:57.117575   39512 checks.go:377] validating the presence of executable ip</span><br><span class="line">I0615 07:02:57.117600   39512 checks.go:377] validating the presence of executable iptables</span><br><span class="line">I0615 07:02:57.117625   39512 checks.go:377] validating the presence of executable mount</span><br><span class="line">I0615 07:02:57.117650   39512 checks.go:377] validating the presence of executable nsenter</span><br><span class="line">I0615 07:02:57.117670   39512 checks.go:377] validating the presence of executable ebtables</span><br><span class="line">I0615 07:02:57.117691   39512 checks.go:377] validating the presence of executable ethtool</span><br><span class="line">I0615 07:02:57.117712   39512 checks.go:377] validating the presence of executable socat</span><br><span class="line">    [WARNING FileExisting-socat]: socat not found <span class="keyword">in</span> system path</span><br><span class="line">I0615 07:02:57.117753   39512 checks.go:377] validating the presence of executable tc</span><br><span class="line">I0615 07:02:57.117774   39512 checks.go:377] validating the presence of executable touch</span><br><span class="line">I0615 07:02:57.117795   39512 checks.go:525] running all checks</span><br><span class="line">I0615 07:02:57.209961   39512 checks.go:408] checking whether the given node name is valid and reachable using net.LookupHost</span><br><span class="line">I0615 07:02:57.210512   39512 checks.go:623] validating kubelet version</span><br><span class="line">I0615 07:02:57.270497   39512 checks.go:129] validating <span class="keyword">if</span> the <span class="string">&quot;kubelet&quot;</span> service is enabled and active</span><br><span class="line">I0615 07:02:57.279877   39512 checks.go:202] validating availability of port 10250</span><br><span class="line">I0615 07:02:57.279947   39512 checks.go:202] validating availability of port 2379</span><br><span class="line">I0615 07:02:57.279966   39512 checks.go:202] validating availability of port 2380</span><br><span class="line">I0615 07:02:57.279985   39512 checks.go:250] validating the existence and emptiness of directory /var/lib/etcd</span><br><span class="line">[preflight] Some fatal errors occurred:</span><br><span class="line">    [ERROR FileExisting-conntrack]: conntrack not found <span class="keyword">in</span> system path</span><br><span class="line">[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</span><br><span class="line">error execution phase preflight</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:235</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:421</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdInit.func1</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/init.go:152</span><br><span class="line">k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:850</span><br><span class="line">k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:958</span><br><span class="line">k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:895</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app.Run</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50</span><br><span class="line">main.main</span><br><span class="line">    _output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25</span><br><span class="line">runtime.main</span><br><span class="line">    /usr/<span class="built_in">local</span>/go/src/runtime/proc.go:225</span><br><span class="line">runtime.goexit</span><br><span class="line">    /usr/<span class="built_in">local</span>/go/src/runtime/asm_amd64.s:1371</span><br></pre></td></tr></table></figure><p>是因为没有安装conntrack：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install conntrack</span><br></pre></td></tr></table></figure><h3 id="作业的运行"><a href="#作业的运行" class="headerlink" title="作业的运行"></a>作业的运行</h3><p>虽然job是running状态，但是job的一些行为十分诡异，需要注意解决。</p><h4 id="问题1-OSError-Errno-30-Read-only-file-system-‘-mnt-tensorboard’"><a href="#问题1-OSError-Errno-30-Read-only-file-system-‘-mnt-tensorboard’" class="headerlink" title="问题1 OSError: [Errno 30] Read-only file system: ‘/mnt/tensorboard’"></a>问题1 OSError: [Errno 30] Read-only file system: ‘/mnt/tensorboard’</h4><p>在job还在运行的时候，具体的job的log可以通过kubectl logs <pod>看到。<br>通过log发现：<code>OSError: [Errno 30] Read-only file system: &#39;/mnt/tensorboard/&#39;</code><br>无论是在docker外面还是里面，检查<code>/mnt</code>下的目录的权限，都是所有人可读写的，甚至这个目录存在的时候，会出现在里面写文件失败的情况，也是Read-only file system这种报错。<br>最后发现是每一个job的adaptdljob.yaml里有这样的一段：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">volumeMounts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">    <span class="attr">mountPath:</span> <span class="string">/mnt</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>把true改成false，删掉daemonsets，删掉build好的container镜像，然后重新运行脚本build镜像就没有这个报错了。</p><h4 id="问题2-把模型放到GPU内存里过程缓慢，需要10min，且训练卡死在forward阶段"><a href="#问题2-把模型放到GPU内存里过程缓慢，需要10min，且训练卡死在forward阶段" class="headerlink" title="问题2 把模型放到GPU内存里过程缓慢，需要10min，且训练卡死在forward阶段"></a>问题2 把模型放到GPU内存里过程缓慢，需要10min，且训练卡死在forward阶段</h4><p>经过profile发现<code>net = net.to(device)</code>需要10min之久，且训练会卡死在第一个<code>outputs = net(inputs)</code>。GPU的内存有一定的使用，但是计算上的利用率一直是0%。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/6.png" width="80%"></center>经过百度发现可能是CUDA、cuDNN、python等版本不匹配的问题。然后我发现在container里面，`torch.version.cuda`的CUDA版本是10.X，但是`nvidia-smi`得到的CUDA版本是11.X，而在container外面，两种方式得到的CUDA版本都是11.X，所以问题一定出在了container的环境上。把每一种job的Dockerfile第一行的镜像版本都改成`nvcr.io/nvidia/pytorch:21.10-py3`之后，还是删掉daemonsets，删掉build好的container镜像，然后重新运行脚本build镜像，一些都变得正常了。<p>后来在全新的环境里，又遇到了全新的问题，<a href="https://github.com/petuum/adaptdl/issues/124">https://github.com/petuum/adaptdl/issues/124</a></p><h2 id="Simulator代码阅读"><a href="#Simulator代码阅读" class="headerlink" title="Simulator代码阅读"></a><a href="https://github.com/petuum/adaptdl/tree/osdi21-artifact">Simulator代码阅读</a></h2><ul><li><code>simulator/traces/model/placements.csv</code>文件的含义，可以通过<code>simulator/application.py</code>看到<ul><li>第一列的字符串，有几个字符，就相当于有几个node</li><li>每个数字代表这个node上有几块GPU在用</li></ul></li></ul><hr><p><strong>原文作者：</strong>Aurick Qiao, Sang Keun Choe, Suhas Jayaram Subramanya, Willie Neiswanger, Qirong Ho, Hao Zhang, Gregory R. Ganger, Eric P. Xing<br><strong>原文链接：</strong><a href="https://arxiv.org/pdf/2008.12260v2.pdf">https://arxiv.org/pdf/2008.12260v2.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/petuum/adaptdl">https://github.com/petuum/adaptdl</a><br><strong>参考文献：</strong>[1] 计算机系统软件顶会OSDI 2021最佳论文出炉，邢波团队研究入选, <a href="https://zhuanlan.zhihu.com/p/390040932">https://zhuanlan.zhihu.com/p/390040932</a></p>]]></content>
    
    
    <summary type="html">本文是OSDI&#39;21会议的best paper，由Petuum磐腾科技提出的GPU集群调度方法，可以在训练时动态调整资源数量，为深度学习训练的不同阶段找到合适的batch size，从而使得训练效率和训练的准确率都能够得到保障。</summary>
    
    
    
    
    <category term="集群调度" scheme="https://diandiangu.github.io/tags/%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6/"/>
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="OSDI" scheme="https://diandiangu.github.io/tags/OSDI/"/>
    
    <category term="弹性训练" scheme="https://diandiangu.github.io/tags/%E5%BC%B9%E6%80%A7%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>AFS：分布式深度学习中的弹性资源共享</title>
    <link href="https://diandiangu.github.io/2021/04/13/AFS/"/>
    <id>https://diandiangu.github.io/2021/04/13/AFS/</id>
    <published>2021-04-13T07:53:33.000Z</published>
    <updated>2021-04-18T03:48:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Elastic-Resource-Sharing-for-Distributed-Deep-Learning"><a href="#Elastic-Resource-Sharing-for-Distributed-Deep-Learning" class="headerlink" title="Elastic Resource Sharing for Distributed Deep Learning"></a>Elastic Resource Sharing for Distributed Deep Learning</h1><p>这篇文章是KAIST的박경수教授指导的工作，发表于NSDI 2021。作者提到，资源的分配和调度策略对于深度学习训练任务（DLT jobs）的平均任务完成时间（JCT， job complete time）有很大的影响。“最短剩余时间优先”等传统的方法在这方面表现得很差（只盲目地优先短的任务太粗粒度了）。这篇文章发现，对于加速DLT任务的执行来说，：（1）资源效率（resource efficiency，即一块GPU的边际吞吐量，如果一个任务使用的GPU数量变多，相应资源效率会下降）比段任务优先更重要；（2）如果对已有任务采用贪心算法会使得JCT增加。根据这些发现，作者提出了AFS来平衡资源效率和短任务的优先，与此同时限制资源分配中的不现实的优化。作者同时建立了一个透明化处理对模型的并行训练，并高效处理共享资源调整的DLT系统CoDDL。</p><h2 id="研究背景和动机"><a href="#研究背景和动机" class="headerlink" title="研究背景和动机"></a>研究背景和动机</h2><h3 id="问题与挑战"><a href="#问题与挑战" class="headerlink" title="问题与挑战"></a>问题与挑战</h3><p>减少集群中任务的JCT很重要。本文的目标：最小化JCT。</p><p>现有工作的局限性：</p><ul><li>Tiresias和Themis等调度器可以提高“结束早的”任务的优先级，却忽略了长时间运行的DL任务</li><li>Tiresias和Gandiva等调度器的算法不支持弹性资源分配（elastic training）</li></ul><p>本文提出了一种可以减小集群中<strong>平均JCT</strong>的动态调度方法，该方法中每个DL任务使用的GPU数量会不断调整。</p><p>挑战：</p><ul><li><p>算法层面：需要同时考虑到任务时长和resource efficiency -&gt; 设计新的调度算法</p></li><li><p>系统层面：现有auto-scaling和GPU间通信的API的overhead太大 -&gt; 实现了CoDDL这个系统来减小overhead</p></li></ul><h2 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h2><p>本文的弹性资源共享希望可以平衡短任务和高效任务之间的优先权。寻找最优分配策略看起来是一个NP完全问题。此外，未来作业的到达在调度时是未知的，可能会严重影响之前的资源重新分配决策。本文首先通过对简化问题的严格分析并观察。</p><h3 id="算法概述"><a href="#算法概述" class="headerlink" title="算法概述"></a>算法概述</h3><ul><li><p>问题的定义</p><ul><li>每个DLT job $j_k$在任一时刻被提交到一个有$M$块GPU的集群 </li><li>集群中的每个job都采用bulk-synchronous-parallel (BSP)模式训练</li><li>一共有$n$个job，对于每个job集群需要两次重新分配：开始和结束。每次重新分配都是一个event。</li><li>找到n维向量$R_u$={$r_{1,u}$, $r_{2,u}$, …,$r_{n,u}$}, $1 ≤ u ≤ 2n$，$r_{k,u}$是在第$u$个event之后分配给$j_k$的GPU数量</li></ul></li><li><p>方法</p><ul><li>找到最优的R_u需要对还没有到达的future job的知识（详见附录A）。</li><li>因此本文希望找到一个可以减少平均JCT的启发式策略。</li><li>不断地使用贪心策略依赖于这样一个假设：已完成任务释放的资源只会被已有任务使用，这在一个新的任务到来时会显得“过于乐观”。</li><li>这说明，调度器假设集群中所有正在执行的任务都只会不断地增加它所使用的资源数量，而不会减少；这和事实严重不符。</li></ul></li><li><p><strong>AFS假设：假设集群中每一个任务在未来的资源使用会保持不变</strong>，然后在此基础上找到最优的方案。这很简单，而且接近实际的集群环境（大多数情况下资源竞争的程度不会有特别大的变化）。</p></li></ul><p><strong>其中，AFS在为任务调整使用的GPU数量的时候是strong sclaing，即global batch size不变。</strong></p><h3 id="通过分析两个任务的情况得到的启示"><a href="#通过分析两个任务的情况得到的启示" class="headerlink" title="通过分析两个任务的情况得到的启示"></a>通过分析两个任务的情况得到的启示</h3><h4 id="两个任务，一块GPU"><a href="#两个任务，一块GPU" class="headerlink" title="两个任务，一块GPU"></a>两个任务，一块GPU</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/AFS/1.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：timeline of jobs示例</div></center><br><p>此时，若任务a更短，则($r_a,1$, r_a,2, r_b,a, r_b,2) = (1, 0, 0, 1)；否则，(r_a,1, r_a,2, r_b,a, r_b,2) = (0, 1, 1, 0)</p><h4 id="两个任务，M块GPU-M-gt-1-，待分配GPU为1块"><a href="#两个任务，M块GPU-M-gt-1-，待分配GPU为1块" class="headerlink" title="两个任务，M块GPU (M &gt; 1)，待分配GPU为1块"></a>两个任务，M块GPU (M &gt; 1)，待分配GPU为1块</h4><p>假设任务a比b先完成。假设已经有M-1块GPU分配给了两个任务如何决定剩下的一块GPU要分给谁？</p><p>用$w_k$来表示任务k所需要的训练iteration数量，$p_k,u$表示任务k分配有$r_k,u$块GPU时的吞吐率。可以用这些符号表示出t1和t2:</p><p>$$<br>t_1 = \frac{w_a} {p_{a, 1}} , t_2 = \frac{w_b - p_{b, 1}t_1}{p_{b, 2}}<br>$$</p><p>将这一块GPU分配给不同的任务，最终的任务完成时间不同。同样可以用这些符号表达出不同情况下的任务结束时间。需要找出可以使得完成时间最早的GPU分配方案。</p><h4 id="两个任务，M块GPU-M-gt-1-，待分配GPU多于1块"><a href="#两个任务，M块GPU-M-gt-1-，待分配GPU多于1块" class="headerlink" title="两个任务，M块GPU (M &gt; 1)，待分配GPU多于1块"></a>两个任务，M块GPU (M &gt; 1)，待分配GPU多于1块</h4><p>反复执行以上步骤，直到分配完成</p><h4 id="处理后续的任务"><a href="#处理后续的任务" class="headerlink" title="处理后续的任务"></a>处理后续的任务</h4><p>通过正文和附录里对多种情况的分析，说明（1）在未来有新的任务加入时，可能会使得之前最优的资源分配不再是最优选择；（2）如果新加入的任务使得资源竞争更加激烈，把更多的GPU分配给所需时间长但是更高效的任务是更好的选择。</p><h4 id="Apathetic-Future-Share"><a href="#Apathetic-Future-Share" class="headerlink" title="Apathetic Future Share"></a>Apathetic Future Share</h4><p>AFS假设：任何当前已有的任务在未来得到的资源分配都和现在的相同，即使这个过程中有的任务结束并释放了资源。基于这个假设，之前用于计算GPU分配的公式变得非常简单：</p><p>$$<br>\frac{p_{b,1}’-p_{b,1}} {p_{b,1}’} &gt; \frac{p_{a,1}’-p_{a,1}} {p_{a,1}’}<br>$$</p><p>其中$p_{k,u}’$表示比$p_{k,u}$多用一块GPU的吞吐率。</p><p>AFS假设在一些情况下并不是最优的，但是作者认为这些情况在实际集群中不常见。</p><h3 id="多任务的AFS算法"><a href="#多任务的AFS算法" class="headerlink" title="多任务的AFS算法"></a>多任务的AFS算法</h3><h4 id="从2个任务扩展到n个任务"><a href="#从2个任务扩展到n个任务" class="headerlink" title="从2个任务扩展到n个任务"></a>从2个任务扩展到n个任务</h4><p>AFS-L：n个任务量量比较优先级，对M块GPU一次分配，时间复杂度为O(M·n)。</p><p>AFS-L算法分为两个阶段：（1）假设所有任务都在单GPU上运行，如果M比任务的数量少，算法停止，否则（2）通过衡量每个任务所需时长和资源效率，决定如何分配剩下的GPU。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/AFS/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">算法1：AFS-L 资源共享</div></center><br><h4 id="处理任务时长未知的情况"><a href="#处理任务时长未知的情况" class="headerlink" title="处理任务时长未知的情况"></a>处理任务时长未知的情况</h4><p>AFS-L算法要求已知任务所需时长。作者把AFS-L算法中找出优先级最高的任务的计算方法，修改为衡量之前提到的简化后的公式。如果通过这个公式也找不到更好的方案，最随机分配GPU。修改后的整个算法被称为AFS-P。在AFS-P中，如果任务的数量多于M，则通过记录下来的每个任务历史的资源分配情况，决定哪些任务可以优先得到GPU。</p><h2 id="CoDDL系统"><a href="#CoDDL系统" class="headerlink" title="CoDDL系统"></a>CoDDL系统</h2><p>CoDDL系统在文中的介绍并不详细，该系统基于团队在OSDI‘18的一篇poster：Efficient Resource Sharing for Distributed Deep Learning。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/AFS/3.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：CoDDL系统架构</div></center><br><p>用户只需提交单机代码，该框架就可以运行数据并行的训练。CoDDL实现了自己的通信栈，在scaling时将无需通信的GPU op与新加入GPU的初始化overlap。</p><p>作者没有提通信上的overlap；据说做一次scale out只需4ms。但是无论在本篇论文中还是之前的poster中，都没有介绍overhead为什么可以这么低。</p><p>作者声称share shrinking中只需发信号给通信栈然后停掉相应kernel，这是zero cost的，只需4ms。</p><p>有许多reconfiguration同时到来的时候，每个worker可以“取消”正在进行初始化的reconfiguration。</p><p>CoDDL有自己的容错机制，而且支持job migration。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>对AFS算法从模拟和实际集群实验两方面进行评估。使用了有16台机器、每台机器4块GTX 1080 GPU的集群进行实验。workload基于微软137天的实际的trace。根据trace计算每个模型训练所需的iteration数，并根据此来进行后面对JCT的模拟。</p><p>对算法的评估分为两组：一组的算法中，任务时长已知，另一组的算法中任务时长未知。对于实际集群中的实验，都从平均JCT、最长完工时间、GPU利用率等等多个方面进行了评估。AFS在各个方面的表现和其他的调度算法相比，都合理且优秀。</p><p>作者还评估了CoDDL share re-adjustment的效率。没有和其他论文的方法进行比较，而是和没有reconfiguration cancelling的方法，以及既没有op overlap也没有reconfiguration cancelling的方法进行了比较，证明这两方面的优化可以让任务更快结束。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇文章的项目开始于2017年，到2021年才发表。AFS可以将集群中任务的平均JCT提升2.2倍至3.1倍。该方法只考虑了数据并行。该算法没有考虑用户对使用的GPU数量有自己的需求，如果用户要求一个任务使用大量的GPU，但是AFS算法认为这样不利于平均JCT，是否就无法满足用户的要求进行gang schedule了呢？另外，本文实验中仅考虑了训练一个模型所需的iteration数，没有考虑在改变了训练所用GPU数量之后，训练了这些iteration后模型的准确率是否达到了标准。</p><p>其他问题：不支持模型并行；没有考虑GPU之间的topology。</p><hr><p><strong>原文作者：</strong>Changho Hwang, Taehyun Kim（共同一作）, Sunghyun Kim, Jinwoo Shin, KyoungSoo Park<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/nsdi21-hwang.pdf">https://www.usenix.org/system/files/nsdi21-hwang.pdf</a><br><strong>CoDDL Poster：</strong><a href="https://chhwang.github.io/pubs/osdi18_poster_hwang.pdf">https://chhwang.github.io/pubs/osdi18_poster_hwang.pdf</a></p>]]></content>
    
    
    <summary type="html">在这篇文章中，作者度量并分析了分布式训练的网络表现。作者预期，度量结果会证实通信是阻碍分布式训练达到linear scale-out效果的原因。但是，作者发现实际上网络带宽利用率很低，如果网络带宽可以被充分利用，分布式训练的scaling factor可以接近于1。</summary>
    
    
    
    
    <category term="NSDI" scheme="https://diandiangu.github.io/tags/NSDI/"/>
    
    <category term="集群调度" scheme="https://diandiangu.github.io/tags/%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6/"/>
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Rammer：如何通过全局视角编译深度学习计算</title>
    <link href="https://diandiangu.github.io/2021/04/09/Rammer/"/>
    <id>https://diandiangu.github.io/2021/04/09/Rammer/</id>
    <published>2021-04-09T09:40:52.000Z</published>
    <updated>2021-11-15T06:33:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Rammer-Enabling-Holistic-Deep-Learning-Compiler-Optimizations-with-rTasks"><a href="#Rammer-Enabling-Holistic-Deep-Learning-Compiler-Optimizations-with-rTasks" class="headerlink" title="Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks"></a>Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks</h1><p>传统深度学习框架由于其自身的局限性，如今还远没有充分发挥出硬件的计算性能。微软亚洲研究院的研究员们在一些测试集上发现，现有的深度学习模型只能用到 GPU 2%到40%的性能。传统的深度学习框架通常通过分层调度来将一个深度学习模型调度到硬件设备（通常是 GPU 这样的协处理器）上进行计算。首先，在上层，深度学习模型通常会被抽象为由算子（Operator）和依赖关系构建而成的数据流图（DFG），深度学习框架主要负责将每个算子按照正确的依赖关系依次调度到下层设备上；接着，在下层硬件设备上（如 GPU），会有一个硬件的调度器将每个算子根据其内部并行性调度到硬件内的并行计算核上。这样两层调度的模型尽管较为简洁明晰，但在实际的部署中，两个调度层互相不感知会导致较大的调度开销，以及较低的硬件利用率。</p><p>传统深度学习框架在实现全面神经网络优化上的核心障碍在于：首先，现有的基于数据流图的抽象无法表示算子内部的并行性，对于由深度学习框架控制的图调度器完全是黑盒；其次，硬件厂商往往将细粒度的任务调度隐藏在硬件中，对上层框架调度也是黑盒。</p><p>针对现有深度学习框架的局限，微软亚洲研究院和北京大学、上海科技大学合作提出了一种可以成倍甚至几十倍地提升深度学习计算速度的编译框架RAMMER。这篇文章发表于OSDI 2020。</p><h2 id="动机与挑战"><a href="#动机与挑战" class="headerlink" title="动机与挑战"></a>动机与挑战</h2><p>目前，对两个层面的并行是通过“两层”的调度来进行的。DFG （data flow graph） scheduler实现inter-op的并行：通过op之间的依赖关系“发射”准备就绪的op；在不同的硬件上，op被看作是库中的“黑盒”function。硬件上的scheduler实现op内的并行：它把op内部的计算映射到加速器上的并行执行单元（EUs）上。在shchedule的overhead可以忽略且intr-op的并行计算可以充分利用全部的EU的情况下，这样两层的架构可以运行得很好。</p><p>然而，通常情况下，GPU设备的利用率会很低，调度op的开销也会很大。另外，这种两层的架构忽视了inter和intra op并行之间微妙的相互作用。比如两个可以同时计算的op可能尽可能地占用更多的EU资源，但是每个EU的利用率并不高。这会导致其中一个op等待另一个op执行完，才能获得足够的资源。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Rammer/1.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：已有方法（a）忽视了inter和intra op并行之间的相互作用，十分不高效；而（b）是优化后的方案</div></center><br><p>因此，Rammer把inter-op和intra-op共同进行调度。有几点挑战：</p><ul><li><p>挑战1：op的intra-op并行是由硬件进行调度的，并没有给软件暴露出细粒度的intra-op并行计算方式</p><p>解决方法：定义了rTask作为在EU上计算的最小单元，并把细粒度的intra-op parallelism暴露出来</p></li><li><p>挑战2：GPU等加速器没有暴露intra-op调度的接口</p><p>解决方法：Rammer把硬件加速器抽象成虚拟的并行设备，把rTask调度到vEU上，每一个vEU再对应一个真实的加速器上的EU。</p></li><li><p>挑战3：细粒度的调度引入跟多的调度overhead</p><p>解决方法：多数DNN的计算图是“可预测的”，且op的performance是deterministic的。因此可以在编译阶段就生成执行方案</p></li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Rammer/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：（a）传统深度学习框架，（b）RAMMER 深度学习编译框架</div></center><h2 id="Wavefront-scheduling-policy"><a href="#Wavefront-scheduling-policy" class="headerlink" title="Wavefront scheduling policy"></a>Wavefront scheduling policy</h2><p>Rammer中为每一个op都实现了一种或多种kernel（具体的实现方式）。有的kernel计算很块（fast），有的kernel使用资源较少（resource-efficient）。Rammer默认使用fastest kernels；在资源比较紧张、可以占满所有EU的时候，使用resource-efficient kernels。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>Rammer可以提高GPU的利用率，但是在某些情况下，讲两层的op并行调度架构改为1层的效果并不明显。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Rammer/3.png" width="85%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：GPU利用率</div></center><br><p>Rammer可以减少调度时间。但是没有评估Rammer导致的compile time的增加。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Rammer/4.png" width="85%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：Scheduling Overhead</div></center><hr><p><strong>原文作者：</strong>Lingxiao Ma∗, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, Lidong Zhou<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/osdi20-ma.pdf">https://www.usenix.org/system/files/osdi20-ma.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/microsoft/nnfusion">https://github.com/microsoft/nnfusion</a><br><strong>参考文献：</strong>[1] OSDI 2020 | 微软亚洲研究院论文一览 <a href="https://mp.weixin.qq.com/s/vHeXvAeEPPiTls349yjkAQ">https://mp.weixin.qq.com/s/vHeXvAeEPPiTls349yjkAQ</a></p>]]></content>
    
    
    <summary type="html">Rammer是一个可以成倍甚至几十倍地提升深度学习inference计算速度的编译框架。</summary>
    
    
    
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="OSDI" scheme="https://diandiangu.github.io/tags/OSDI/"/>
    
    <category term="推理系统" scheme="https://diandiangu.github.io/tags/%E6%8E%A8%E7%90%86%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="深度学习编译器" scheme="https://diandiangu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>BytePS：加速异构集群中分布式训练的统一架构</title>
    <link href="https://diandiangu.github.io/2021/03/15/BytePS/"/>
    <id>https://diandiangu.github.io/2021/03/15/BytePS/</id>
    <published>2021-03-15T10:12:33.000Z</published>
    <updated>2021-03-22T02:07:57.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="A-Unified-Architecture-for-Accelerating-Distributed-DNN-Training-in-Heterogeneous-GPU-CPU-Clusters"><a href="#A-Unified-Architecture-for-Accelerating-Distributed-DNN-Training-in-Heterogeneous-GPU-CPU-Clusters" class="headerlink" title="A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters"></a>A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters</h1><p>本文是字节跳动、清华大学和Google合作的项目，提出一种分布式DNN训练的统一架构<strong>BytePS</strong>，并延续了字节跳动在RDMA方面的研究，利用了RDMA高速网络的特性对集群的通信和算力资源利用率进行优化。</p><h2 id="背景和动机"><a href="#背景和动机" class="headerlink" title="背景和动机"></a>背景和动机</h2><p>即使目前已经有了许多，数据并行的效率远远没有达到理想的情况。</p><p>本文希望解决的三个问题：</p><ul><li>多机之间通信并不充分</li><li>单机内部GPU之间的通信也有瓶颈</li><li>CPU的性能限制了训练的效率</li></ul><p>通过从字节跳动异构集群中收集的3个月的trace，可以看出：（1）CPU并没有被很好的利用；（2）很多服务器运行的是非分布式任务，这些机器的贷款并没有被很好的利用。这提供了一个很好的机会：异构集群中有剩余的CPU和带宽。本文的设计目标是去充分利用这些没有被利用起来的资源。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BytePS/1.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：集群中收集到的DNN训练相关数据</div></center><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BytePS/2.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：BytePS架构</div></center><h3 id="多机之间的通信"><a href="#多机之间的通信" class="headerlink" title="多机之间的通信"></a>多机之间的通信</h3><p>一方面，PS只利用GPU和CPU之间的带宽进行通信。如果CPU服务器的数量不充足，那么GPU服务器的带宽就没有被充分利用。另一方面，all-reduce只利用GPU服务器之间的连接，CPU服务器的带宽完全没有被利用。所以最好的策略是把二者结合起来，这样所有机器之间的连接都能被利用，并且能充分利用CPU资源。这带来一个问题：如何分割这些link workload？</p><p>用x和y分别代表CPU-GPU traffic所占百分比和GPU-GPU traffic所占百分比。可以计算出最佳的策略下x和y的值。其中，n和k分别是GPU和CPU服务器的数量。在理论上，这样的partition策略可以达到最短的通信时间。<br>$$<br>x = \frac{2k(n - 1)} {n^2+kn-2k}, y = \frac{n(n - k)} {n^2+kn-2k}<br>$$</p><h3 id="单机内部的通信"><a href="#单机内部的通信" class="headerlink" title="单机内部的通信"></a>单机内部的通信</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BytePS/3.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：只有PCIe拓扑结构时的优化方案</div></center><br><p>如图所示，当单机之间只有PCIe拓扑结构时，瓶颈在于CPU和PCIe switch之间的连接。本文希望能够尽可能减少这个连接之间通信。然而，像MPI或者NCCL等通信库直接对这8个GPU做all reduce。根据all-reduce算法可以计算出，直接做all reduce时，bottleneck link上有7M/4的通信量，M是每个GPU上的模型大小。这个通信量对于bottleneck link来说太大了。</p><p>BytePS通过<strong>CPU-assisted aggregation</strong>的方法来解决这个问题。首先，在同一个PCIe switch下的四个GPU之间进行一次local reduce-scatter。这样每块GPU上有1/4的aggregated gradient。然后每个GPU把它所拥有的四分之一copy到主存中。最后，CPU把两个PCIe switch下的gradient加和。现在在bottleneck link上的通信量只有M。理论上该方法比MPI/NCCL快24%。</p><p>对于有NVlink的GPU machine，可以用类似的方法分析瓶颈所在的连接，并设计方法减少瓶颈连接上的通信量。</p><p>对于不同的拓扑结构来说，最优的通信算法不同，并不存在一个对所有拓扑结构都适用的算法。</p><h3 id="解决CPU瓶颈"><a href="#解决CPU瓶颈" class="headerlink" title="解决CPU瓶颈"></a>解决CPU瓶颈</h3><p>现象：optimization可以分成两个阶段：梯度加和和参数更新。前者对CPU友好，后者对于CPU来说计算量过大。</p><p>parameter server把forward和backward放在GPU上，把整个optimization放在CPU上。本文提出<strong>Summation Service</strong>：把optimization中的参数更新放到GPU上计算。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BytePS/4.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：不同策略下component的放置</div></center><br>但是这样是否会有过多的context switch从而带来这方面的overhead，作者没有讨论。<h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>多种模型、框架、baseline。</p><p>多机之间通信：byteps的通信表现可以达到几乎最优；CPU服务器越多，BytePS的表现越好。</p><p>单机内部通信：对于有PCIe和NVlink的机器，都有很大的performance提升。</p><p>End-to end scalability：在各种情况下的throughput都有所提升，GPU数量越多，提升越大。</p><hr><p><strong>原文作者：</strong>Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, Chuanxiong Guo<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/osdi20-jiang.pdf">https://www.usenix.org/system/files/osdi20-jiang.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/alibaba/GPU-scheduler-for-deep-learning">https://github.com/alibaba/GPU-scheduler-for-deep-learning</a></p>]]></content>
    
    
    <summary type="html">本文介绍了BytePS，是字节跳动、清华大学和Google合作的项目，提出一种分布式DNN训练的统一架构BytePS，并延续了字节跳动在RDMA方面的研究，利用了RDMA高速网络的特性对集群的通信和算力资源利用率进行优化。</summary>
    
    
    
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="SOSP" scheme="https://diandiangu.github.io/tags/SOSP/"/>
    
    <category term="通信调度" scheme="https://diandiangu.github.io/tags/%E9%80%9A%E4%BF%A1%E8%B0%83%E5%BA%A6/"/>
    
    <category term="深度学习框架" scheme="https://diandiangu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch Distributed：加速数据并行训练的经验</title>
    <link href="https://diandiangu.github.io/2021/03/08/DDP/"/>
    <id>https://diandiangu.github.io/2021/03/08/DDP/</id>
    <published>2021-03-08T06:08:00.000Z</published>
    <updated>2021-03-09T09:43:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training"><a href="#PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training" class="headerlink" title="PyTorch Distributed: Experiences on Accelerating Data Parallel Training"></a>PyTorch Distributed: Experiences on Accelerating Data Parallel Training</h1><p>本文介绍了PyTorch DDP模块的设计、实现和评估。提供一个通用的数据并行训练package有三方面考验：</p><ul><li><strong>数学上的等价：</strong>需要保证和本地训练一样的训练收益</li><li><strong>无需大量修改用户代码的以及截获性的API：</strong>方便用户运行数据并行训练，也要允许用户自定义通信和优化</li><li><strong>高效：</strong>较高的训练吞吐量</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>训练DNN模型的三步：</p><ol><li>Forward pass: 计算损失</li><li>Backward pass: 计算梯度</li><li>Optimizer step: 更新参数</li></ol><p>PyTorch DDP采用all-reduce通信。All-recude是一种同步的通信机制，因为all-reduce只有在所有进程都准备好之后才能开始通信。与之相反，PS中采用的是P2P的通信方式。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="梯度Reduction"><a href="#梯度Reduction" class="headerlink" title="梯度Reduction"></a>梯度Reduction</h3><p>在过去的版本中，梯度Reduction的计算方法一直在不断改进。</p><ol><li><p><strong>Naive solution：</strong>DDP控制所有的训练进程（1）从同一个模型状态开始训练；（2）在每个iteration用同样的梯度。<br>通过在每次local backward之后梯度同步，或者加hook就可以实现（2），但是有两个performance上的问题：<br>小tensor中collective communication效率非常低<br>把梯度计算和同步分开之后，就不能把它们overlap了</p></li><li><p><strong>Gradient bucketing：</strong>Collective communications在大tensor上通信效率更高。因此，可以通过gradient reduction将几个梯度打包成一个allreduce操作。然而，为了让通信和计算overlap，DDP不能把所有的梯度都打包到一个allreduce里。</p></li><li><p><strong>计算和通信overlap：</strong>在bucketing中，DDP只需要等一个bucket中的所有数据都准备好通信。需要注意（1）在所有进程中reduce的顺序一定要相同；（2）有时不同进程里backward中gradient计算顺序不同，或有的gradient被跳过，此时会导致hang。</p></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/DDP/2.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：分布式梯度reduction</div></center><br><ol start="4"><li><strong>梯度累积：</strong>每n个iteration进行一次allreduce。</li></ol><h3 id="Collective-Communication"><a href="#Collective-Communication" class="headerlink" title="Collective Communication"></a>Collective Communication</h3><p>DDP建立在NCCL、Gloo和MPI等通信库之上，把通信库里的API打包到ProcessGroup API中。在DDP里，worker需要加入通信组才能进行相关通信。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><ul><li>Python前端：有很多可以configure的节点，例如process group等等；支持单机多卡的模型并行；支持模型buffer，rank 0负责存储buffer的内容，在forward前将rank 0上buffer的内容broadcast到其他设备上。</li><li>核心梯度reduction：确保同一bucket中的parameter都来自同一个device；通过一个count来判断当前backward到了第几层，从而在合适的时候allreduce；默认bucket size为25M；在CPU上创建位图来保存本地没有使用的参数信息，并通过一个额外的allreduce得到global bitmap。</li></ul><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><ul><li>比较了不同模型、使用不同backend、有无通信和训练的overlap，并将latency breakdown为不同的部分来分析。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/DDP/3.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：breakdown每个iteration的latency</div></center><br><ul><li>比较了不同的bucket size的影响。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/DDP/4.png" width="95%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：bucket size v.s. latency</div></center><br><ul><li><p>比较了使用不同数量GPU时的latency、不同同步步调的latency以及loss。</p></li><li><p>DDP通过使用多个round-robin（轮询调度）进程组从而充分利用带宽。实验比较了使用不同数量进程组对latency的影响。</p></li></ul><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>没有适用于所有情况的configuration，但是可以总结出一些规则来帮助找到最佳configuration：</p><ul><li><p>通信backend：多数情况下，NCCL比Gloo快</p></li><li><p>Bucket size：随着模型的增大而增大</p></li><li><p>Resource allocation：用NCCL时，建议把所有的worker都放到同一台机器上的同一个进程组中</p></li></ul><p>未来优化方向:</p><ul><li><p>梯度顺序预测：使用自动梯度计算hook记录backward的order，并相应地向bucket mapping中更新参数</p></li><li><p>Layer dropping：在forward的过程中随机drop掉几层（加速训练，避免过拟合）；与此同时相应修改parameter-to-bucket mapping，或从bucket level drop</p></li><li><p>梯度压缩： 只通信需要高精度的梯度</p></li></ul><hr><p><strong>原文作者：</strong>Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, Soumith Chintala†<br><strong>原文链接：</strong><a href="https://arxiv.org/pdf/2006.15704.pdf">https://arxiv.org/pdf/2006.15704.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a><br><strong>参考文献：</strong>[1] PyTorch Distributed: Experiences on Accelerating Data Parallel Training <a href="https://ruipeterpan.gitbook.io/paper-reading-notes/machine-learning-systems/index/pytorch-distributed-experiences-on-accelerating-data-parallel-training">https://ruipeterpan.gitbook.io/paper-reading-notes/machine-learning-systems/index/pytorch-distributed-experiences-on-accelerating-data-parallel-training</a></p>]]></content>
    
    
    <summary type="html">本文介绍了PyTorch DDP模块的设计、实现和评估。</summary>
    
    
    
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="深度学习框架" scheme="https://diandiangu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>Elan: 面向深度学习的通用弹性训练</title>
    <link href="https://diandiangu.github.io/2021/03/07/Elan/"/>
    <id>https://diandiangu.github.io/2021/03/07/Elan/</id>
    <published>2021-03-07T07:45:42.000Z</published>
    <updated>2021-04-27T05:57:04.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Elan-Towards-Generic-and-Efficient-Elastic-Training-for-Deep-Learning"><a href="#Elan-Towards-Generic-and-Efficient-Elastic-Training-for-Deep-Learning" class="headerlink" title="Elan: Towards Generic and Efficient Elastic Training for Deep Learning"></a>Elan: Towards Generic and Efficient Elastic Training for Deep Learning</h1><p>Elasticity可以带来很多好处：（1）有利于集群管理；（2）有许多动态调整batch size，或需要动态资源的优化算法。但是深度学习框架没有很好的支持elastic training，如果在固定数量的资源上运行这些算法，甚至会导致运行效率减慢。因此，需要有一个系统很好地支持elastic training。实现这样一个系统主要由两方面的挑战：（1）如何找到一个既能保证训练效率又能保证convergence的合适的batch size；（2）如何高效进行state replication。</p><p>本文由清华大学和商汤合作，获得了ICDCS 2020的最佳论文提名奖。本文仅适用于数据并行。</p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>Elan为每一个任务设置一个application master（AM）。AM负责为调度器提供资源调整服务，并为所有的worker进行资源调整。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/1.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Elan的资源调整过程（以sclae out为例）</div></center><br><ol><li>Request：调度器请求资源调整（scale in、scale out或migration），将请求发给AM并启动新的worker；</li><li>Report：新的worker在启动并完成初始化后，将自身状态告知AM；</li><li>Coordinate：已有的worker和AM进行协作调整；</li><li>State Replication：将训练状态复制给新的worker；</li><li>State Adjustment：例如重新划分数据、重构通信组、调整batch size等。</li></ol><h2 id="混合scaling机制"><a href="#混合scaling机制" class="headerlink" title="混合scaling机制"></a>混合scaling机制</h2><h3 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h3><p>数据并行中scaling out有两种常见策略：strong scaling out（total batch size固定）和weak scaling out（local batch size固定）。前者不能很好地保证训练效率，后者不能保证训练的准确率。本文提出一种将两种方式混合的机制，既能保证效率，也能保证准确率。</p><p>对于strong scaling out来说，随着worker数量增大，训练的throughput先上升后下降。对于一个scaling out的请求，Elan首先检查，在调整后的worker数量是否超过throughput最大时的worker数量。如果不超过，则无需调整batch size；否则，将global batch size扩大为两倍，然后再将目标worker数量和新的global batch size下的optimal worker数量进行比较，反复进行这个操作。global batch size扩大的倍数不能超过实际worker数量扩大的倍数。具体算法如下：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/2.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Parallelism Scaling算法</div></center><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>Elan采用progressive linear scaling 规则调整learning rate。根据SGD，learning rate应该随着batch size的增加成比例变化。但是learning rate突然增大会导致模型不converge。因此，Elan在T个iteration的时间里，逐步调整learning rate。</p><h2 id="无IO的并发状态复制"><a href="#无IO的并发状态复制" class="headerlink" title="无IO的并发状态复制"></a>无IO的并发状态复制</h2><h3 id="训练状态的特点"><a href="#训练状态的特点" class="headerlink" title="训练状态的特点"></a>训练状态的特点</h3><p>数据(加载)、模型、通信组和优化器以及一些运行时的信息（例如当前的epoch和iteration数）共同组成数据并行任务里的训练状态。</p><p>训练状态有两个特点：（1）训练状态分布在异构的设备中；（2）GPU中的状态比CPU中的状态大得多。</p><p>由于这两个特点，状态复制的机制必须满足：（1）很好地管理状态的异构性；（2）高效复制GPU上的状态。</p><h3 id="带宽的不同"><a href="#带宽的不同" class="headerlink" title="带宽的不同"></a>带宽的不同</h3><p>PCIe连接的GPU之间有三种通信方式：P2P、CPU共享内存 (SHM) 和网络（NET; 在本文中是56Gbps Infiniband）。</p><p>集群中GPU间的通信又有四种典型的通信方式： L1：PCIe switches；L2：PCIe host bridge；L3：socket-level link (e.g., QPI)；L4：network。对于通信来说，P2P is 只能用于L1，L2和L3可以用SHM，L4只能用NET。</p><h3 id="状态复制"><a href="#状态复制" class="headerlink" title="状态复制"></a>状态复制</h3><ol><li>对于所有的worker建立拓扑树，在复制状态的时候，选择和目标worker最近的worker作为source（P2P &gt; SHM &gt; NET）；</li><li>并发地进行多个replication以充分利用带宽。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/3.png" width="40%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：向A、B、C、D中添加E、F两个新worker</div></center><br><p>此外，CPU和GPU上的状态复制是同时进行的。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>资源调整的过程是shut-down free的，且将启动和初始化新机器的过程和训练overlap起来。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/4.png" width="40%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：使用Elan scale out的时间轴</div></center><br><p>在数据并行中，进行了资源的调整后，需要对数据集进行重新划分。传统的方法是把整个数据集划分为几个部分，剩余的、还未用于训练的数据是碎片化的。Elan采用了一种序列化的方式，这样，剩余的数据就是连续的了，数据集的状态仅仅用一个index来表示未使用的数据的位置就可以了。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/5.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：chunk-based和序列化的数据加载</div></center><br><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>本文从三个角度对Elan进行评估：资源调整的overhead、动态batch size中获益以及弹性训练调度的获益。</p><h3 id="资源调整"><a href="#资源调整" class="headerlink" title="资源调整"></a>资源调整</h3><p>使用了8台服务器，各8张卡。</p><p>两个baseline：shutdown-restart和Litz（CPU-GPU上下文切换）。</p><p>Overhead定义为：如果没有进行资源调整，维持elasticity所浪费的时间（对于Elan来说指的是第四步）。S&amp;R和Elan在这方面overhead是相同的。使用Elan进行资源调整的overhead小于训练总时长的3‰。对于Litz，由于系统的设计不同（？），很难直接比较overhead的大小。作者比较了Litz和Elan的throughput，发现Litz的throughput比Elan小，进而说明Litz的方法中，overhead对训练的影响更大。</p><p>另外， 作者比较了S&amp;R和Elan migration,、scaling in和scaling out所用的时间，Elan在各种情况下都优于S&amp;R。</p><h3 id="弹性训练"><a href="#弹性训练" class="headerlink" title="弹性训练"></a>弹性训练</h3><p>用AdaBatch训练ResNet50。两个baseline：固定batch size和固定worker数量。Elan的训练效率最高且不用想模型converge。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/6.png" width="40%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：不同configuration下的训练效率</div></center><h3 id="弹性调度"><a href="#弹性调度" class="headerlink" title="弹性调度"></a>弹性调度</h3><p>Baseline：FIFO 和 Backfill (BF)。</p><p>收集了商汤的trace，并为每个job定义了min_res和max_res。</p><p>调度策略：如果有空闲的资源，则为边际增益最大的job添加GPU（类似<a href="">AFS</a>）。</p><p>实验表明，弹性调度可以提高资源利用率，且是必要的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/7.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图6：不同弹性训练系统下的平均表现</div></center><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文的工作非常全面，实验从多个角度证明了Elastic Training的优点。虽然说调度器理论上不应该任意调整用户定义好的batch size，但是除此以外挑不太出来什么明显的问题，不愧是best paper candidate。</p><hr><p><strong>原文作者：</strong>Lei Xie, Jidong Zhai, Baodong Wu, Yuanbo Wang, Xingcheng Zhang, Peng Sun, Shengen Yan<br><strong>原文链接：</strong><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9355755&amp;tag=1">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9355755&amp;tag=1</a></p>]]></content>
    
    
    <summary type="html">本文是Gandiva的后续工作，提出了一个可以保证异构GPU集群中深度学习任务效率和用户间公平性的调度方法。</summary>
    
    
    
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="ICDCS" scheme="https://diandiangu.github.io/tags/ICDCS/"/>
    
    <category term="弹性训练" scheme="https://diandiangu.github.io/tags/%E5%BC%B9%E6%80%A7%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>在异构GPU集群中权衡效率与公平性</title>
    <link href="https://diandiangu.github.io/2021/03/07/Gandiva-fair/"/>
    <id>https://diandiangu.github.io/2021/03/07/Gandiva-fair/</id>
    <published>2021-03-07T06:17:58.000Z</published>
    <updated>2021-04-28T14:26:04.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Balancing-Efficiency-and-Fairness-in-Heterogeneous-GPU-Clusters-for-Deep-Learning"><a href="#Balancing-Efficiency-and-Fairness-in-Heterogeneous-GPU-Clusters-for-Deep-Learning" class="headerlink" title="Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning"></a>Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning</h1><p>本文是Gandiva的后续工作，来自微软印度研究院，发表于EuroSys’20。类似于HiveD，本文认为，在共享集群中，DLT job的performance不能比在使用同样资源的私有集群上差；如果用户有不能充分利用的资源，没有被使用的那一部分资源要被分享给其他用户已提升集群的效率。集群中有不同性能的资源，为不同的job分配不同性能的设备是不公平的。然而已有的一部分工作即使在同构集群中也不能保证公平性，这些工作往往注重与减少JCT或者提升集群整体效率。本文提出了一个通过把为充分利用的计算资源分配给其他活跃用户来保证集群中GPU时间公平性的调度器Gandiva_fair。Gandiva_fair用“ticket”表示公平性。Gandiva_fair通过三点来同时保证公平性和效率：（1）通过中心调度器调度大型任务，每个server上的<strong>split scheduler</strong>负责调度该服务器上的小任务；（2）Gandiva_fair中的<strong>load balancer</strong>通过job迁移来把任务均匀地分布在集群中；（3）通过<strong>自动资源交易</strong>的方法解决异构性带来的效率问题。</p><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><h3 id="Split-Stride-Gang-scheduling"><a href="#Split-Stride-Gang-scheduling" class="headerlink" title="Split Stride Gang-scheduling"></a>Split Stride Gang-scheduling</h3><h4 id="小任务（单机任务）"><a href="#小任务（单机任务）" class="headerlink" title="小任务（单机任务）"></a>小任务（单机任务）</h4><p>给个job都有一个pass值，每运行一个时间单位，就运行当前pass值最小的job，pass值累加这个job的ticket数的倒数。如果某次调度的时候，剩余的GPU数量少于需要被调度的job所需GPU数量，那就先去调度下一个job，等到下一轮的时候，这个job就成为了pass最小的，它一定会被最先分配到GPU。</p><h4 id="大任务（多机任务）"><a href="#大任务（多机任务）" class="headerlink" title="大任务（多机任务）"></a>大任务（多机任务）</h4><p>对于多机任务来说，需要一次性分配好所有的GPU；对于小任务来说，每个server上单独调度小任务效率更高。因此，Gandiva_fair采用Split Stride scheduler。其中，每个server上的per-server scheduler用于调度单机任务，central scheduler用于调度多机任务和每个server上所有小任务组成的“集成任务”。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Gandiva_fair/1.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Split Stride Scheduler</div></center><h3 id="Load-Balance"><a href="#Load-Balance" class="headerlink" title="Load Balance"></a>Load Balance</h3><p>fair的定义：inter-user fairness，根据每个user的ticket的数量占总的ticket数量的比例，来分配GPU资源。local per-server scheduler可以根据ticket的比例调度每个小job的资源。在此基础上，如果能保证center scheduler对每个local per-server scheduler的资源调度是公平的，就能保证整体的公平性了。这只需每个server上的ticket数量尽量平均（每个node上的ticketLoadPerGPU尽量相等）（load balance）。</p><p>在有资源空闲，但与此同时也有等待中的任务的时候，Gandiva_fair会（1）将部分任务迁移到其他的server上；（2）考虑把任务“pack”起来（参考Gandiva）（pack不影响前面对ticketLoadPerGPU的计算）。</p><h3 id="处理异构性问题"><a href="#处理异构性问题" class="headerlink" title="处理异构性问题"></a>处理异构性问题</h3><p>仅针对调参的时候，一个用户提交大量相似的任务的情景。通过同一用户的不同job在不同类型的GPU上运行，记录效率，从而决定怎样为这个任务分配GPU是最优的。对于不同的用户，最开始给他们分配的GPU型号是相对公平的。但是因为他们的任务不同，Gandiva_fair可能会根据job的特点，在他们之间进行GPU的交易，从而使得每个用户的运行效率都有所提升。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Gandiva_fair/2.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">资源配置算法</div></center><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>Job migration：修改了PyTorch和TensorFlow框架的源码。借助了已有的process migration工具CRIU，使用了GPU和CPU之间的上下文切换和checkpoint，切换的overhead在百毫秒数量级。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>50台服务器，200块GPU。PyTorch、TensorFlow和Horovod三种框架。</p><p>首先，只用1台/两台机器和少量的任务，来验证公平性。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Gandiva_fair/3.png" width="90%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：单机/双机上用户的GPU吞吐量</div></center><br><p>在同构、异构集群无trading和异构集群有trading的情况下分别验证了公平性和效率的问题。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要解决了两件事：（1）通过调度算法和负载均衡，保证不同用户之间的公平性；（2）充分利用异构集群的特点来提高集群的效率。</p><hr><p><strong>原文作者：</strong>Shubham Chaudhary, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Srinidhi Viswanatha<br><strong>原文链接：</strong><a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/05/gandiva-fair-eurosys20.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2020/05/gandiva-fair-eurosys20.pdf</a></p>]]></content>
    
    
    <summary type="html">本文是Gandiva的后续工作，提出了一个可以保证异构GPU集群中深度学习任务效率和用户间公平性的调度方法。</summary>
    
    
    
    
    <category term="集群调度" scheme="https://diandiangu.github.io/tags/%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6/"/>
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="EuroSys" scheme="https://diandiangu.github.io/tags/EuroSys/"/>
    
  </entry>
  
  <entry>
    <title>Resources for Machine Learning System</title>
    <link href="https://diandiangu.github.io/2021/03/02/MLsystem/"/>
    <id>https://diandiangu.github.io/2021/03/02/MLsystem/</id>
    <published>2021-03-02T07:46:38.000Z</published>
    <updated>2023-03-03T08:37:35.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Meta-Papers"><a href="#Meta-Papers" class="headerlink" title="Meta Papers"></a>Meta Papers</h1><ul><li><input disabled="" type="checkbox"> <strong>How to Read a Paper</strong> [<a href="https://dl.acm.org/doi/pdf/10.1145/1273445.1273458">PDF</a>]</li><li><input checked="" disabled="" type="checkbox"> <strong>Low-level Advice for Systems Research</strong> [<a href="https://lalith.in/2020/09/27/Low-Level-Advice-For-Systems-Research/">Original Link</a>]</li><li><input checked="" disabled="" type="checkbox"> <strong>A Berkeley View of Systems Challenges for AI</strong> [<a href="https://arxiv.org/pdf/1712.05855.pdf">PDF</a>]</li><li><input checked="" disabled="" type="checkbox"> <strong>MLSys: The New Frontier of Machine Learning Systems</strong> [<a href="https://arxiv.org/pdf/1904.03257.pdf">PDF</a>]</li><li><a href="https://diandiangu.github.io/2020/12/25/survey/"><strong>综述类文章汇总</strong></a></li></ul><h1 id="课程资源"><a href="#课程资源" class="headerlink" title="课程资源"></a>课程资源</h1><ul><li><strong>MSRA</strong> <a href="https://github.com/microsoft/ai-edu">微软人工智能教育与学习共建社区</a> <a href="https://github.com/microsoft/AI-System">人工智能系统</a>（中文）</li><li><strong>陈天奇</strong> <a href="http://dlsys-course.github.io/">CSE 599W: System for ML</a>（经典）</li><li><strong>UC Berkley</strong> <a href="https://ucbrise.github.io/cs294-ai-sys-sp19/">CS294: AI For Systems and Systems For AI</a> [<a href="https://github.com/ucbrise/cs294-ai-sys-sp19">GitHub</a>]</li><li><strong>Michigan</strong> <a href="https://github.com/mosharaf/eecs598">EECS 598: System for AL (W’21)</a></li></ul><h1 id="概念补课"><a href="#概念补课" class="headerlink" title="概念补课"></a>概念补课</h1><ul><li><input disabled="" type="checkbox"> 什么是“云”？什么是“云原生”？</li></ul><ul><li><input disabled="" type="checkbox"> 什么是“软件定义”？</li></ul><ul><li><input disabled="" type="checkbox"> 什么是“disaggregation”？</li></ul><h1 id="Book-List"><a href="#Book-List" class="headerlink" title="Book List"></a>Book List</h1><ul><li><strong>System for AI</strong> <a href="https://microsoft.github.io/AI-System/">微软人工智能系统课程</a> （中文）</li><li><strong>机器学习系统</strong> <a href="https://openmlsys.github.io/index.html">机器学习系统：设计与实现</a>（中文）</li><li><strong>AutoML</strong> <a href="https://www.automl.org/book/">AutoML: Methods, Systems, Challenges (first book on AutoML)</a></li></ul><h1 id="Paper-Reading-Lists"><a href="#Paper-Reading-Lists" class="headerlink" title="Paper Reading Lists"></a>Paper Reading Lists</h1><ul><li><a href="https://diandiangu.github.io/frameworks/">分布式训练框架</a></li><li><a href="https://diandiangu.github.io/training/">训练优化</a></li><li><a href="https://diandiangu.github.io/scheduling/">集群调度</a></li><li><a href="https://diandiangu.github.io/sharing/">GPU资源利用</a></li><li><a href="https://diandiangu.github.io/elasticity/">弹性训练</a></li><li><a href="https://diandiangu.github.io/others/">其他</a></li></ul><h1 id="心态调整与研究方法"><a href="#心态调整与研究方法" class="headerlink" title="心态调整与研究方法"></a>心态调整与研究方法</h1><ul><li><a href="https://meditator-hkx.github.io/Research/pguo-PhD-grind.pdf">The PhD Grind</a></li><li><a href="/attaches/system_research.pdf">一名系统研究者的攀登之路</a></li></ul>]]></content>
    
    
    <summary type="html">总结机器学习系统领域的学习资源，以及值得关注和阅读的paper，主要涉及分布式深度学习、集群调度等。</summary>
    
    
    
    
    <category term="集群调度" scheme="https://diandiangu.github.io/tags/%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6/"/>
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="深度学习框架" scheme="https://diandiangu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>Surveys on ML/DL</title>
    <link href="https://diandiangu.github.io/2020/12/25/survey/"/>
    <id>https://diandiangu.github.io/2020/12/25/survey/</id>
    <published>2020-12-25T05:13:39.000Z</published>
    <updated>2023-04-25T08:50:23.050Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分布式机器学习"><a href="#分布式机器学习" class="headerlink" title="分布式机器学习"></a>分布式机器学习</h2><ul><li><p><strong>A Quick Survey on Large Scale Distributed Deep Learning Systems</strong> (ICPADS’18) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8644613">PDF</a>]</p><ul><li>从算法角度、分布式系统角度和应用角度分析分布式深度学习系统</li></ul></li><li><p><strong>Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques, and Tools</strong> (ACM Comput. Surv. 2019 53(1)) [<a href="https://diandiangu.github.io/2020/07/20/DistributedInfrastructures/">阅读笔记</a>][<a href="https://www.researchgate.net/profile/Ruben_Mayer/publication/332033965_Scalable_Deep_Learning_on_Distributed_Infrastructures_Challenges_Techniques_and_Tools/links/5d90f42492851c33e9488837/Scalable-Deep-Learning-on-Distributed-Infrastructures-Challenges-Techniques-and-Tools.pdf">PDF</a>]</p><ul><li>汇集、分类和比较了来自不同社区的分布式基础设施方面的大量工作</li><li>对现有的开源DL框架和工具进行了概述和比较</li><li>强调并讨论了该领域的开放性研究挑战。</li></ul></li><li><p><strong>Demystifying Parallel and Distributed Deep Learning: An In-depth Concurrency Analysis</strong> (ACM Comput. Surv. 2019 53(4)) [<a href="https://dl.acm.org/doi/pdf/10.1145/3320060">PDF</a>]</p><ul><li>深度学习中的并行和分布式</li><li>用于DNN评估的并行策略及其实现</li><li>针对支持分布式环境的训练算法和系统的扩展</li><li>对这些训练算法和系统的扩展的并发性和平均并行性的分析</li></ul></li><li><p><strong>A Survey on Distributed Machine Learning</strong> (ACM Comput. Surv. 53(2)) [<a href="https://arxiv.org/pdf/1912.09789.pdf">PDF</a>]</p><ul><li>机器学习的系统挑战，以及如何采用来自高性能计算（HPC）的思想来加速和提高可伸缩性</li><li>分布式机器学习的体系结构</li><li>最广泛使用的系统和库的生态系统及其底层设计</li><li>分布式机器学习的主要挑战</li></ul></li><li><p><strong>Communication-Efficient Distributed Deep Learning: A Comprehensive Survey</strong> (ArXiv’20) [<a href="https://arxiv.org/pdf/2003.06307.pdf">PDF</a>]</p><ul><li>在系统级探究系统为了降低通信成本进行的的设计和实现</li><li>在算法层面比较不同算法的理论收敛边界和通信复杂度</li></ul></li></ul><h2 id="深度学习调度器"><a href="#深度学习调度器" class="headerlink" title="深度学习调度器"></a>深度学习调度器</h2><ul><li><strong>Deep Learning Workload Scheduling in GPU Datacenters: Taxonomy, Challenges and Vision</strong> (ArXiv’22) [<a href="https://arxiv.org/abs/2205.11913">PDF</a>][<a href="https://github.com/S-Lab-System-Group/Awesome-DL-Scheduling-Papers">Awesome list</a>]</li></ul><h2 id="机器学习框架"><a href="#机器学习框架" class="headerlink" title="机器学习框架"></a>机器学习框架</h2><ul><li><p><strong>A Survey on Deep Learning for Big Data</strong> (J. Big Data 2018 6: 60) [<a href="https://pdf.sciencedirectassets.com/272144/1-s2.0-S1566253517X00079/1-s2.0-S1566253517305328/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjED0aCXVzLWVhc3QtMSJHMEUCICQli7LuHSc8WT3T32ooDPRkaYxuR3PfQ5sVNrQPERZ9AiEA0K1fr/yVaOw3zxrgPxcziYAk9q7KiUstik/lbZmQ3wYqvQMI5f//////////ARADGgwwNTkwMDM1NDY4NjUiDIPQspmsaW4TfpP9qiqRA/XeAonahY3FUp7arBQKkZuo+9+PibBudwcOo/wDhv8Fwi0F+c1pLYADKs8CfMLMZi5FOHayr0YCNszIAkjQemqjV+2VlddNu76Sc+oGYzBs5+p2/WD+PHlo8CZbC6PcPRMx57v/d1XzwgUzjg16Gs9PL3X4jztfx62yNe8W7qDtEUTCf3TEtvNg4qZTl0McfQlyqiOKUmDa88Pc3TRxWGo4igVH4fuQNg42hMjArtF+07EzqyEso73onIOayOh+g/Y4CFeJVvf/g+vANM00lEe75ufdEet4YAZhLo9M3Q+WsubvnpI2KFBgzx1MLMv7OS5SLnH6n9zpYFOjJXt2JLUcFEVdre3FQaheLs4sPYZzjH1Z1QPfd6zprvZP/J+kP4i5gcj1+LJ4aMLK6uQh8mrr8F+lXBusdKpVOvURElESeVrj2WNYG8BCB5oQ3quCi1G992NeP6BrwX2eg/zs/MX6olmBhrLlnDPf9lENjDpC7lF3rS2KGr+gLrZTK3wExBk5hbaNvwvIUJ5HzxduQiRfMJTQlf8FOusBivIkhmQf3IOPFPnPehjVmof1A9fD/l33jV4W1iIg2COsjdWd53mD6lPm/3x0/gOEhRW8uXHRTjv0OctmZsaqsuM107mKC1auJy7ldWSlITNn6Gzdyq1VzkOomDY5CRF4hYXoR75E67pbnuZ8ypzqQePDShSfUGl7sN+YFzcMMJGP0LlCt57xQVad5/tdyrDRBAW9jmS0FyIUy8rpgHA5gPIF+a2RxsJwH5hhF1SgDkpGF/k8dJw5OyNdEk3vyqMnw6qOmmeUR3SfWgbm+Uiu/GA+jI4C295dtWms84FGldfbR5xFLqvzEp0Tzg==&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20201225T045839Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3W64ASUD/20201225/us-east-1/s3/aws4_request&X-Amz-Signature=2fe7482479451e03541d9eeda76643075f925d11e8f278c9135b851f0f21f2be&hash=58e9266d45270e5b47ed648751ad8d84117e83a0e1656af04a4bed641fb35060&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1566253517305328&tid=spdf-ab42b773-613a-437d-bad7-68376d675327&sid=29cd82ba5cbcc749ce184bc048d206988b38gxrqa&type=client">PDF</a>]</p><ul><li>回顾大数据特征学习的深度学习模型的研究进展</li><li>指出大数据深度学习所面临的挑战，并讨论未来的课题</li></ul></li><li><p><strong>Machine Learning and Deep Learning Frameworks and Libraries for Large-Scale Data Mining: a Survey</strong> (Artif. Intell. Rev. 2019 52(1)) [<a href="https://link.springer.com/article/10.1007/s10462-018-09679-z">PDF</a>]</p><ul><li>概述了ML和DL技术以及它们的发展和新趋势</li><li>动态链接语言与加速计算的关系</li><li>最新的ML和DL框架和库：机器学习框架和没有特殊硬件支持的库、具有GPU支持的深度学习框架和库以及支持MapReduce的机器学习和深度学习框架和库</li><li>总结趋势和发展方向</li></ul></li><li><p><strong>Various Frameworks and Libraries of Machine Learning and Deep Learning: A Survey</strong> (Archives of Computational Methods in Engineering 2019) [<a href="https://link.springer.com/article/10.1007/s11831-018-09312-w">PDF</a>]</p><ul><li>比较了18个常用的深度学习框架和库并介绍了大量的benchmarking数据</li><li>从模型设计能力、接口属性、部署能力、性能、框架设计和发展前景六个方面对8个主流深度学习框架进行了打分</li></ul></li><li><p><strong>A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning</strong> (IPDPS’19) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8821020">PDF</a>]</p><ul><li>白盒测试，能够公平地分析和比较不同的DL工作负载和算法</li></ul></li></ul><h2 id="机器学习测试"><a href="#机器学习测试" class="headerlink" title="机器学习测试"></a>机器学习测试</h2><ul><li><strong>Machine Learning Testing: Survey, Landscapes and Horizons</strong> (IEEE Transactions on Software Engineering 2020) [<a href="https://arxiv.org/pdf/1906.10742.pdf">PDF</a>]<ul><li>介绍机器学习测试并介绍已有的对各种方面进行测试的方法</li><li>指出了几去学习测试面临的挑战、存在的问题和有前途的研究方向</li></ul></li></ul><h2 id="Efficient-DNN"><a href="#Efficient-DNN" class="headerlink" title="Efficient DNN"></a>Efficient DNN</h2><ul><li><p><strong>Efficient Processing of Deep Neural Networks: A Tutorial and Survey</strong></p></li><li><p><strong>Energy-Aware Scheduling for Real-Time Systems: A Survey</strong></p></li></ul>]]></content>
    
    
    <summary type="html">总结与深度学习相关的Survey，主要是分布式机器学习和深度学习框架相关。</summary>
    
    
    
    
    <category term="调研" scheme="https://diandiangu.github.io/tags/%E8%B0%83%E7%A0%94/"/>
    
    <category term="深度学习" scheme="https://diandiangu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Empirical Studies on DL and Other Areas</title>
    <link href="https://diandiangu.github.io/2020/12/25/empirical/"/>
    <id>https://diandiangu.github.io/2020/12/25/empirical/</id>
    <published>2020-12-25T02:38:22.000Z</published>
    <updated>2022-03-08T01:43:39.881Z</updated>
    
    <content type="html"><![CDATA[<h2 id="深度学习领域的Empirical-Study"><a href="#深度学习领域的Empirical-Study" class="headerlink" title="深度学习领域的Empirical Study"></a>深度学习领域的Empirical Study</h2><ul><li><p><strong>An Empirical Study on TensorFlow Program Bugs</strong> (ISSTA’18) [<a href="http://sccpu2.cse.ust.hk/castle/materials/issta18main-p98-p.pdf#:~:text=An%20Empirical%20Study%20on%20TensorFlow%20Program%20Bugs%20ISSTA'18%2C,further%20divided%20into%20two%20sub-phases%3A%20training%20and%20testing.">PDF</a>]</p><p>  <strong>内容：</strong>探究使用TensorFlow框架的程序中bug产生的原因、检测bug的挑战、用户如何应对检测bug的挑战、定位bug的挑战、用户如何应对定位bug的挑战。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据。</p></li><li><p><strong>A Comprehensive Study on Deep Learning Bug Characteristics</strong> (ESEC/FSE’19)[<a href="https://dl.acm.org/doi/pdf/10.1145/3338906.3338955">PDF</a>]</p><p>  <strong>内容：</strong>探究使用5种流行的深度学习框架会遇到的bug类型、根本原因、产生的影响、Bug出现的阶段、共性以及bug的演变。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据。</p></li><li><p><strong>An Empirical Study on Program Failures of Deep Learning Jobs</strong> (ICSE’20 <strong>Distinguished Paper Award</strong>) [<a href="https://wencongxiao.github.io/res/icse20/icse20-main-199.pdf">PDF</a>]</p><p>  <strong>内容：</strong>深度学习任务failure的种类、根本原因和现有测试/诊断方法。</p><p>  <strong>方法：</strong>通过研究微软集群中实际遇到的fault，并对6位微软的开发者进行访问。</p></li><li><p><strong>Taxonomy of Real Faults in Deep Learning Systems</strong> (ICSE’20) [<a href="https://arxiv.org/pdf/1910.11015.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究在使用深度学习系统中会遇到的错误。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据，并通过20位科研工作者/从业人员进行label。</p></li><li><p><strong>Repairing Deep Neural Networks: Fix Patterns and Challenges</strong> (ICSE’20) [<a href="https://dl.acm.org/doi/pdf/10.1145/3377811.3380378">PDF</a>]</p><p>  <strong>内容：</strong>探究在深度神经网络中中bug的fix pattern和挑战。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据。</p></li><li><p><strong>An Empirical Study on Bugs inside TensorFlow</strong> (DASFAA’20) [<a href="http://www.cs.sjtu.edu.cn/~zhonghao/paper/tfbug.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究TensorFlow框架内部bug的症状、根本原因和位置。</p><p>  <strong>方法：</strong>挖掘2017-2019年之间github上的bug。</p></li><li><p><strong>A Comprehensive Study on Challenges in Deploying Deep Learning Based Software</strong> (ESEC/FSE’20) [<a href="https://chenzhenpeng18.github.io/papers/FSE20_1.pdf">PDF</a>]</p><p>  内容：探究部署深度学习应用的挑战。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li><li><p><strong>An Empirical Study on Deployment Faults of Deep Learning Based Mobile Applications</strong> (ICSE’21) [<a href="https://chenzhenpeng18.github.io/papers/ICSE21.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究在移动端部署深度学习应用时的错误的症状和解决办法。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据。</p></li></ul><h2 id="其他领域的相关工作"><a href="#其他领域的相关工作" class="headerlink" title="其他领域的相关工作"></a>其他领域的相关工作</h2><ul><li><p><strong>A Characteristic Study on Failures of Production Distributed Data-Parallel Programs</strong> (ICSE’13) [<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/icse13seip.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究分布式数据并行程序中数据和代码的缺陷的symptom、root cause、fix以及debugging practice。</p><p>  <strong>方法：</strong>挖掘Microsoft集群中的SCOPE job。</p></li><li><p><strong>Software Documentation Issues Unveiled</strong> (ICSE’19) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8811931">PDF</a>]</p><p>  <strong>内容：</strong>对文档中的问题进行分类。</p><p>  <strong>方法：</strong>挖掘emails, github issue, GitHub pull-requests和 Stack Overflow中的数据。</p></li><li><p><strong>Going Big: A Large-Scale Study on What Big Data Developers Ask</strong> (ESEC/FSE’19) [<a href="https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1615&context=hc_pubs">PDF</a>]</p><p>  <strong>内容：</strong>探究大数据开发者所关心的主题、主题分类、主题流行度、主题难度、主题流行度和难度之间的关联度。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li></ul><ul><li><p><strong>Understanding Privacy-Related Questions on Stack Overflow</strong> (CHI’20) [<a href="https://groups.inf.ed.ac.uk/tulips/papers/tahaei2020SO.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究SO用户会将什么话题与“隐私”问题关联，以及用户为什么要参与到隐私相关话题中。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li><li><p><strong>Understanding Build Issue Resolution in Practice: Symptoms and Fix Patterns</strong> (ESEC/FSE’20) [<a href="https://chenzhenpeng18.github.io/papers/FSE20_2.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究构建（build）中用户所关心的话题、遇到问题的症状及根本原因。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li><li><p><strong>Interpreting Cloud Computer Vision Pain-Points: A Mining Study of Stack Overflow</strong> (ICSE’20) [<a href="https://arxiv.org/pdf/2001.10130.pdf">PDF</a>]</p><p>  <strong>内容：</strong>分析探究Cloud CV领域研究者遇到的痛点。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li><li><p><strong>An Empirical Study on API Usage</strong> (TSE’19) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8186224">PDF</a>]</p></li><li><p><strong>An Empirical Study on API-Misuse Bugs in Open-Source C Programs</strong> (COMPSAC’19) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8754426">PDF</a>]</p></li></ul>]]></content>
    
    
    <summary type="html">总结与深度学习相关的Empirical Study，以及其他领域中类似的Empirical Study。</summary>
    
    
    
    
    <category term="深度学习" scheme="https://diandiangu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="软件工程" scheme="https://diandiangu.github.io/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
    <category term="实证研究" scheme="https://diandiangu.github.io/tags/%E5%AE%9E%E8%AF%81%E7%A0%94%E7%A9%B6/"/>
    
  </entry>
  
  <entry>
    <title>HiveD：新的多租户GPU集群管理方案</title>
    <link href="https://diandiangu.github.io/2020/12/22/HiveD/"/>
    <id>https://diandiangu.github.io/2020/12/22/HiveD/</id>
    <published>2020-12-22T09:47:02.000Z</published>
    <updated>2021-04-27T05:56:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HiveD-Sharing-a-GPU-Cluster-for-Deep-Learning-with-Guarantees"><a href="#HiveD-Sharing-a-GPU-Cluster-for-Deep-Learning-with-Guarantees" class="headerlink" title="HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees"></a>HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees</h1><p>随着深度学习训练需求的不断扩张，很多组织和机构都会选择自建多租户集群来共享昂贵的 GPU 资源。然而现有的GPU集群管理方案因为使用了GPU配额（Quota）机制，可能导致严重的共享异常（Sharing Anomaly）现象：某些租户的深度学习任务甚至比在私有集群中性能更差。为了从根本上解决该问题，微软亚洲研究院和微软（亚洲）互联网工程院、北京大学、香港大学合作提出了一个新的多租户GPU集群管理方案HiveD，通过新的资源抽象和调度框架从而100% 保证共享安全（Sharing Safety），同时不失一般性地和任何任务调度策略兼容。</p><h2 id="背景和动机"><a href="#背景和动机" class="headerlink" title="背景和动机"></a>背景和动机</h2><p><strong>多租户集群（multi-tenant cluster）：</strong>HiveD的目标场景是一个集群里有多个tenants（比如公司里不同的团队/部门），每个tenant会贡献一定的资源，这部分贡献的资源就是所谓的私有集群（private cluster）。要保证这些tenants有意愿去共享资源的前提就是，它在共享集群中的性能应该不差于单独拥有一个私有集群时的性能，否则tenant会prefer单独占有这些资源。这个概念在调度领域也叫sharing incentive。</p><p><strong>传统的对多租户GPU集群的管理方法：</strong>用户申请一定数量的GPU（quota）。为了提高训练速度，用户通常对一个深度学习任务有<strong>affinity requirement</strong>，即对资源分布的要求（例如，一个需要64张卡的任务需要跑在8台机器各8张卡上）。在多租户集群中没有满足要求的资源的时候，资源管理器可以选择排队等待或用更宽松的affinity requirement。</p><p><strong>共享异常：</strong>和内存管理中的外部碎片类似。有时候多租户集群中有足够的资源数量，但是这些资源很“碎片化”，无法满足用户的“affinity”的需求，这样的现象被称为“共享异常”。</p><p><strong>HiveD</strong>：通过一种调度的方法减少全局资源碎片，从而缓解共享异常的现象。但是，减少碎片可能会增大不同任务之间的影响，从而使得任务的训练速度变慢。HiveD不用quota来描述tenant的请求，而是用cell来描述VC（Virtual Cluster）的结构，并给每一个用户分配VC，这样的描述方法可以很好的描述出VC的结构。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>HiveD采用了两层的架构：第一层是Virtual Private Cluster虚拟私有集群，这一层为用户提供了一个私有集群的“假象”；第二层是实际的集群，从虚拟私有集群（VC）到实际的集群中的设备之间存在着映射关系，如下图所示。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/1.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：系统架构：两层设计</div></center><br><p>HiveD的这两个layer上各有两点设计/贡献：</p><ul><li><p>VC</p><ul><li>（和quota相比）提出了一种新的对资源抽象的方式——cell，使用这种资源抽象可以同时描述GPU资源的quota和affinity。</li><li>在VC层面，可以使用现有的深度学习调度器对资源进行调度。</li></ul></li><li><p>From Virtual to Physical</p><ul><li>提出了可以保证共享安全的dynamic cell allocation算法。</li><li>支持low-priority jobs 。</li></ul></li></ul><p>其中，用cell对资源进行抽象以及cell allocation算法可以保证HiveD中的共享安全。在VC层面对调度器的使用可以保证调度效率。对low-priority jobs的支持保证了整个集群中的资源利用率。</p><h3 id="带有cell的虚拟私有集群"><a href="#带有cell的虚拟私有集群" class="headerlink" title="带有cell的虚拟私有集群"></a>带有cell的虚拟私有集群</h3><p>为了对私有GPU集群进行描述，HiveD定义了一个多层cell结构的层次结构。某一层次上的cell，是具有相应拓扑连接的GPU的对应集合。在每个虚拟私有集群（VC）使用每个级别上的cell数来模拟/描述相应的私有集群。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/2.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：cell的5个层级</div></center><br><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/3.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：多级cell分配的例子</div></center><br><p>在图3这个例子中，一共有四个层级的cell结构，图中用不同颜色框出了租户A、B和C的一种可能的GPU分配情况，但这不是唯一可行的方案。</p><p>在cell层次结构中，k级的cell由一组（k−1）级的cell组成。这些（k−1）级的cell称为伙伴单元（buddy cells）；伙伴单元可以合并为下一个更高级别的cell。本文假设cell具有层次一致的可组合性：（i）所有k级cell在满足租户对k级cell的请求方面是等价的，并且（ii）所有k级dell都可以拆分成相同数量的（k−1）级的cell。</p><h3 id="Buddy-Cell-Allocation算法"><a href="#Buddy-Cell-Allocation算法" class="headerlink" title="Buddy Cell Allocation算法"></a>Buddy Cell Allocation算法</h3><p>Buddy Cell Allocation算法其实非常的简单直观：如果需要在VC中分配一个k级cell，算法从k级开始，首先检查是否有可用的k级cell，如果有可用的k级cell则分配一个。否则，算法将逐级上移，去检查(k+1)级或者更高级别的cell，直到有一个可用的l级cell，其中l&gt;k。然后，算法将递归地将一个空闲的l级cell拆分为多个较低级别的cell，直到k级cell可用为止。每次拆分都会在下一个较低级别生成一组buddy cells，这些cell将添加到该较低级别的空闲列表中。</p><p>如果有cell被释放的话，该算法会按照与分配cell相反的方式把空闲的cell尽量merge起来，这样保证空闲的cell中有尽量多的更高level的cell。作者通过数学归纳法证明了这一算法可以保证：如果租户原有的VC可以满足一个cell分配，使用该算法在多租户集群中这样的cell分配也一定可以被满足。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/4.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">算法1：Buddy Cell Allocation算法</div></center><br><p>该算法可以扩展到支持低优先级的作业（即opportunistic jobs），被低优先级的作业占有的cell可以被高优先级作业抢占。支持这种低优先级的job有助于提高GPU的整体利用率，同时又不损害为VC提供的共享安全保证。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>HiveD已经被继承到了基于Kubernetes的开源深度学习训练平台——OpenPAI。HiveD已经在微软已经使用了1年多的时间，用于管理超过1000块异构GPU，为科研和产业workload提供服务。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>作者首先在96块GPU的集群上进行试验：本文在分别使用了三种state-of-the-art的调度器应用于VC上，进行资源调度。通过比较可以看出，与使用Quota相比，HiveD可以有效消除共享异常；和原始的私有集群相比，任务的queuing delay有明显减小；与此同时，和直接在整个集群上使用调度器相比，任务完成时间（JCT）是差不多一样的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/5.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：租户prod-a的平均排队延迟</div></center><br><p>作者还从一个具有2232块GPU、11个租户的集群中手机了2个月的trace，基于trace用96块GPU对这样一个大规模集群上的运行情况进行模拟。在这种情况下，使用HiveD的queuing delay依旧是最短的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/6.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：租户prod-e的排队延迟</div></center><br><p>除此以外，作者还对GPU affinity需求和共享安全的权衡等问题进行了的讨论，也通过模拟实验评估了buddy cell allocation算法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>HiveD通过一系列方法应对共享多租户GPU集群的挑战：（i）保障了用户的共享安全，（ii）提出一种新的资源抽象单元cell，用来来模拟虚拟私有集群，（iii）提出了buddy cell allocation算法，该算法被证明能够保证共享安全，并可以扩展到支持低优先级作业，以及（iv）设计一个灵活的体系结构，以结合最新技术共享安全和调度效率的调度程序。</p><p>总的来说，HiveD做的事情就是给各个tenant一个正在使用自己私有集群的假象，同时赋予tenants使用自己VC以外的资源来从共享集群中获益的能力。这两方面目标由安全性的定义、cell和VC的抽象以及buddy cell allocation以一个统一的方式实现了。</p><p>本文方法仍存在一定的问题。即使使用HiveD，集群中仍然可能会存在碎片。对于HiveD是否能真的像作者提到的那样“完全消除共享异常”，有待讨论。<del>另外cell或许并不能完全准确地描述affinity的需求（不能全面描述GPU之间的拓扑关系）。</del></p><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><ul><li><p><strong>Q：</strong>Tenant是什么？<br>  <strong>A：</strong>目标的场景就是说一个集群里有多个tenants，比如不同的团队/部门，这在真实场景中都是非常常见的。每个tenant会贡献一定的资源，这部分贡献的资源就是我们所谓的私有集群（private<br>cluster）。要保证这些tenants有意愿去共享资源的前提就是，它在共享集群中的性能应该不差于单独拥有一个私有集群时的性能，否则tenant会prefer单独占有这些资源。这个概念在调度领域也叫sharing<br>incentive。</p></li><li><p><strong>Q：</strong>当一个VC中的全部job都结束之后，HiveD是否会继续维护这个VC？<br>  <strong>A：</strong>一般来说，VC和job是无关的。如上面所说，VC本质上就是建模一个tenant原本的私有资源。它跟里面跑什么job没有必然联系，job结束了VC也还在。当然我们支持对VC的reconfiguration，未来可能也会有动态和自动的reconfiguration。</p></li><li><p><strong>Q：</strong>HiveD加入了VC这个新抽象以后，VC上的scheduler能知道它能用的GPU之间的拓扑结构之类的信息？<br>  <strong>A：</strong>每个VC分配的是cell，cell的层次结构里就包含了GPU的topo，GPU本身是leaf cell。Cell是一个tree 的层次结构，所以就算不在一个server里也是有topo的，比如图2 level-5这个cluster里的一个rack cell，里面包含了2个node cell，然后每个node cell下有2个CPU cell，每个CPU cell有两个PCIe switch cell，每个PCIe switch cell下有2个GPU cell，GPU cell是leaf cell。如果config的最高level的cell就是node level，没有上面的rack和cluster了，那几个node cell 之间就没有topo（比如不知道是一个rack下），这个取决于cell的配置里有没有。Cell是tree，所以scheduler里存的也是 tree，除了数量以外层次结构也都在，所以scheduler能知道任意两个cell在 tree 里的位置。</p></li></ul><hr><p><strong>原文作者：</strong>Hanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang, Fan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau, Yuqi Wang, Yifan Xiong, Bin Wang<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/osdi20-zhao_hanyu.pdf">https://www.usenix.org/system/files/osdi20-zhao_hanyu.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/microsoft/hivedscheduler">https://github.com/microsoft/hivedscheduler</a><br><strong>参考文献：</strong>[1] OSDI2020——SJTU-IPADS的“云见闻”（二） <a href="https://mp.weixin.qq.com/s/GzQSfBYhcU8XccnWV7epKg">https://mp.weixin.qq.com/s/GzQSfBYhcU8XccnWV7epKg</a><br>[2] OSDI 2020 | 微软亚洲研究院论文一览 <a href="https://mp.weixin.qq.com/s/vHeXvAeEPPiTls349yjkAQ">https://mp.weixin.qq.com/s/vHeXvAeEPPiTls349yjkAQ</a></p><p>特别感谢原文作者赵汉宇师兄和熊一帆师兄对HiveD设计的讲解！</p>]]></content>
    
    
    <summary type="html">随着深度学习训练需求的不断扩张，很多组织和机构都会选择自建多租户集群来共享昂贵的 GPU 资源。然而现有的 GPU 集群管理方案因为使用了 GPU 配额（Quota）机制，可能导致严重的共享异常（Sharing Anomaly）现象：某些租户的深度学习任务甚至比在私有集群中性能更差。为了从根本上解决该问题，本文提出了一个新的多租户GPU集群管理方案HiveD，通过新的资源抽象和调度框架从而100% 保证共享安全（Sharing Safety），同时不失一般性地和任何任务调度策略兼容。</summary>
    
    
    
    
    <category term="集群调度" scheme="https://diandiangu.github.io/tags/%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6/"/>
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="机器学习系统" scheme="https://diandiangu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="OSDI" scheme="https://diandiangu.github.io/tags/OSDI/"/>
    
  </entry>
  
  <entry>
    <title>网络是分布式训练的瓶颈吗？</title>
    <link href="https://diandiangu.github.io/2020/12/22/sigcomm20jin/"/>
    <id>https://diandiangu.github.io/2020/12/22/sigcomm20jin/</id>
    <published>2020-12-22T09:08:59.000Z</published>
    <updated>2021-03-07T07:44:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Is-Network-the-Bottleneck-of-Distributed-Training"><a href="#Is-Network-the-Bottleneck-of-Distributed-Training" class="headerlink" title="Is Network the Bottleneck of Distributed Training?"></a>Is Network the Bottleneck of Distributed Training?</h1><p>近期有许多工作致力于提高分布式训练的通信效率，但是很少有工作致力于系统地研究网络是否是瓶颈、以及网络在多大程度上限制了分布式训练的效率。在这篇文章中，作者度量并分析了分布式训练的网络表现。作者预期，度量结果会证实通信是阻碍分布式训练达到linear scale-out效果的原因。但是，作者发现实际上网络带宽利用率很低，如果网络带宽可以被充分利用，分布式训练的scaling factor可以接近于1。</p><p>另外，目前很多人提出压缩率超过了100的梯度压缩方法，作者发现在充分利用起来带宽的情况下，在100Gbps带宽下没有梯度压缩的必要。另一方面，10Gbps的低带宽下仅需要2-5倍的梯度压缩率就可以达到接近linear layout的效果。和梯度压缩这种应用层级的方法相比，网络层级的优化不需要对应用作出改变也不会有模型表现的损失。因此，作者提倡对分布式的网络社区做出真正的改变，以充分利用网络能力，达到linear scale-out。</p><p>本文由约翰斯·霍普金斯大学金鑫老师的团队发表于SIGCOMM’20，目前金鑫老师已就职于北京大学软件工程研究所。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>DNN模型越来越大，越来越深。然而，与快速增长的DNN模型需求相比，单芯片提供的计算能力仍然有限。因此，训练大型DNN模型不可避免地通过扩展（scale out）得到越来越多的分布。横向扩展的最优目标都是线性扩展（linear scalability）。也就是说，假设单个设备的吞吐量是T，那么具有n个设备的系统的吞吐量应该是nT。让具有n个设备的系统实际实现的吞吐量为Tn。我们将<strong>scaling factor</strong>定义为：<br>$$<br>scaling  factor = \frac{T_{n}} {nT}<br>$$<br>线性扩展要求任何n的比例因子都为1。</p><p>人们普遍认为，网络带宽是阻碍分布式训练提前扩展的瓶颈。因为计算阶段是并行的，因为每个线程都独立地处理自己的batch，所以n个worker的吞吐量是一个worker的n倍，因此只有通信阶段才能减慢训练过程。针对这一点，最近几年，机器学习和系统界对提高分布式训练的通信效率进行了大量的研究。这些工作主要在应用层完成，且假设网络已经尽了最大的通信效率。然而，很少有工作系统地理解网络是否是瓶颈以及在多大程度上是瓶颈。</p><p>本文首先分析了分布式训练的原理和方法：对几种具有代表性的DNN模型在AWS上的训练吞吐量进行了measurement study。实验结果证实了通信是阻止分布式训练线性扩展的因素。然而，作者发现，网络带宽其实并不是瓶颈，因为带宽的利用率很低。</p><p>基于记录结果，作者做了一个假设分析，在该分析中，作者控制网络带宽并对假设带宽被充分利用。分析结果表明，在充分利用网络的情况下，分布式训练可以达到99%以上的scaling factor。作者通过梯度压缩进一步扩展了假设分析。</p><p>本文的两个主要贡献点：</p><ul><li>系统地测量和分析了分布式训练的性能瓶颈。与一般的观点相反，作者揭示了主要问题不在于网络速度，而在于通信阶段的软件实现问题。</li><li>进行假设分析，以评估高性能网络传输对分布式训练带来的好处。作者认为真正的挑战是网络社区为分布式训练开发高性能的网络传输，以充分利用网络容量并实现线性扩展。</li></ul><h2 id="记录训练的performance"><a href="#记录训练的performance" class="headerlink" title="记录训练的performance"></a>记录训练的performance</h2><h3 id="Set-Up"><a href="#Set-Up" class="headerlink" title="Set Up"></a>Set Up</h3><ul><li><p><strong>硬件：</strong>Amazon EC2 p3dn.24xlarge instances with 8 GPUs (NVIDIA Tesla V100)。</p></li><li><p><strong>软件：</strong> Horovod 0.18.2, PyTorch 1.3.0, Torchvision 0.4.1, NCCL 2.4.8, cuDNN 6.6.0.64, and Open MPI 4.0.2. Horovod, NCCL, and Open MPI 采用 Linux kernel TCP通信。</p></li><li><p><strong>WorkLoad：</strong>ResNet50、 ResNet101和 VGG16三种模型，ImageNet数据集。</p></li></ul><h3 id="当前的scaling-factor"><a href="#当前的scaling-factor" class="headerlink" title="当前的scaling factor"></a>当前的scaling factor</h3><p>作者采用单个GPU上的吞吐量作为基准吞吐量T。实验中，改变服务器的数量。对于每种情况，测量服务器可以实现的总吞吐量，并计算scaling factor。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/1.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：不同数量的服务器下的scaling factor</div></center><br><p>图1显示了不同服务器数量下每个模型的scaling factor。结果表明，ResNet50、ResNet101和VGG16的scaling factor分别为75.05%、68.92%和55.99%，4台服务器分别为74.24%、66.28%和63.01%，8台服务器分别为71.6%、66.99%和59.8%。ResNet50实现了比ResNet101和VGG16更好的scaling factor，因为ResNet50模型相对比较小，通信负担轻。然而，对于这三种模型，Horovod在AWS上无法达到超过76%的比例因子。</p><p>这些结果证实了目前像Horovod这样的现成的分布式训练框架不能实现linear scaling，且与linear scaling存在着很大的差距。</p><h3 id="计算是瓶颈吗？"><a href="#计算是瓶颈吗？" class="headerlink" title="计算是瓶颈吗？"></a>计算是瓶颈吗？</h3><p>在计算部分，每个worker将一个batch的label好的图像输入神经网络模型，并局部计算梯度。<strong>如果一个worker完成一个batch的计算时间随着worker数量的增加而增加，那么计算就是分布式训练的瓶颈。</strong></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：不同数量的服务器下的计算时间</div></center><br><p>图2显示了具有不同worker数量的三个模型的计算时间（正向和反向传播）。可以看到计算时间几乎相同。单个GPU和多个GPU之间的时间差主要来自两个因素。首先，分布式训练中反向传递的运行时间不仅包括backward op，而且还包括所有的reduce操作，因为它们在GPU上是异步的，并且是重叠的。然而，对于单个GPU的情况，不存在所有的reduce操作。其次，Horovod在分布式训练过程中为模型中的每一层加了一个hook，这在单个GPU训练中是不存在的。然而，即使我们认为这些计算时间的gap是不可避免的，scaling factor仍应在90%左右，而不是测量出来的56%-75%，因为在分布式训练中测量的计算时间最多增加15%。因此，作者认为计算时间的gap不是分布式训练不能linear scaling的一个因素。</p><h3 id="网络是瓶颈吗？"><a href="#网络是瓶颈吗？" class="headerlink" title="网络是瓶颈吗？"></a>网络是瓶颈吗？</h3><p>由于计算所花费的时间与服务器数量无关，因此唯一的可能性是通信组件是系统扩展时的瓶颈。为了验证这种情况，作者首先测量不同网络带宽下的scaling factor。如图3所示，ResNet50的scaling factor随着网络带宽的增加而增加。在两台服务器的情况下，当带宽从1 Gbps增加到10 Gbps时，扩展因子从13%增加到68%。这是可以理解的，因为带宽越高，worker之间交换相同数量的数据所需的时间就越短。基于all-reduce算法，由于要交换的数据更多，所以随着worker数量的增加，scaling factor会降低。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/3.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：ResNet50模型的scaling factor随网络带宽的变化</div></center><br><p>大家普遍认为网络太慢时发送梯度太慢从而限制训练效率。然而，与之相反的事，图3显示网络在25 Gbps后趋于稳定。<strong>这意味着系统不能从更快的网络中获益。</strong>为了验证这一点，作者通过记录实时网络吞吐量来测量服务器的网络利用率。图4表明，服务器确实在低带宽（例如1 Gbps）下充分利用网络，但在高带宽（例如100 Gbps）下，它们只使用很小一部分带宽。这意味着仅仅增加带宽以使网络更快，对于提高某一点后的sclaing factor是没有用的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/4.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：网络带宽利用率</div></center><br><p>在高带宽下利用率低的一种可能性是CPU是瓶颈，因为实验在TCP上运行Horovod，而且以100Gbps这样的高速运行TCP是CPU密集型的。然而，分布式训练的计算大多是由GPU完成的，而且大多数GPU都配备了足够多的CPU。通过实验，作者发现在分布式训练时，CPU的利用率很低，因此CPU不是充分利用100Gbps网络带宽的瓶颈。</p><p><strong>总之，实验结果证实了通信是瓶颈。但与普遍的看法不同，这并不是因为网络发送数据太慢。根本原因在于网络传输的实现不佳，导致不能充分利用可用带宽。</strong></p><h2 id="假设分析"><a href="#假设分析" class="headerlink" title="假设分析"></a>假设分析</h2><h3 id="如果带宽被充分利用，会发生什么？"><a href="#如果带宽被充分利用，会发生什么？" class="headerlink" title="如果带宽被充分利用，会发生什么？"></a>如果带宽被充分利用，会发生什么？</h3><p>作者首先分析，如果网络得到充分利用，可以达到什么样的scaling factor。为了进行假设分析，作者通过hook记录了模型不同层的梯度计算时间。</p><p>作者进行了两个模拟进程：对backward的模拟，和对all-reduce的模拟。两个进程通过一个message queue来通信。backward进程通过log记录的数据模拟backward计算。在某一层的backward计算完成后，它不会立刻向all-reduce进程发送请求，而是把几层的梯度放到buffer里，这里才用了Horovod fusion buffer的strategy。buffer满了或者time out之后才进行all reduce。数据传输的时间用(2S(N-1)/N)/bw来预估，S为all reduce size。向量相加的时间用(N-1)*AddEst(S/N)来预估，AddEst(x)通过在GPU上profile等方式预估。</p><p>overhead由 $t_overhead =t_sync −t_back$ 预估，scaling factor由 $f_sim =tbatch/(t_batch +t_overhead)$ 预估。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/5.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：不同带宽下模拟得到的scaling factor v.s. 测量得到的scaling factor</div></center><br><p>图5中模拟得到的scaling factor和实际测量得出的scaling factor的对比说明，网络在低带宽下得到了充分利用；在高带宽下（即在25 Gbps之后），两条线路开始明显偏离。</p><p>作者还使用假设分析来评估不同worker数量下的scaling factor。假设网络被充分利用，即使是64个GPU，这三个模型都可以达到接近100%的scaling factor。总的来说，假设分析证实了分布式训练可以从高网络带宽中获益，而且如果充分利用网络，scaling factor可以提高到接近100%。</p><h3 id="应用层级优化有多有用？"><a href="#应用层级优化有多有用？" class="headerlink" title="应用层级优化有多有用？"></a>应用层级优化有多有用？</h3><p>在这个subsection的实验保持其他模拟步骤与之前提到的相同中相同，但将梯度传输的时间成本除以梯度压缩比。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/6.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图6：不同压缩率下模拟得到的scaling factor</div></center><br><p>梯度压缩可能会降低向量加法的成本，从而进一步提高模拟得到的scaling factor。但是，如图所示，模拟的结果说明我们可能不需要像一些过去的工作中所说的那样需要高压缩比。对于VGG16这样的模型来说，10×的压缩比足够大，在10Gbps网络中可以获得接近100%的scaling factor。100 Gbps的结果也表明压缩在高速网络中没有多大用处。因此，梯度压缩技术在低速网络中是有用的，但是在现代网络环境中不需要有很大的压缩比。</p><h2 id="讨论与未来工作"><a href="#讨论与未来工作" class="headerlink" title="讨论与未来工作"></a>讨论与未来工作</h2><p><strong>研究结果背后的基本原理</strong> 如果网络得到充分利用，scaling factor可以接近100%，这个发现是合理的。首先，很大的网络带宽下，传输ResNet50、ResNet101和VGG16的所有参数仅需7.8 ms、13.6 ms和42.2 ms。第二，在计算和通信之间有很大的overlap。最后一层的所有reduce可以在backward计算完最后一层的梯度后立即开始，而不必等待整个backward过程完成。所以，结合高效的通信和计算与通信的overlap，scaling factor可以达到接近100%</p><p><strong>应用层优化的权衡</strong> 假设分析表明，应用层中的梯度压缩只在低网络带宽下笔比较有效。作者认为，它对云上的分布式训练或配备GPU或TPU的本地集群没有特别大的用处。</p><p>作者还认为，在如今的分布式训练中，网络带宽不是主要问题，主要问题在于网络传输是否有很好的实现。</p><hr><p><strong>原文作者：</strong>Zhen Zhang, Chaokun Chang, Haibin Lin, Yida Wang, Raman Arora, Xin Jin<br><strong>原文链接：</strong><a href="https://xinjin.github.io/files/NetAI20_Training.pdf">https://xinjin.github.io/files/NetAI20_Training.pdf</a></p>]]></content>
    
    
    <summary type="html">在这篇文章中，作者度量并分析了分布式训练的网络表现。作者预期，度量结果会证实通信是阻碍分布式训练达到linear scale-out效果的原因。但是，作者发现实际上网络带宽利用率很低，如果网络带宽可以被充分利用，分布式训练的scaling factor可以接近于1。</summary>
    
    
    
    
    <category term="分布式训练" scheme="https://diandiangu.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="SIGCOMM" scheme="https://diandiangu.github.io/tags/SIGCOMM/"/>
    
    <category term="网络通信" scheme="https://diandiangu.github.io/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"/>
    
  </entry>
  
</feed>
