<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>BPipe: 面向大语言模型训练的内存均衡的流水线并行</title>
      <link href="2023/09/25/BPipe/"/>
      <url>2023/09/25/BPipe/</url>
      
        <content type="html"><![CDATA[<h1 id="BPIPE-Memory-Balanced-Pipeline-Parallelism-for-Training-Large-Language-Models"><a href="#BPIPE-Memory-Balanced-Pipeline-Parallelism-for-Training-Large-Language-Models" class="headerlink" title="BPIPE: Memory-Balanced Pipeline Parallelism for Training Large Language Models"></a>BPIPE: Memory-Balanced Pipeline Parallelism for Training Large Language Models</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>流水线并行为每个GPU分配一个单独的流水线阶段（stage），用于计算模型中的不同层。因此，每个流水线阶段都具有对其他阶段的数据依赖性，并导致计算暂停直到所需的数据到达，通常称为“流水线气泡”（bubble）。为了最大限度地减少气泡，1F1B（一个正向和一个反向）流水线将输入micro-batch，并交替处理正向计算和反向计算。为了使所有流水线阶段饱和，早期阶段比后期阶段保留更多的内存用于计算更多的前向微批。因此，流水线阶段之间存在内存不平衡，如果早期阶段内存不足，则执行模型失败，如图1所示。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/1.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：BPIPE如何处理内存不足问题的说明</div></center><br><p>作者观察到，后期阶段无法使用与早期阶段相同大小的GPU内存来预计算前向micro batch。因此，如果我们能够利用后期的空闲内存作为前期的额外内存，那么均衡的内存负载将缓解内存压力。此外，降低的内存压力使我们能够利用更多的内存来加速训练，避免冗余的重新计算、增加微批量大小或降低模型并行度。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="激活值平衡"><a href="#激活值平衡" class="headerlink" title="激活值平衡"></a>激活值平衡</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/2.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：8个微批次的4路1F1B流水线并行</div></center><br><p>在具有m个微批次的p路流水线并行中，用A(s)表示第s个阶段最多存储的激活值的内存量，W(s)为包括优化器状态在内的模型参数大小，则第s个阶段的内存使用量M(s) = A(s) + W(s)。假如没有一个阶段的模型层数相同，则W(s)为一个恒定的值W_0。μ(s)为第s个阶段最多能保存的微批次数量，A(s) = A0μ(s)，M(s) = W0 + A0μ(s)。</p><p>根据图2，1F1B流水线中的每一个阶段最多通信μ(s) = min(p − s, m)次。实际上，为了让所有流水线的阶段饱和，m远大于p。因此，μ(s) = p − s。</p><p>作者将阶段s和阶段p−s−1配对，每一对中的内存不均衡可以被写为：M(s) − M(p − s − 1) = A_0(p − 2s − 1)。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/3.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：激活平衡的图示</div></center><br><h3 id="传输调度"><a href="#传输调度" class="headerlink" title="传输调度"></a>传输调度</h3><p>为了达到内存均衡的目标，不同阶段之间数据的传输需要调度，调度算法决定何时传输或者加载什么数据。算法的核心思想是：（1）保证μ(s)不超过优化的最优值；（2）反向传播计算需要某个激活值，但是该激活值不在相应的GPU上的时候，去其配对的GPU上加载激活值；（3）如果某次激活值的加载会使得μ(s)超过优化的最优值，则算法会提前传输出去一个激活值，以此预留出足够的内存。</p><h3 id="“阶段对”的GPU分配"><a href="#“阶段对”的GPU分配" class="headerlink" title="“阶段对”的GPU分配"></a>“阶段对”的GPU分配</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/4.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：标准GPU分配和BPipe的配对分配</div></center><br><p>如图4所示，BPipe把配对的一对GPU分配在相邻的位置，这样可以减小激活值传输的时间，从而让激活值传输的时间被计算的时间完全overlap，从而减小BPipe内存优化对计算性能的影响。这样虽然会让训练时不同stage之间的通信变慢，但是内存平衡的通信时间比训练时不同stage数据依赖的通信量更大。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>作者通过分别的CUDA stream进行激活值的传输和加载，为激活值手动管理CUDA内存。作者在6台具有8个A100 GPU的服务器上进行了实验，实验中训练了GPT-3模型。</p><p>实验结果表明，BPipe按照Megatron-LM无法执行的配置进行GPT-3的训练，比Megatron-LM执行最快的配置快1.25倍。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BPipe/5.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：标准化后的训练吞吐量</div></center><br><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文提出了BPipe，通过在流水线阶段之间传输中间激活的激活平衡，BPIPE解决了流水线并行的内存不平衡问题。实验评估表明，BPipe通过执行更快的训练配置来加快大规模GPT-3模型的训练，并且如果没有BPipe，这样的配置无法运行起来。</p><hr><p><strong>原文作者：</strong>Taebum Kim, Hyoungjoo Kim, Gyeong-In Yu, Byung-Gon Chun<br><strong>原文链接：</strong><a href="https://openreview.net/pdf?id=HVKmLi1iR4">https://openreview.net/pdf?id=HVKmLi1iR4</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> ICML </tag>
            
            <tag> 大模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lucid：一个可扩展、可解释的实用型深度学习作业调度器</title>
      <link href="2023/04/25/Lucid/"/>
      <url>2023/04/25/Lucid/</url>
      
        <content type="html"><![CDATA[<h1 id="Lucid-A-Non-intrusive-Scalable-and-Interpretable-Scheduler-for-Deep-Learning-Training-Jobs"><a href="#Lucid-A-Non-intrusive-Scalable-and-Interpretable-Scheduler-for-Deep-Learning-Training-Jobs" class="headerlink" title="Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs"></a>Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs</h1><p>本文由南洋理工大学与上海人工智能实验室合作，发表于ASPLOS 2023。</p><p>现有的面向深度学习的调度器存在两个方面、五点问题：</p><ul><li>为了获得更好的系统性能，大多数现有的方法依赖于支持抢占的调度：<ul><li>有些调度器要求用户导入特定的库并修改代码来实现这些机制</li><li>有些调度器需要修改底层DL框架</li><li>很多调度器需要适应性地调整batch size和learning rate，影响训练的质量</li></ul></li><li>许多调度器在实际部署中也存在较大问题：<ul><li>可扩展性有限</li><li>决策不透明，难以调整</li></ul></li></ul><h2 id="机会"><a href="#机会" class="headerlink" title="机会"></a>机会</h2><ol><li>对于大部分情况来说，如果把两个job打包放到一个GPU上执行，互相的影响不大；如果考虑混合精度训练中的AMP，packing对训练速度的影响更小：<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"  src="/img/Lucid/1.png" width="55%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图1：Motivation</div></center></li></ol><br><ol start="2"><li><p>无需修改DL框架进行packing，训练速度也足够好了。</p></li><li><p>很多job都是重复提交的。可以通过profiling寻找和其相似的以前提交过的job。</p></li></ol><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>Lucid 遵循三个设计原则:</p><ul><li>非侵入性。整个调度工作流遵循无抢占方式，不需要用户操作和 DL 框架修改。</li><li>可伸缩性。该系统可以快速获得大量复杂工作负载的调度策略。</li><li>可解释性。所有的模块都是透明的，可以由集群运营商进行清晰的调整。</li></ul><p>此外，Lucid 还提高了资源利用率，并提供了及时的调试反馈。我们未来的工作旨在为更多的调度目标服务，比如公平性和服务水平保证。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Lucid/cover.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：System Overview</div></center><br><p>蓝色块为关键调度器模块，紫色块为用于性能增强和维护的两个系统优化器。工作流：分析作业，记录作业资源使用情况等信息，并把作业分类；绑定器判断是否以及如何打包作业；资源编排器根据概要分析和用户提供的特性，为每个作业分配一个优先级值，并选择要分配的作业。</p><h3 id="非侵入的Job-Profiler"><a href="#非侵入的Job-Profiler" class="headerlink" title="非侵入的Job Profiler"></a>非侵入的Job Profiler</h3><h3 id="Affine-作业对-绑定器"><a href="#Affine-作业对-绑定器" class="headerlink" title="Affine 作业对 绑定器"></a>Affine 作业对 绑定器</h3><h3 id="资源编排器"><a href="#资源编排器" class="headerlink" title="资源编排器"></a>资源编排器</h3><h3 id="可解释模型"><a href="#可解释模型" class="headerlink" title="可解释模型"></a>可解释模型</h3><h3 id="系统优化"><a href="#系统优化" class="headerlink" title="系统优化"></a>系统优化</h3><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>32GPU的tesebed，商汤的trace。可以降低JCT，提高scalability。</p><hr><p><strong>原文作者：</strong>Qinghao Hu, Meng Zhang, Peng Sun, Yonggang Wen, Tianwei Zhang<br><strong>原文链接：</strong><a href="https://dl.acm.org/doi/pdf/10.1145/3575693.3575705">https://dl.acm.org/doi/pdf/10.1145/3575693.3575705</a><br><strong>Code：</strong><a href="https://github.com/S-Lab-System-Group/Lucid">https://github.com/S-Lab-System-Group/Lucid</a><br><strong>参考链接：</strong>[1] 【论文笔记】Lucid论文阅读笔记 <a href="https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0lucid%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">https://tweakzx.github.io/p/论文笔记lucid论文阅读笔记/</a><br>[2] Lucid总结：一个可扩展和可解释的实用型深度学习任务调度器 <a href="https://zhuanlan.zhihu.com/p/612076435">https://zhuanlan.zhihu.com/p/612076435</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 集群调度 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> ASPLOS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pheromone：服务器无感知计算平台中以数据中为心的函数编排</title>
      <link href="2022/11/13/Pheromone/"/>
      <url>2022/11/13/Pheromone/</url>
      
        <content type="html"><![CDATA[<h1 id="Following-the-Data-Not-the-Function-Rethinking-Function-Orchestration-in-Serverless-Computing"><a href="#Following-the-Data-Not-the-Function-Rethinking-Function-Orchestration-in-Serverless-Computing" class="headerlink" title="Following the Data, Not the Function: Rethinking Function Orchestration in Serverless Computing"></a>Following the Data, Not the Function: Rethinking Function Orchestration in Serverless Computing</h1><p>本文是对即将发表于NSDI 2023会议的论文《Following the Data, Not the Function: Rethinking Function Orchestration in Serverless Computing》的解读。服务器无感知（serverless）应用程序通常由函数（function）工作流组成，交换数据需要通过触发多个短期函数以响应事件或更改状态。当前的服务器无感知计算平台通过高级调用依赖关系来协调和触发函数，但忽略了函数之间的底层数据交换。这种设计在编排复杂工作流时既不高效，也不易使用。本文作者认为函数编排应该遵循以数据为中心的方法。本文的Pheromone平台提供了一个数据桶抽象来保存函数生成的中间数据。开发人员可以使用数据触发器原语来控制每个函数的输出何时以及如何传递给工作流中的下一个函数。通过明确数据消耗并允许其触发函数和驱动工作流，可以轻松高效地支持复杂的函数交互。与商业平台和开源平台相比，Phromone将函数交互和数据交换的延迟降低了一个数量级，可扩展到大型工作流，并可轻松实现复杂应用程序。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>当前的服务器无感知计算平台使用“面向函数的方法”来编排和激活服务器无感知工作流的函数：每个函数被视为一个单独的单元，函数之间的交互在工作流中单独表达。这样的方式存在以下三个缺点：</p><ul><li><p>表达能力有限<br>当前的函数编排假定数据流与在工作流中调用函数的流相同，并且一个函数会立刻直接调用其他函数，将全部输出传递出去。 这个假设并不适用于很多应用程序。</p></li><li><p>可用性有限<br>当前的服务器无感知计算平台为函数之间的数据交换提供了各种选项：函数可以通过消息代理或共享存储同步或异步地交换数据，还可以处理来自各种来源的数据，例如嵌套函数调用、消息队列或其他云服务。因此，当前服务器无感知计算平台缺乏一种在函数之间交换数据的最佳方法，开发人员为了有效地在函数之间传递数据必须找到自己的方法，这些方法可能是动态的、琐碎的，这导致服务器无感知应用程序的开发和部署变得非常复杂。</p></li><li><p>适用性有限</p></li></ul><h2 id="以数据为中心的函数编排"><a href="#以数据为中心的函数编排" class="headerlink" title="以数据为中心的函数编排"></a>以数据为中心的函数编排</h2><p>本文作者注意到中间数据（即函数返回的结果）通常是短暂的且不可变的：在数据生成后会等待被消耗，然后过时。由于中间数据一旦生成就不会更新，因此使用它们来触发函数不会导致一致性问题。作者将数据消耗显式化，并使其能够触发目标函数；开发人员可以指定何时以及如何将中间数据传递给目标函数，并触发它们的激活，从而驱动整个工作流的执行。</p><p>为了方便以数据为中心的函数编排，作者设计了一个数据桶（bucket）抽象和一个触发器原语列表。 图1概述了如何触发函数。 服务器无感知应用程序创建一个或多个保存中间数据的数据桶。 开发人员可以用触发器配置每个桶，这些触发器指定数据何时以及如何调用目标函数并由它们使用。 执行工作流时，源函数直接将其结果发送到指定的桶。 每个桶检查配置的触发条件是否满足（例如，所需的数据已经完成并准备好被消耗）。 如果是，则桶自动触发目标函数并将所需数据传递给它们。 这个过程发生在所有桶中， 这些桶共同驱动整个工作流的执行。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/1.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：数据桶</div></center><br><p>作者为桶设计了各种触发原语，以指定如何触发函数。 函数间的交互模式大致可分为直接触发器原语、条件触发原语、动态触发器原语三类，如表1所示。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/table1.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表1：原语</div></center><p>Pheromone平台目前支持用C++编写的函数，并能够支持更多语言。 Pheromone还提供了一个Python客户端，开发人员可以通过该客户端编写函数交互。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/table2.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表2：API</div></center><h2 id="Pheromone系统设计"><a href="#Pheromone系统设计" class="headerlink" title="Pheromone系统设计"></a>Pheromone系统设计</h2><p>图2展示了Pheromone的系统架构。 每个工作节点接收来自本地调度器的指令，并运行多个执行器，这些执行器根据需要加载和执行用户函数代码。 工作节点还维护一个共享内存对象存储，该存储保存由函数生成的中间数据。 对象存储提供了一个数据桶接口，通过该接口，函数可以在一个节点内以及与其他节点有效地交换数据。Pheromone还会通过可持续的键值存储同步需要长期保存的数据，当新数据添加到对象存储时，本地调度器检查关联的桶触发器。 如果满足触发条件，本地调度器将在全局协调器的帮助下本地或远程调用目标函数，全局协调器在工作节点以外的单独机器上运行。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/2.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：Architecture</div></center><br><p>本文设计了一个两层的分布式调度方案来将Pheromone扩展到一个大的集群中。 具体地说，外部请求首先到达全局协调器，全局协调器将请求发送到工作节点上的本地调度器。 本地调度器尽可能通过调度使得后续函数在本地执行工作流，从而减少调用延迟。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/3.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：两层调度设计</div></center><br><p>Pheromone使用节点上的共享内存对象存储来维护数据对象，这样函数就可以通过指针直接访问数据。当源函数通过send_object()将数据对象放入bucket时，就标记它为“就绪”。 桶可以分布在其负责的协调器和多个工作节点上，其中每个工作节点跟踪本地数据状态，而协调器持有全局视图。桶状态同步只需要在负责的协调器和工作节点之间进行，因为不同工作节点的本地状态只跟踪他们的本地对象。</p><p>由于中间数据是短暂且不可变的，本文用它们的持久性来换取快速的数据共享和低资源占用。通过节点上的共享内存对象存储，Peromone可以通过将数据对象的指针传递到目标函数，实现本地函数之间的“零复制”数据共享，从而避免了大量的数据复制和序列化开销。</p><p>为了有效地将数据传递给远程函数，Pheromone可以在节点之间直接传递数据对象。 一个函数将数据对象的元数据（例如，locator）打包成发送到远程节点的函数请求。远程节点上的目标函数使用这样的元数据直接检索所需的数据对象。与使用远程存储进行跨节点数据共享相比，这种直接数据传输避免了不必要的数据复制，从而减少了网络和存储开销。虽然远程存储方法可以确保更好的数据持久性和一致性，但对于中间数据来说这些是不必要的。</p><p>Pheromone支持各种类型的容错。如果执行器宕机或工作节点上的数据对象丢失，Pheromone会重新启动失败的函数以复现丢失的数据并恢复中断的工作流。 如果在指定的超时时间内没有收到预期的输出，桶触发器就会重新执行源函数。</p><h2 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h2><p>本文评估了在没有任何有效负载的情况下函数的延迟。 本文考虑了三种常见的调用模式：顺序执行（例如，两个函数链）、并行调用（扇出）和聚合调用（扇入）。 作者在实验中改变了并行和聚合调用的函数的数量，以控制并行度。 图4显示了在这三种模式下调用的延迟。 每个延迟数据被进一步拆分为外部调用和内部调用的开销。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pheromone/4.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：三种workflow下函数的延迟</div></center><br><p>左图比较了在五个平台上测量的两个函数的函数链的调用延迟。 Pheromone明显优于其他方法。 其中Pheromone的基于共享内存的消息传递只带来不到20微秒的开销，将本地调用延迟降低到40微秒，比CloudBurst快10倍。 与其他平台相比，延迟的改善变得更加显著。 当调用远程函数时，Feromone和CloudBurst都需要网络传输，从而导致类似的内部调用延迟。 然而，由于CloudBurst需要在服务请求之前调度整个工作流的功能，CloudBurst对外部调用的开销比Pheromone高，从而导致整体性能更差。</p><p>中间和右图分别显示了并行调用和聚合调用下的调用延迟。 本文在每个worker上配置12个执行器，从而在运行16个函数时强制远程调用。 结果显示，Pheromone在各种情况下都具有最佳性能，并且在所有情况下只产生亚毫秒级的延迟，即使对于跨节点函数调用也是如此。 </p><hr><p><strong>原文作者：</strong>Minchen Yu, Tingjia Cao, Wei Wang, Ruichuan Chen<br><strong>原文链接：</strong><a href="https://arxiv.org/pdf/2109.13492.pdf">https://arxiv.org/pdf/2109.13492.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> NSDI </tag>
            
            <tag> serverless </tag>
            
            <tag> 函数编排 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TOPOOPT：面向分布式训练作业的网络拓扑与并行策略协同优化</title>
      <link href="2022/11/04/TOPOOPT/"/>
      <url>2022/11/04/TOPOOPT/</url>
      
        <content type="html"><![CDATA[<h1 id="TOPOOPT-Co-optimizing-Network-Topology-and-Parallelization-Strategy-for-Distributed-Training-Jobs"><a href="#TOPOOPT-Co-optimizing-Network-Topology-and-Parallelization-Strategy-for-Distributed-Training-Jobs" class="headerlink" title="TOPOOPT: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs"></a>TOPOOPT: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs</h1><p>如今的DNN训练系统建立在传统数据中心集群的基础上，交换机以多层胖树拓扑结构（Fat-tree）排列。胖树拓扑忽略流量的特征，服务器对之间的带宽和延迟一致。当工作负载不可预测且主要由短传输组成时，是一种理想的网络拓扑结构；但是胖树网络是分布式DNN训练工作负载的瓶颈：根据Meta的生产环境中，网络overhead占据训练时间的60%。由于DNN训练的通信流量特征是可预测的，可以根据这个特点协同优化网络拓扑和并行策略。</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><h3 id="DNN训练的特点"><a href="#DNN训练的特点" class="headerlink" title="DNN训练的特点"></a>DNN训练的特点</h3><p>数据并行时分布式训练中的一种常见做法，进行数据并行时不同工作节点之间通常以参数服务器、ring-AllReduce 、tree-AllReduce等拓扑结构进行通信。此外，单个GPU的内存无法容纳大型DNN模型，因此需要使用模型并行性将模型划分到多个GPU上。此外，由于跨GPU同步模型参数的成本越来越高，纯数据并行性对于大型训练作业来说很不理想，因此，人们使用数据和模型并行的混合来分布大型DNN，其中DNN的不同部分及其数据集在不同的GPU上并行处理。</p><p>每个训练迭代都包括两种主要类型的数据依赖关系。（1）在正向和反向传播期间计算的激活和梯度数据；（2）一旦处理了一批样本，就通过AllReduce步骤在加速器之间同步模型权重。根据并行化策略，这些数据依赖性可能产生本地内存访问或跨GPU流量。下文将类型（1）产生的网络流量称为MP传输，将类型（2）产生的网络流量称为AllReduce传输。</p><p>以深度学习推荐模型（DLRM）为例，如果模型有4个embedding表，embedding具有512列、10^7行，只采用数据并行的时候，会产生44GB的AllReduce数据传输，但是如果采用混合并行，最大的数据传输量只有4GB。因此，数据并行和模型并行结合对于大型模型来说十分必要。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：不同并行策略下的DLRM模型通信heatmap</div></center><br><h3 id="生产环境数据分析"><a href="#生产环境数据分析" class="headerlink" title="生产环境数据分析"></a>生产环境数据分析</h3><p>图2展示了Meta产业环境中的网络overhead，随着GPU数量的增加，网络很快占据了训练迭代时间的很大一部分。网络开销占Meta生产环境中DNN训练迭代时间的60%。这个瓶颈表明，现有的数据中心网络不足以满足新兴的DNN训练的工作负载。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/3.png" width="35%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：Meta产业环境中的网络overhead</div></center><br><p>图3展示了Meta生产GPU集群中运行的四个训练作业的服务器到服务器流量的heatmap。图中的所有heatmap都包含对角线上的正方形（深蓝色），这表示服务器之间的环形通信模式。但是MP传输（浅蓝色和绿色方块）取决于模型，因为MP传输取决于训练作业的并行化策略和设备放置。在整个训练过程中，不同的迭代之间，训练作业的流量模式是一样的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/4.png" width="85%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：Meta生产GPU集群中运行的四个训练作业的服务器到服务器流量的heatmap</div></center><br><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>本文提出的TOPOOPT是一种基于光学设备的新型系统，它联合优化DNN并行策略和拓扑结构，以加速训练作业。</p><p>TOPOOPT集群是一种可分片的直接连接结构，其中每台服务器都有d个接口连接到d个光交换机的核心层，如图4所示。光交换机使TOPOOPT能够将集群分片，产生每个训练作业的专用分区。每个分片的大小取决于作业申请的服务器数量。给定DNN训练工作和一组服务器，TOPOOPT首先离线在服务器之间找到最佳并行化策略和拓扑结构；然后，TOPOOPT重新配置光交换机以实现作业的目标拓扑。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/5.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：TOPOOPT的连接</div></center><br><p>此外，本文使用了Host-based forwarding等现有工作使用了的优化方法优化网络。</p><h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><p>单独寻找最优并行策略是一个NP-complete的问题，网络拓扑和路由使得问题更加困难。</p><p>TOPOOPT将搜索空间分成两个平面：Comp.×Comm. 和Comm.×Topo.。本文交替优化，在一个平面中迭代搜索，同时保持另一个平面的结果不变。图5说明了交替优化框架：TOPOOPT使用FlexFlow的MCMC（马尔可夫链蒙特卡罗）搜索算法，在考虑通信成本的同时，为给定网络拓扑找到最佳并行化策略。如果并行化策略提高了训练迭代时间，TOPOOPT将其提供给Comm.×Topo.平面，使用TOPOOPT的TOPOLOGY FINDER算法找到有效的网络拓扑和路由。然后将发现的拓扑反馈到Comp.×Comm.。这进一步优化了基于新拓扑的并行化策略和设备布局。该优化循环重复直到收敛或k次迭代之后，其中k是可配置的超参数。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/1.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：TOPOOPT算法流程</div></center><br><p>TOPOOPT有两个目标：（i）为AllReduce传输分配充足的带宽（因为大部分流量属于AllReduce）；（ii）确保MP传输的跳数较小。这两个目标是可以同时实现的，因为DNN训练流量具有一种独特的特性——AllReduce流量是可变的。</p><p>例如，图6展示了三种不同的AllReduce链接方式，它们对应的traffic heatmap分别如图7所示。蓝色的AllReduce通信的位置是可以变化的，但是MP传输的heatmap保持不变。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/6.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图6：Ring-AllReduce permutations</div></center><br><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/7.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图7：DLRM traffic heatmaps</div></center><br>流量易变性意味着，如果一组服务器按特定顺序连接，只需重新排列服务器的标签，就可以得到另一种顺序，以相同的延迟完成AllReduce操作，同时可能为MP传输提供较小的跳数。TOPOOPT不是只选择一个AllReduce顺序，而是为每个AllReducer组找到多个排列，并与它们对应的子拓扑重叠。这样，TOPOOPT可以有效地服务AllReduce流量，同时减少MP传输的跳数。<p>为了减少所有可能排列的搜索空间，本文设计了TotientPerms算法，以基于群论找到所有规则环（连续服务器的索引之间的距离相等的环）的生成法则。然后，SelectPermutations算法可以减少集群的直径，以利于MP传输。</p><h2 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h2><p>本文在6种不同的DNN模型上进行了模拟实验比较，其中DLRM和NCF模型的MP传输量更大。与成本相似的胖树拓扑相比，TOPOOPT将DNN训练时间减少了3.4倍。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/TOPOOPT/8.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图8：在128台服务器上的效果</div></center><br><hr><p><strong>原文作者：</strong>Weiyang Wang, Moein Khazraee, Zhizhen Zhong, Manya Ghobadi, Zhihao Jia, Dheevatsa Mudigere, Ying Zhang, Anthony Kewitsch<br><strong>原文链接：</strong><a href="https://arxiv.org/abs/2202.00433">https://arxiv.org/abs/2202.00433</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> NSDI </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> 网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大规模异构GPU集群中的作业负载分析与调度</title>
      <link href="2022/02/20/MLaaS/"/>
      <url>2022/02/20/MLaaS/</url>
      
        <content type="html"><![CDATA[<h1 id="MLaaS-in-the-Wild-Workload-Analysis-and-Scheduling-in-Large-Scale-Heterogeneous-GPU-Clusters"><a href="#MLaaS-in-the-Wild-Workload-Analysis-and-Scheduling-in-Large-Scale-Heterogeneous-GPU-Clusters" class="headerlink" title="MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters"></a>MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters</h1><p>随着机器学习技术的持续进步和海量数据集的使用，科技公司正在部署大型ML-as-a-Service（MLaaS）云，通常带有异构GPU，以提供大量ML应用程序。然而，在异构GPU集群中运行不同的ML工作负载带来了许多挑战。为了适应快速增长的ML工作负载计算需求，阿里云提供了人工智能机器学习平台（PAI），这是一个多功能的MLaaS平台，使开发人员能够高效、灵活、简化地使用ML技术。其架构如图1所示。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/MLaaS/1.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：PAI的架构</div></center><br>在本文中，作者对从阿里巴巴 PAI 6000多个GPU的生产MLaaS集群收集的两个月工作负载跟踪进行了表征研究，解释了集群调度面临的挑战，包括GPU利用率低、排队延迟长、难以调度的任务需要高端GPU、具有苛刻的调度要求、异构机器之间的负载不平衡，以及CPU上的潜在瓶颈。作者描述了目前的解决方案，并呼吁进一步调查仍有待解决的挑战。作者公开了相关数据集。<h2 id="数据集概览"><a href="#数据集概览" class="headerlink" title="数据集概览"></a>数据集概览</h2><p>PAI的数据集中，大多数作业要求多个GPU。数据集包括不同级别（如作业、任务和实例）的工作负载的到达时间、完成时间、资源请求和GPU、CPU、GPU内存和主内存中的使用情况。数据集还提供了机器级信息，包括守护进程代理收集的硬件规格、资源利用率等。</p><p>具有以下特点：</p><p><strong>严重倾斜的实例分布。</strong>PAI跟踪包含超过1300名用户提交的120万个任务的750万个实例。用户运行的任务实例的分布是严重倾斜的。大约77%的任务实例由前5%的用户提交，每个用户运行的实例超过17.5k个，而后50%的用户每个用户运行的实例不到180个。</p><p><strong>组调度（gang scheduling）占主流。</strong>在所有任务实例中，大约85%的作业要求组调度，其中20%必须在100多个GPU上调度，有些甚至要求超过1000个。加在一起，带有组调度实例的作业占GPU总需求的79%。这类任务的普遍性使得很难实现高利用率。</p><p><strong>GPU局部性。</strong>除了组调度，作业可能会请求在同一台机器上的多个GPU上运行其所有实例，这一要求称为GPU的局部性。虽然这种要求通常会导致长时间的调度延迟，但它允许在单个节点（例如NVLink和NVSwitch）内使用高速GPU到GPU互连，从而显著加快分布式训练。在PAI中，对某些训练任务实施GPU局部性可以有超过10倍的加速比。</p><p><strong>GPU共享。</strong>PAI支持GPU共享，允许多个任务实例以低成本时间共享一个GPU。使用此功能，用户可以在（0，1）区间内指定GPU请求，并使用部分GPU运行其任务实例。GPU共享可以节约大量的GPU资源。</p><p>在时间维度上，作业提交频率具有日间模式，工作日提交的作业略多于周末，其中除了白天，午夜也是任务提交的高峰时间；实例运行时间的变化范围很大（第90百分位的运行时间为4.5小时，比Philly的25小时短），作业排队的延迟时间分布很不均匀。</p><p>在空间维度上，资源请求的分布呈重尾分布（一小部分的实例请求了大量的资源），GPU使用率很低但CPU使用率较高。</p><h2 id="GPU机器利用率"><a href="#GPU机器利用率" class="headerlink" title="GPU机器利用率"></a>GPU机器利用率</h2><h3 id="计算资源利用率"><a href="#计算资源利用率" class="headerlink" title="计算资源利用率"></a>计算资源利用率</h3><p>如图2所示，与内存（包括主存和GPU内存）相比，GPU和CPU的利用率更高。在8-GPU机器（左上角）中，GPU（红色虚线）和CPU（蓝色实线）的平均P90利用率分别达到82%和77%。在2-GPU机器中（右上角），P90 GPU的利用率仍然很高（平均77%），而P90 CPU的利用率在平均值上下降到42%，这是因为CPU与GPU的比率很大（每个GPU配有32或48个CPU）。在这两种类型的机器中，主存和GPU内存的P90利用率几乎一直低于60%，这说明数据集中的作业的内存密集度较低。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/MLaaS/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：资源利用率的CDF</div></center><br>与其他资源相比，在GPU上测量的利用率变化更大。如图2所示，P90 GPU利用率的分布范围很广，在40%到100%之间变化；GPU上的尾部和中值利用率之间的差异也比其他资源上的差异更大。这种巨大的差异是由于在ML工作负载中发现的突发性GPU使用模式。PAI使用的调度器中packing的优先级高于负载平衡也是到这支个现象的原因。<h3 id="网络与I-O利用率"><a href="#网络与I-O利用率" class="headerlink" title="网络与I/O利用率"></a>网络与I/O利用率</h3><p>网络：P100（或Misc）、T4和V100机器中P95网络输入速率分别为带宽的54%、48%和34%。<br>I/O：I/O等待上的CPU时间比usr比内核模式下的CPU时间小三个数量级，这说明CPU主要忙于处理数据，而不是等待I/O完成。</p><h2 id="集群管理的机遇"><a href="#集群管理的机遇" class="headerlink" title="集群管理的机遇"></a>集群管理的机遇</h2><p><strong>GPU共享：</strong>GPU独享导致GPU利用率很低。为了避免这一问题，PAI支持GPU共享，且GPU共享在及群众补货导致激烈的资源竞争。<br><strong>重复性任务的可预测执行时间：</strong>已知任务执行时间是作出更好调度结果的关键。现有调度器根据训练进度（例如，操作次数、损失曲线和目标精度）和任务速度来预测执行时间。获取此类信息需要特定的框架支持（例如TensorFlow和PyTorch），这在实际集群中并不总是可能的，因为用户可能运行各种标准或版本的框架，而且他们提交的任务可能不会执行迭代训练）。然而，作者发现大多数任务都是重复性的：大约65%的任务至少重复运行5次。作者通过任务定期运行、平均实例运行时间稳定的特征来预测人物运行时间。经过实验评估，PAI使用的预测方法的准确率对于做出调度决定来说是足够的。</p><h2 id="集群管理的挑战"><a href="#集群管理的挑战" class="headerlink" title="集群管理的挑战"></a>集群管理的挑战</h2><p>挑战包括：机器规格和实例请求之间不匹配、老旧GPU机器中的任务过多、高端机器中负载不均衡、CPU成为瓶颈等</p><hr><p><strong>原文作者：</strong>Qizhen Weng, Wencong Xiao, Yinghao Yu, Wei Wang, Cheng Wang, Jian He, Yong Li, Liping Zhang, Wei Lin, Yu Ding<br><strong>原文链接：</strong><a href="https://qzweng.github.io/files/2022NSDI-MLaaS-Weng.pdf">https://qzweng.github.io/files/2022NSDI-MLaaS-Weng.pdf</a><br><strong>公开数据集：</strong><a href="https://github.com/alibaba/clusterdata">https://github.com/alibaba/clusterdata</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> NSDI </tag>
            
            <tag> 集群调度 </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>每天一个没用的代码小技巧</title>
      <link href="2022/01/10/techniques/"/>
      <url>2022/01/10/techniques/</url>
      
        <content type="html"><![CDATA[<h2 id="服务器使用"><a href="#服务器使用" class="headerlink" title="服务器使用"></a>服务器使用</h2><ol><li><p>可以使用使NVMeNon-Volatile Memory express）SSD加速disk上文件的读写。<br>首先查看是否有NVMe SSD</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /dev/nvme*</span><br></pre></td></tr></table></figure><p>然后将NVMe mount到需要的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.ext4 /dev/nvme1n1 <span class="comment"># 格式化硬盘 </span></span><br><span class="line">sudo mkdir /mnt/data1 </span><br><span class="line">sudo mount /dev/nvme1n1 /mnt/data1 </span><br></pre></td></tr></table></figure></li><li><p>RAM disk: 不真正地使用disk读写，而是把文件放进内存。[<a href="https://www.linuxbabe.com/command-line/create-ramdisk-linux">参考链接</a>]</p></li><li><p>如果使用ssh登录服务器遇到问题，可以通过<code>ssh -vv</code>查看verbose log。</p></li><li><p><code>parallel-ssh</code> 和<code>parallel-scp</code>命令可以快速登陆多个服务器/向多个服务器同时传输文件。<br>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install pssh </span><br></pre></td></tr></table></figure><p>使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parallel-ssh -i -t 0 -h hostfile <span class="string">&quot;hostname&quot;</span> </span><br><span class="line">parallel-scp –h hostfile -l &lt;user&gt; src dst </span><br></pre></td></tr></table></figure><p>如果出现</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Usage: parallel-scp [OPTIONS] <span class="built_in">local</span> remote</span><br><span class="line">parallel-scp: error: Hosts not specified.</span><br></pre></td></tr></table></figure><p>那就改用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parallel-scp --hosts=hostfile --user=&lt;user&gt; src dst</span><br></pre></td></tr></table></figure></li><li><p>如果服务器不能连到外网，而自己有ClashX代理，可以配置反向代理。<br>在本地：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -NfR 7820:127.0.0.1:7890 username@server_ip</span><br></pre></td></tr></table></figure><p>注意第一个端口号需要和代理的端口号不同，而第二个端口号7890是ClashX的默认端口号，根据实际情况替换。<br>然后正常ssh到服务器。<br>打开ClashX，command+C复制，然后粘贴到服务器的命令行里，把端口号都改成刚才设置的第一个端口号（7820），然后就可以上网了。另外可以注意一下需要按照<a href="https://www.zhang-hb.com/2022/02/02/%E7%94%A8ssh%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E8%AE%A9%E4%B8%8D%E8%81%94%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%BD%91/">这篇博客</a> 设置<code>/etc/ssh/sshd_config</code>。</p></li><li><p>使用NFS共享存储<br>服务器端安装NFS服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nfs-kernel-server</span><br></pre></td></tr></table></figure><p>服务器端在<code>/etc/exports</code>里添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/dir *(rw,sync,no_subtree_check,no_root_squash)</span><br></pre></td></tr></table></figure><p>然后命令行执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nfs-kernel-server restart</span><br></pre></td></tr></table></figure><p>客户端命令行运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nfs-common --force-yes</span><br><span class="line">sudo mount -t nfs <span class="variable">$1</span>:/dir /dir</span><br></pre></td></tr></table></figure><p>不用的时候需要在服务器端stop service，并在客户端unmount。</p></li><li><p>查看端口使用情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -tunlp | grep &lt;port_number&gt;</span><br></pre></td></tr></table></figure></li><li><p>不要轻易使用<code>conda install python==x.x</code>更换已有环境的python版本，以前安装的pip包会被覆盖。</p></li><li><p>查看物理CPU个数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep <span class="string">&quot;physical id&quot;</span>| sort| uniq| wc -l</span><br></pre></td></tr></table></figure><p>查看逻辑CPU个数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep <span class="string">&quot;processor&quot;</span>| wc -l</span><br></pre></td></tr></table></figure><p>查看每个CPU的核数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep <span class="string">&quot;cpu cores&quot;</span>| uniq</span><br></pre></td></tr></table></figure><p>查看CPU型号：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c</span><br></pre></td></tr></table></figure><p>查看Linux内核：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -a</span><br></pre></td></tr></table></figure><p>查看内存使用情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">free -m</span><br></pre></td></tr></table></figure></li></ol><h2 id="GPU-使用"><a href="#GPU-使用" class="headerlink" title="GPU 使用"></a>GPU 使用</h2><ol><li><p>Persistent mode 可以加速GPU上的计算和内存操作 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-smi -pm 1</span><br></pre></td></tr></table></figure></li><li><p><code>dmesg</code>命令可以查看硬件上的历史报错信息。</p></li><li><p>检测不到GPU device：可能是fabric manager版本不匹配的问题。<br>检测方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nvidia-fabricmanager status</span><br></pre></td></tr></table></figure><p>如果检测出版本不匹配的问题，则重新安装fabric manager：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/nvidia-fabricmanager-450_450.80.02-1_amd64.deb </span><br><span class="line">sudo apt install ./nvidia-fabricmanager-450_450.80.02-1_amd64.deb </span><br><span class="line">sudo systemctl <span class="built_in">enable</span> nvidia-fabricmanager </span><br><span class="line">sudo systemctl restart nvidia-fabricmanager </span><br></pre></td></tr></table></figure><p>在有些机器上，fabric manager会自动更新其版本，需要停止自动跟新更新才能保证版本始终可以匹配。[<a href="https://github.com/Azure/azhpc-images/blob/master/ubuntu/common/disable_auto_upgrade.sh">停止自动更新的脚本</a>]</p></li><li><p>改变GPU的power上限：<code>nvidia-smi -i %s -pl [power_upper_limit]</code></p></li><li><p>改变GPU的frequency：<code>&quot;nvidia-smi -i %s -ac %s,%s&quot; % (gid, mem, gra)</code><br>查看GPU的frequency：<code>nvidia-smi --query-gpu=clocks.applications.graphics,clocks.applications.mem -i %s --format=csv,noheader,nounits</code><br>恢复默认值：<code>nvidia-smi -rac</code><br>锁住固定的frequency：<code>nvidia-smi -lgc $frequency -i $i</code></p></li><li><p>AWS EC2服务器暂时不能直接使用 GPU，可能是因为NVIDIA更新了公钥，但是AWS系统更新的时候没暂时更新驱动，因此需要自己装一遍驱动才能使用GPU<br>修改环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BASE_URL&#x3D;https:&#x2F;&#x2F;us.download.nvidia.com&#x2F;tesla</span><br><span class="line">DRIVER_VERSION&#x3D;450.80.02 # 510.47.03 for A100</span><br></pre></td></tr></table></figure><p>然后执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -fSsl -O <span class="variable">$BASE_URL</span>/<span class="variable">$DRIVER_VERSION</span>/NVIDIA-Linux-x86_64-<span class="variable">$DRIVER_VERSION</span>.run</span><br><span class="line">sudo sh NVIDIA-Linux-x86_64-<span class="variable">$DRIVER_VERSION</span>.run</span><br></pre></td></tr></table></figure><p>或者修复apt-get之后通过<a href="https://blog.csdn.net/qq_28256407/article/details/115548675">https://blog.csdn.net/qq_28256407/article/details/115548675</a> 安装</p></li></ol><h2 id="Deep-Learning-开发-主PyTorch-NCCL分布式训练"><a href="#Deep-Learning-开发-主PyTorch-NCCL分布式训练" class="headerlink" title="Deep Learning 开发 (主PyTorch+NCCL分布式训练)"></a>Deep Learning 开发 (主PyTorch+NCCL分布式训练)</h2><ol><li><p>使用pip install安装的PyTorch使用静态链接的NCCL。如果想要更新NCCL版本，则可以使用一下命令（适用于A100 GPU）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b v2.11.4-1 https://github.com/NVIDIA/nccl /nccl-2.11.4 </span><br><span class="line"><span class="built_in">cd</span> /nccl-2.11.4 </span><br><span class="line">make -j src.build TRACE=1 NVCC_GENCODE=<span class="string">&quot;-gencode=arch=compute_80,code=sm_80&quot;</span> </span><br><span class="line">make install </span><br></pre></td></tr></table></figure></li><li><p>使用Pytorch DDP后清理GPU内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.synchronize()</span><br><span class="line"><span class="keyword">del</span> ddp_model</span><br><span class="line"><span class="keyword">del</span> dataset</span><br><span class="line"><span class="keyword">del</span> optimizer</span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line">dist.destroy_process_group() </span><br></pre></td></tr></table></figure><p>但是目前版本的PyTorch中，<code>dist.destroy_process_group(group=subgroup)</code>并不能真正清理掉subgroup使用的内存，且这个内存泄漏无法被常用的内存泄漏检测方法检测到。<br>（<a href="https://gist.github.com/MInner/8968b3b120c95d3f50b8a22a74bf66bc">内存泄漏检测方法1</a>，<a href="https://discuss.pytorch.org/t/how-to-debug-causes-of-gpu-memory-leaks/6741/3">内存泄漏检测方法2</a>）</p></li><li><p>使用<code>NCCL_DEBUG=TRACE</code>需要使用<code>TRACE=1</code>这一flag重新编译NCCL，然后运行<code>make install</code>。<br>还需要保证NCCL的安装路径在LD_LIBRARY_PATH里。打出来的trace前面有时间戳，单位是ms。 </p></li><li><p>如果使用NCCL时遇到了奇怪的bug（如segmentation fault），可以检查环境变量的设置是否正确。即使通过<code>NCCL_DEBUG=INFO</code>打印出来的log一切正常，也需要再设置一次环境变量。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> NCCL_IB_PCI_RELAXED_ORDERING=1</span><br><span class="line"><span class="built_in">export</span> NCCL_SOCKET_IFNAME=eth0</span><br><span class="line"><span class="built_in">export</span> CUDA_DEVICE_ORDER=PCI_BUS_ID</span><br><span class="line"><span class="built_in">export</span> NCCL_NET_GDR_LEVEL=5</span><br><span class="line"><span class="built_in">export</span> LD_PRELOAD=/opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so <span class="comment"># path to ibnccl-net.so</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/opt/hpcx/nccl_rdma_sharp_plugin/lib:<span class="variable">$LD_LIBRARY_PATH</span> <span class="comment"># the dir where ibnccl-net.so is in</span></span><br></pre></td></tr></table></figure><p>其中，如果不设置NCCL_IB_PCI_RELAXED_ORDERING、CUDA_DEVICE_ORDER和NCCL_NET_GDR_LEVEL，（单机）多卡之间的通信可能会非常慢。</p></li><li><p>如果使用NCCL时能建立起来进程组，但是在通信时报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: NCCL error <span class="keyword">in</span>: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:957, internal error, NCCL version 21.1.4</span><br><span class="line">ncclInternalError: Internal check failed. This is either a bug <span class="keyword">in</span> NCCL or due to memory corruption</span><br></pre></td></tr></table></figure><p>检查master IP、port的设置是否正确，如果是多机环境，检查不同机器上环境变量的设置是否一致、NCCL版本是否一致。</p><p>如果在卡住一段时间之后报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;test.py&quot;</span>, line 112, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    ddp_model = DDP(model, device_ids=[args.rank% torch.cuda.device_count()], output_device=args.rank% torch.cuda.device_count())</span><br><span class="line">  File <span class="string">&quot;/data/gdd/software/miniconda3/envs/env/lib/python3.7/site-packages/torch/nn/parallel/distributed.py&quot;</span>, line 578, <span class="keyword">in</span> __init__</span><br><span class="line">    dist._verify_model_across_ranks(self.process_group, parameters)</span><br><span class="line">RuntimeError: NCCL error <span class="keyword">in</span>: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:957, unhandled system error, NCCL version 21.0.3</span><br><span class="line">ncclSystemError: System call (socket, malloc, munmap, etc) failed.</span><br></pre></td></tr></table></figure><p>则需要设置NCCL_SOCKET_IFNAME环境变量！</p></li><li><p>如果使用NCCL时无法建立进程组，并报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ProcessGroupNCCL.cpp:957, invalid usage, NCCL version 21.0.3</span><br><span class="line">ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams <span class="keyword">in</span> a group, etc).</span><br></pre></td></tr></table></figure><p>检查同一台机器上不同进程（rank）的<code>CUDA_VISIBLE_DEVICES</code>环境变量是否不同。</p></li><li><p>使用PyTorch训练时报错</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, xxxx])</span><br></pre></td></tr></table></figure><p>检查<code>DataLoader</code>是否设置了<code>drop_last=True</code>，并检查batch size是否大于1（如果模型中有batch Normalization层）。</p></li><li><p><a href="https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile">PyTorch Profiler</a>，分析性能的好帮手</p></li><li><p>如果PyTorch dataloader报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Exception ignored <span class="keyword">in</span>: &lt;<span class="keyword">function</span> _MultiProcessingDataLoaderIter.__del__ at 0x7f733cbf6f70&gt;</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;</span>, line 1510, <span class="keyword">in</span> __del__</span><br><span class="line">    self._shutdown_workers()</span><br><span class="line">  File <span class="string">&quot;/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;</span>, line 1493, <span class="keyword">in</span> _shutdown_workers</span><br><span class="line">    <span class="keyword">if</span> w.is_alive():</span><br><span class="line">  File <span class="string">&quot;/opt/conda/lib/python3.8/multiprocessing/process.py&quot;</span>, line 160, <span class="keyword">in</span> is_alive</span><br><span class="line">    assert self._parent_pid == os.getpid(), <span class="string">&#x27;can only test a child process&#x27;</span></span><br><span class="line">AssertionError: can only <span class="built_in">test</span> a child process</span><br><span class="line">terminate called after throwing an instance of <span class="string">&#x27;c10::CUDAError&#x27;</span></span><br><span class="line">  what():  CUDA error: initialization error</span><br><span class="line">CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.</span><br><span class="line">For debugging consider passing CUDA_LAUNCH_BLOCKING=1.</span><br></pre></td></tr></table></figure><p>可能是数据集corrupt了。</p></li></ol><h2 id="Graceful-Python-Projects"><a href="#Graceful-Python-Projects" class="headerlink" title="Graceful Python Projects"></a>Graceful Python Projects</h2><ol><li>使用logging包是，如果在一个文件里add handler了，就无需在这个包所import的其他py文件里再次add handler了，只需：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOG = logging.getLogger(__name__)</span><br><span class="line">LOG.setLevel(logging.INFO)</span><br></pre></td></tr></table></figure></li><li>删除<code>__pycache__</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find . -name <span class="string">&quot;*.pyc&quot;</span> -<span class="built_in">type</span> f -<span class="built_in">print</span> -<span class="built_in">exec</span> rm -rf &#123;&#125; \;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h2 id="Docker使用"><a href="#Docker使用" class="headerlink" title="Docker使用"></a>Docker使用</h2><ol><li><p>从远程镜像启动容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it -d --name=&lt;name&gt; --privileged --net=host --ipc=host --gpus=all -v /opt:/opt2 repo/tag</span><br></pre></td></tr></table></figure></li><li><p>进入容器的bash命令行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker <span class="built_in">exec</span> -it &lt;name&gt; bash</span><br></pre></td></tr></table></figure></li><li><p>make and run</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker build -t &lt;name&gt; -f Dockerfile .</span><br></pre></td></tr></table></figure></li><li><p>将当前容器的镜像push到远程repo<br>首先确保自己登录了docker，没有登录的话先登录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u &lt;用户名&gt; -p &lt;密码&gt;</span><br></pre></td></tr></table></figure></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo docker commit &lt;container_id&gt; &lt;repo&gt;/&lt;tag&gt;</span><br><span class="line">sudo docker push &lt;repo&gt;/&lt;tag&gt;</span><br></pre></td></tr></table></figure><ol start="5"><li><p>删除image之前需要先删除容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo docker ps</span><br><span class="line">sudo docker rm &lt;container&gt;</span><br><span class="line">sudo docker image ls</span><br><span class="line">sudo docker rmi &lt;image&gt;</span><br></pre></td></tr></table></figure></li><li><p>重启docker服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure></li><li><p>如果无法pull镜像，但能正常联网，可能是docker代理配置的问题，参考：<br><a href="https://forums.docker.com/t/docker-pull-results-in-request-canceled-while-waiting-for-connection-client-timeout-exceeded-while-awaiting-headers/73064/26">https://forums.docker.com/t/docker-pull-results-in-request-canceled-while-waiting-for-connection-client-timeout-exceeded-while-awaiting-headers/73064/26</a><br>两种常见的解决方案：</p></li></ol><ul><li>设置系统DNS：<a href="https://docs.docker.com/config/daemon/systemd/#httphttps-proxy">https://docs.docker.com/config/daemon/systemd/#httphttps-proxy</a></li><li>Docker代理配置：<a href="https://datawookie.dev/blog/2018/10/dns-on-ubuntu/">https://datawookie.dev/blog/2018/10/dns-on-ubuntu/</a></li></ul><p>可以通过<code>docker info</code>查看是否有默认的代理配置。如果编辑了<code>/etc/systemd/system/docker.service.d/</code>并重启Docker不能覆盖这些代理的配置，可能是因为这些配置被写在了<code>/lib/systemd/system/docker.service</code>文件里，需要去编辑<code>/lib/systemd/system/docker.service</code>。</p><h2 id="Kubernetes使用"><a href="#Kubernetes使用" class="headerlink" title="Kubernetes使用"></a>Kubernetes使用</h2><ol><li><p>初始化：<code>sudo kubeadm init --pod-network-cidr=192.168.0.0/16</code></p></li><li><p>让master节点也成为worker参与到调度中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line">kubectl get nodes -o wide <span class="comment"># 查看节点</span></span><br></pre></td></tr></table></figure><p>其中第一条命令会去掉所有node的node-role.kubernetes.io/master相关taint（有些taint后面会带有NoSchedule的标记，具体有哪些taint可以通过<code>kubectl describe &lt;node-name&gt;</code>查看）。</p></li><li><p>查看资源详细描述：<code>kubectl describe</code><br>查看提交任务的状态：<code>kubectl get pods</code><br>查看具体pod的log：<code>kubectl logs &lt;pod-name&gt;</code></p></li><li><p>删除pod之前需要先删除deployment/stateful set/daemonset等，不然pod被删除后会一直被重启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deployment -n &lt;namespace&gt;</span><br><span class="line">kubectl delete deployment &lt;deployment&gt; -n &lt;namespace&gt;</span><br><span class="line">kubectl delete pod &lt;pod-name&gt; -n &lt;namespace&gt;</span><br></pre></td></tr></table></figure></li><li><p>如果<code>/</code>目录下已经用了超过80%的空间，会有disk pressure的问题，需要清理磁盘。</p></li><li><p>Kubernetes inter-pod networking test：<a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/hardway/test-networking">https://projectcalico.docs.tigera.io/getting-started/kubernetes/hardway/test-networking</a></p></li><li><p>使用Kubernetes时让DL job内使用InfiniBand通信：<a href="https://github.com/gudiandian/k8s-rdma-sriov-dev-plugin">https://github.com/gudiandian/k8s-rdma-sriov-dev-plugin</a></p></li></ol><h2 id="Latex"><a href="#Latex" class="headerlink" title="Latex"></a>Latex</h2><ol><li><p>去掉行号：<code>\documentclass</code>里面去掉<code>review</code></p></li><li><p>表格中用对号和叉，使其更美观。对号：\ding{52}；叉：\ding{56}</p></li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol><li>用命令行下载Google drive上的大文件（例如数据集）：</li></ol><p>进入链接：<a href="https://developers.google.com/oauthplayground/">https://developers.google.com/oauthplayground/</a></p><p>在 Select the Scope 栏复制 <a href="https://www.googleapis.com/auth/drive.readonly">https://www.googleapis.com/auth/drive.readonly</a></p><p>点击Authorize APIs 然后 Exchange authorization code for tokens</p><p>复制 Access token</p><p>在命令行运行 <code>curl -H &quot;Authorization: Bearer ACCESS_TOKEN&quot; https://www.googleapis.com/drive/v3/files/FILE_ID?alt=media -o FILE_NAME</code></p><p>参考链接：<a href="https://stackoverflow.com/questions/65312867/how-to-download-large-file-from-google-drive-from-terminal-gdown-doesnt-work">https://stackoverflow.com/questions/65312867/how-to-download-large-file-from-google-drive-from-terminal-gdown-doesnt-work</a></p><ol start="2"><li>统计项目代码行数: <a href="https://github.com/AlDanial/cloc">cloc</a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cloc [options] &lt;file(s)/dir(s)/git <span class="built_in">hash</span>(es)&gt;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>碳排放与大规模神经网络训练</title>
      <link href="2021/11/14/CO2e/"/>
      <url>2021/11/14/CO2e/</url>
      
        <content type="html"><![CDATA[<h1 id="Carbon-Emissions-and-Large-Neural-Network-Training"><a href="#Carbon-Emissions-and-Large-Neural-Network-Training" class="headerlink" title="Carbon Emissions and Large Neural Network Training"></a>Carbon Emissions and Large Neural Network Training</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文调查并分享这些最新和大型NLP模型的能耗和CO<sub>2</sub>e（CO<sub>2</sub> equivalance emissions）估算值。通过描述硬件和数据中心上的实际搜索过程，本文还将Evolved Transformer的神经架构搜索（neural architecture search，NAS）的CO<sub>2</sub>e早期估计值减少了88倍。</p><p>本文对CO<sub>2</sub>e的调查揭示了对DNN生命周期、运行它们的数据中心和硬件、能量组合的变化以及准确评估CO<sub>2</sub>e的困难的不同于以往认知之处和误解。值得注意的是，本文测评了运行中的计算机和数据中心的CO<sub>2</sub>e，而不是制造或者回收它们CO<sub>2</sub>e。</p><p>为了让ML社区更容易理解训练的实际影响以及如何减少训练的影响，本文呼吁：</p><ol><li>本文呼吁更多的研究者测量能源的消耗并公开相关数据；</li><li>除了准确率和相关的指标以外，能源使用效率应该是发表计算密集型模型的ML研究的评估标准，因为最可持续的能源是你不使用的能源；</li><li>即使我们可以在云数据中心将CO<sub>2</sub>e降至零，减少训练时间也很重要，这既因为“时间就是金钱”，也因为更低的训练成本可以让更多人参与进来。因此，本文也支持让更多的研究人员公布训练时使用的加速器的数量及其训练时间，以鼓励在降低训练成本方面取得进展。本文作者相信，这些新的激励措施可能会导致一个良性循环，让ML从业者竞相增加收入。</li></ol><h2 id="NLP模型的资源消耗和碳足迹"><a href="#NLP模型的资源消耗和碳足迹" class="headerlink" title="NLP模型的资源消耗和碳足迹"></a>NLP模型的资源消耗和碳足迹</h2><p>跑一个机器学习所需要的电量是一个关于算法、实现算法的程序、运行程序所需处理器的数量、这些处理器的速度和耗电量、数据中心传递能量和冷却处理器的效率、能源供应组合（新能源、燃气、煤等）。</p><p>很多公司在DNN模型inference上小号的能量比训练要多。比如NVIDIA估计80-90%的机器学习workload是在进行inference。Amazon Web 90%的云端ML服务是inference。因此很多公司专门为inference设计了加速器。本文主要关注training（training的耗能更独立且方便于探究），但是inference中的耗能也是不可忽略的。</p><p>表1为CO<sub>2</sub>e分解，本文之后将进一步解释这些改进的业务原理，展示了提高ML效率的交叉激励。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/CO2e/1.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表1：基于能耗测量，估计CO<sub>2</sub>e for Transformer and Evolved Transformer for P100 and TPU v2</div></center><br><p>图1说明了每一步的收益：CO<sub>2</sub>e的总体改善为57倍。这一巨大的收益说明了为什么选择DNN模型、处理器、数据中心和地理位置对于改善二氧化碳排放至关重要。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/CO2e/2.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Improvement in CO<sub>2</sub>e over Transformer (Big) on P100 GPUs in an average US datacenter versus Evolved Transformer (Medium) on TPU v2s in the Google Iowa datacenter</div></center><br><h3 id="算法-程序的提升"><a href="#算法-程序的提升" class="headerlink" title="算法/程序的提升"></a>算法/程序的提升</h3><p>和Transformer比起来，使用了NAS的Evolved Transformer训练更快，准确率略高。<br><em>商业原则：</em>更快的训练不仅可以节省ML科研工作者的时间，还可以为他们的组织节省资金，减少CO<sub>2</sub>e排放。</p><h3 id="处理器的提升"><a href="#处理器的提升" class="headerlink" title="处理器的提升"></a>处理器的提升</h3><p>TPU比P100 GPU训练更快，且消耗的能源更少。<br><em>商业原则：</em>在过去十几年中，深度学习的范围和规模大幅增加，这为构建定制硬件创造了机会，该硬件可根据DNN模型训练和inference所涉及的计算类型进行定制。在过去的七年中，谷歌没有像许多其他组织那样使用GPU，而是为DNN设计、构建和部署了四代定制张量处理单元（TPU）硬件，以加速模型训练和inference。为了获得更好的投资回报，云计算公司的目标实际上是提高性价比，而不仅仅是性能。这里的成本是指总拥有成本（TCO），包括年度运营成本，如耗电量和计算机、冷却、配电和建筑的资本支出摊销。功耗与TCO几乎完全线性相关，因此提高性能/TCO也有助于性能/瓦特，节省资金并减少CO<sub>2</sub>e排放。</p><h3 id="数据中心的提升"><a href="#数据中心的提升" class="headerlink" title="数据中心的提升"></a>数据中心的提升</h3><p>衡量数据中心效率的一个指标是它超出了数据中心内计算设备的直接动力的能量开销。由于服务器利用率等其他因素的影响，云数据中心的能效大约是典型企业数据中心的2倍。更广泛地说，由于云数据中心比典型的企业的数据中心的能效要高得多，数据中心能源使用量并没有激增。最近发表在《科学》杂志上的一篇论文发现，尽管计算能力在同一时期增加了550%，但全球数据中心能耗与2010年相比仅增加了6%。<br><em>商业原则：</em>云计算公司致力于节能数据中心，因为它可以节省资金并降低排放。“能量就是金钱。”</p><h3 id="能源供应组合的提升"><a href="#能源供应组合的提升" class="headerlink" title="能源供应组合的提升"></a>能源供应组合的提升</h3><p>根据美国平均能源供应组合，能源的总碳强度为0.429kg CO<sub>2</sub>e/千瓦时。在按照谷歌的24/7无碳能源框架匹配谷歌的清洁能源购买后，作者在爱荷华州数据中心运行Evolved Transformer的净CO<sub>2</sub>e下降到0.080，比原来的好5.4倍。<br><em>商业原则：</em>与通过光纤以光子形式发送信息相比，远距离传输电力成本更高，效率更低。云计算允许像谷歌这样的公司拥有一个全球数据中心组合，其中许多位于电网更清洁的地方（如芬兰）或公司可以直接购买清洁能源的地方（如爱荷华州）。2020年，谷歌在其能源战略中宣布了一个新目标：到2030年，它的目标是让所有谷歌数据中心和办公室全天候使用无碳能源。对于这个24/7无碳能源核算，作者从每小时消耗量中扣除在同一地理位置的本地电网和同一小时购买的所有清洁能源，从而得出净CO<sub>2</sub>e/KWh值。由于爱荷华州夜间风力强劲，谷歌的风力发电组合将爱荷华州数据中心2020年12月的CO<sub>2</sub>e/KWh总平均值降低了6倍，从当地电网的0.478千克降至净平均值0.080千克。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>$$<br>KWh = (Hours\ to\ train) \times (Number\ of\ Processers) \times (Average \  Power \ per \ Processor) \times PUE \div 1000<br>$$<br>$$<br>tCO_2e = KWh \times kg \  CO_2e \  per \  KWh  \div 1000<br>$$</p><h2 id="5种大型NLP模型的C能源使用和CO2e排放"><a href="#5种大型NLP模型的C能源使用和CO2e排放" class="headerlink" title="5种大型NLP模型的C能源使用和CO2e排放"></a>5种大型NLP模型的C能源使用和CO<sub>2</sub>e排放</h2><ul><li>T5: 86MWh, 47tCO<sub>2</sub>e。</li><li>Meena：232MWh, 96tCO<sub>2</sub>e。</li><li>GShard：24MWh, 4.3tCO<sub>2</sub>e。</li><li>Switch Transformer：179MWh, 59tCO<sub>2</sub>e。</li><li>GPT-3：552MWh, 1287tCO<sub>2</sub>e。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/CO2e/3.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表2：CO<sub>2</sub>e for NLP models</div></center><br><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/CO2e/4.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：训练五个大型DNN模型所需要的加速器年、能量消耗和CO<sub>2</sub>e排放量</div></center><br><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>在本节中，作者讨论因训练NLP模型而产生的与碳排放相关的其他因素。</p><h3 id="估计NAS的能量消耗"><a href="#估计NAS的能量消耗" class="headerlink" title="估计NAS的能量消耗"></a>估计NAS的能量消耗</h3><p>使用NAS后，模型参数减少37%，训练收敛所需时间减少25%，CO<sub>2</sub>e排放量减少48.5t。</p><h3 id="整个训练过程中消耗的能源远远不止最后一次训练所消耗的能源"><a href="#整个训练过程中消耗的能源远远不止最后一次训练所消耗的能源" class="headerlink" title="整个训练过程中消耗的能源远远不止最后一次训练所消耗的能源"></a>整个训练过程中消耗的能源远远不止最后一次训练所消耗的能源</h3><p>作者认为ML从业者可以研究整个训练模型的生命周期，制定经验法则，根据最终训练成本估算总体碳足迹。</p><h3 id="测量比推测更有趣"><a href="#测量比推测更有趣" class="headerlink" title="测量比推测更有趣"></a>测量比推测更有趣</h3><p>虽然对碳排放量做出估计相对容易，但我们应更多地关注已经进行的实际实验，而不是假设的案例研究。无论是处理器还是DNN，实际测量情况和理论情况都有很大不同。</p><h3 id="标准的ML算法技术可以提高能源效率"><a href="#标准的ML算法技术可以提高能源效率" class="headerlink" title="标准的ML算法技术可以提高能源效率"></a>标准的ML算法技术可以提高能源效率</h3><p>有许多算法技术可以提高机器学习模型的能量效率。有些技术可以用较少的总体计算达到相同的精度。另外有的技术可以使用一个已经训练过的大型模型作为起点，生成一个重量更轻、计算效率更高、精度几乎相同的模型。这些技术都有助于减少计算成本，从而减少模型的能源和碳排放。这些技术包括知识蒸馏、剪枝量化高效编程、迁移学习与微调、稀疏激活的专家风格混合的模型等。</p><p><font size=3.5 color=Red><strong>即使在同一个组织里，使用哪一个数据中心很重要</strong></font><br>作者发现在何时何地训练DNN非常重要。此外，该方案可能是ML从业者减少CO<sub>2</sub>e排放的最简单途径。</p><h3 id="许多人都可以访问能源优化的数据中心"><a href="#许多人都可以访问能源优化的数据中心" class="headerlink" title="许多人都可以访问能源优化的数据中心"></a>许多人都可以访问能源优化的数据中心</h3><p>自2010年以来，云计算的使用量不断增加，数据中心的能源强度每年降低20%。访问能源优化、低成本的云数据中心并不局限于少数公司的员工；世界各地的人们可以使用阿里云、亚马逊网络服务、谷歌云平台和Microsoft Azure等服务租用服务器。此外，阿里巴巴、亚马逊和谷歌还通过云服务提供对其自定义DNN处理器的访问。自2010年以来，公共云的业务年增长率高达50%，这表明公共云非常受欢迎。许多人认为，云在成本和能源方面的效率意味着它是所有数据中心的最终未来。</p><h3 id="训练成本也很重要"><a href="#训练成本也很重要" class="headerlink" title="训练成本也很重要"></a>训练成本也很重要</h3><p>尽管许多人可以使用这些相对高效的计算资源，云公司在未来可能会大幅减少碳足迹，但降低训练的经济成本仍然很重要。显然，节约资金对每个人都很重要，但NLP模型的昂贵的训练成本也使许多研究人员无法实现这种研究。与环境问题一样，获取最先进模型的不平等性是另一个强有力的动机，它可以激励研究者开发高效节能的ML模型，且该模型的计算能力要与其他模型一样强。</p><h3 id="训练大型NLP模型与其他活动相比如何？"><a href="#训练大型NLP模型与其他活动相比如何？" class="headerlink" title="训练大型NLP模型与其他活动相比如何？"></a>训练大型NLP模型与其他活动相比如何？</h3><p>作者将训练NLP模型的能量消耗与航空飞行、比特币做了比较。<br>训练这四个大型NLP模型并不是谷歌能源消耗的重要部分。</p><h3 id="NLP模型带来的效益值得所需的能量消耗吗？"><a href="#NLP模型带来的效益值得所需的能量消耗吗？" class="headerlink" title="NLP模型带来的效益值得所需的能量消耗吗？"></a>NLP模型带来的效益值得所需的能量消耗吗？</h3><p>MoE模型越大，BLEU分数的获益越大。<br>训练模型的分摊人均CO<sub>2</sub>e影响小于发送一条短信的CO<sub>2</sub>e。</p><hr><p><strong>原文作者：</strong>David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean<br><strong>原文链接：</strong><a href="https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf">https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习系统 </tag>
            
            <tag> 绿色机器学习 </tag>
            
            <tag> 能耗 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Blink：面向分布式机器学习的快速、通用的通信原语</title>
      <link href="2021/09/19/Blink/"/>
      <url>2021/09/19/Blink/</url>
      
        <content type="html"><![CDATA[<h1 id="Blink-Fast-and-Generic-Collectives-for-Distributed-ML"><a href="#Blink-Fast-and-Generic-Collectives-for-Distributed-ML" class="headerlink" title="Blink: Fast and Generic Collectives for Distributed ML"></a>Blink: Fast and Generic Collectives for Distributed ML</h1><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>目前用于深度学习的数据集规模越来越大，在训练效果得到提升的同时，与之同时，漫长的训练时间却成为了另一个让人头疼的问题。基于多个 GPU 的数据并行训练（Data-Parallel Training）大大缓解了大规模深度学习模型训练耗时长的问题，但是数据并行需要同步模型参数，跨 GPU 的参数同步在大规模训练时产生了较大开销，这一开销成为了限制训练速度的瓶颈。</p><p>为了缓解通信上的这一瓶颈，这几年在软件和硬件上都有很大的提升。在硬件方面，NVIDIA提供了多种先进的服务器，例如DGX-1、DGX-2等。在这些服务器中，有GPU用于计算的P100或者V100，有用于通信的PCIe和NCLink。在软件层面，很多公司提供了自己的通信库，例如NVIDIA的NCCL，Facebook的gloo，还有Uber的Horovod。</p><p>作者发现，即使使用了当前很先进的的硬件，通信开销仍然很高。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Blink/1.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：不同服务器上通信开销占训练时间的比例</div></center><br><p>而且，随着GPU的计算能力的增强，通信开销的带来的影响会越来越大。因此，作者认为，在现有的硬件条件下，我们需要可以更快进行通信的protocol。</p><p>为了减少通信开销，作者设计了Blink这一集合通信库。</p><h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><ol><li>Different server configuration。图2中有两种DGX-1服务器link的拓扑和带宽不同。 目前的通信库通常会忽略这个问题，从而导致很多的link不能被高效利用。<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"  src="/img/Blink/2.png" width="60%"> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图2：两种DGX-1服务器的拓扑结构</div></center></li></ol><br><ol start="2"><li><p>Link heterogeneoty。SOTA通信库的通信原语都是基于“环”结构的。那么什么是基于“环”的原语呢？以broadcast为例，左边有四块GPU，他们通过环形的拓扑连接起来。（broadcast的过程）而一个server中会有多种不同种类的link。（PPT图有pcie和nvlink）如果使用这种基于“环”结构的通信原语的话，他只能利用同种类型的link。如果使用了不同类型的link，通信过程中的bottleneck在于这个ring中bandwidth最小的那一个link（例如，pcie bottleneck）。</p></li><li><p>Fragmentation in multi-tenant clusters。 在多租户的集群中，由调度器把GPU分配给不同的job。有时候一个机器里会有多个job，这几个job之间有可能会共用PCIe或者某种link，从而互相影响。<br>在从微软收集到的的40,000多个job的trace中，可以看到，即使用户一般都会申请试用2的n次方块GPU，但是，在每个server中分配的GPU数量可能是3，5，6，7这种数字。比如一个使用8块GPU的job如果被分在2个server上，可能是2+6或者3+5这种数字的组合。为什么会有这种碎片化的问题呢？因为集群的调度器不是topology-aware的。目前还没有通用的、高效进行migration的方法，为了避免过长的queuing delay，这些DNN的任务必须面对碎片化的问题。在这种情况下，会有一些GPU的分配中，同一个job使用的GPU没法直接形成“环”（ppt图）。在NCCL中如果遇到这种情况，会使放弃使用贷款更高的NVLink，而去使用PCIe。</p></li></ol><h3 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Blink/3.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：Blink Workflow</div></center><br>（1）给定深度学习任务，一旦安排并分配了一组 GPU，Blink 就能够探测机器的拓扑结构，并通过分配的 GPU 推断互连拓扑结构。<p>（2）给定拓扑，将集体通信操作建模为有向图上的流，并计算生成树的最大分数填充（TreeGen）此步骤输出一组生成树和对应于应通过它们发送多少数据的权重。</p><p>（3）CodeGen 解析生成树并生成 CUDA 代码。生成的代码与 NCCL 提供的 API 匹配，并打包到共享库 libblink.so 中。</p><p>（4）设置 LD_PRELOAD 标志，以便在调用主程序时动态加载 Blink 实现。这确保了现有程序可以在没有任何修改的情况下运行。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="TreeGen"><a href="#TreeGen" class="headerlink" title="TreeGen"></a>TreeGen</h3><h4 id="packing-max-spanning-trees"><a href="#packing-max-spanning-trees" class="headerlink" title="packing max. spanning trees"></a>packing max. spanning trees</h4><p>将从分配的资源推断出的拓扑建模为一个有向图，其中每个 GPU 是顶点 V，每个链路（NVLink 或 PCIe）标记为有向边缘 E。每个有向边缘还具有带宽比例容量。通过找到图中的有向生成树或树状图的最大填充可以达到最优速率。每个树状图 Ti 从根涡生成，沿着有向链接扩展到其它涡点。通过确定满足能力限制的最大权重树状图，可以解决在广播中确定最佳时间表的问题。</p><p>由上式，给定一个图 G，顶点为 V，边为 E，根为 r，生成树为 T1, T2, T3, … Ti，希望找到权重 w_i，使得通过任何边的权重树的总和不超过特定边的容量。</p><p>如何生成这些树？MWU算法+一个新提出的算法[5]，接近线性时间。</p><p>这样做的问题是，最优解种可能包含了太多的树。而且每个树中需要传输的数据量太小了，不能充分利用每一个link的带宽。</p><p>为了解决这个问题，新增一个限制条件：对于每一个link，要么不用这个link，要么把这个link的带宽用满。（181-&gt;6）</p><h4 id="推广到多对多的情况"><a href="#推广到多对多的情况" class="headerlink" title="推广到多对多的情况"></a>推广到多对多的情况</h4><p>为了处理多对多的操作，作者利用了这样一个事实：在这些机器中发现的所有链接本质上都是双向的，因此可以创建一个无向图，用链接的一个方向运行多对一原语，并相应地在另一个方向运行一对多原语。allreduce = reduce+broadcast<br>(这种使用两个无向树的策略也与 AllReduce 操作所需的消息数下限相匹配。</p><p>AllReduce 的进程需要发送的最小消息数是 2x|(N-1)/N|。N 个顶点上的生成树包含 N-1 条边，并考虑了两个方向上的树（一个用于 Reduce，一个用于 Broadcast），类似的，本文方法同样有 2x(N-1）条消息。)</p><h3 id="CodeGen-implementation"><a href="#CodeGen-implementation" class="headerlink" title="CodeGen(implementation)"></a>CodeGen(implementation)</h3><p>通过pipeline减小数据传输的延迟。并通过前几个iter的profile选取合适的chunck size。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>在实验评估环节，作者将Blink与目前最先进的、基于环的集体通信协议（如 NCCL2）相比较。实验结果表明，Blink 可以实现高达8倍的模型同步速度，并将端到端DNN模型训练时间减少 40%。</p><hr><p><strong>原文作者：</strong>Guanhua Wang, Shivaram Venkataraman, Amar Phanishayee, Jorgen Thelin, Nikhil R. Devanur, Ion Stoica<br><strong>原文链接：</strong><a href="https://arxiv.org/pdf/1910.04940.pdf">https://arxiv.org/pdf/1910.04940.pdf</a><br><strong>参考文献：</strong><br>[1] MLSys提前看 | 机器学习的分布式优化方法, <a href="https://zhuanlan.zhihu.com/p/108289809">https://zhuanlan.zhihu.com/p/108289809</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> MLSys </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pollux：面向深度学习有效吞吐量优化的协同适应集群调度</title>
      <link href="2021/05/14/Pollux/"/>
      <url>2021/05/14/Pollux/</url>
      
        <content type="html"><![CDATA[<h1 id="Pollux-Co-adaptive-Cluster-Scheduling-for-Goodput-Optimized-Deep-Learning"><a href="#Pollux-Co-adaptive-Cluster-Scheduling-for-Goodput-Optimized-Deep-Learning" class="headerlink" title="Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning"></a>Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning</h1><p>现有的深度学习调度器程序要求提交作业的用户指定各种超参数，如果设置得不正确，这些参数会大大降低作业性能和资源效率。在这些参数中，batch size和learning rate非常依赖于其资源分配（GPU数量），这使得在共享资源环境中很难预先确定这些参数。此外，深度学习任务能否有效利用的资源分配不仅取决于所训练模型的结构，而且还取决于batch size和learning rate。资源数量、batch size、learning rate之间的这种相互依赖性，使得用户必须同时考虑到这些因素才能配置其作业以实现高效执行和资源利用。</p><p>从根本上讲，一个有效配置的深度学习任务需要在两种期望之间取得trade off：（1）系统吞吐量，即每个单位时间处理的训练实例的数量，以及（2）统计效率，即每个处理的训练实例所取得的进展量。如图1a所示，可以通过增加batch size来增加系统吞吐量。较大的batch size可以提高更多资源的利用率（例如，较大数量的GPU）。但是，当batch size增加时，必须重新调整learning rate。否则，统计效率将会降低，使得总训练时间不会更短，从而浪费额外分配的GPU。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/1.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Batch size、resource scalability、stage of training之间的trade-off（ResNet18 on CIFAR-10，为每一个batch size调整了learning rate）</div></center><br><p>本文提出了一种混合资源调度器Pollux，可以在为共享集群中的每个深度学习任务调整batch size和learning rate的同时协同适应地分配资源。这篇工作由CMU和磐腾科技合作完成，获得OSDI’21会议的best paper。</p><p>然而，调度器或许不应该任意调整用户指定的batch size，尤其是在调整超参数的任务中，这一行为是不被允许的。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>深度训练任务有两种衡量指标：System Throughput和Statistical Efficiency。<br>系统吞吐量（System Throughput）是每单位时间处理的训练样本数。当一个DL任务分布在多个节点上时，其系统吞吐量由几个因素决定，包括（1）分配给该作业的资源的分配和放置，（2）分布式执行和同步的方法，以及（3）SGD算法使用的batch size大小。由于深度学习任务的scalibility是次线性的（sublinear），使用太多数量的GPU不一定会提升系统吞吐量。通常我们可以使用更大的batch size来提升系统的吞吐量。</p><p>然而，我们也不能一味地增加batch size，因为在增大batch size的同时，统计效率（Statistical Efficiency）会下降。统计效率是每处理一单位数据所取得的训练进度，当统计效率下降时，训练所需要的epoch数量会增加。最佳的训练效果需要在系统吞吐量和统计效率之间权衡。</p><h2 id="DL训练的Goodput"><a href="#DL训练的Goodput" class="headerlink" title="DL训练的Goodput"></a>DL训练的Goodput</h2><p>本文定义了goodput：在第t个iteration的goodput为系统吞吐量和统计效率的乘积。</p><p>$$<br>GOODPUT_t(a,m) = THROUGHPUT(a,m) * EFFICIENCY_t(m)<br>$$</p><p>其中，$a \in R^N$是allocation vector，$a_n$是从节点n分配的GPU数量，$m$是batch size。</p><p>一个DL任务在使用batch size m ≥ m0时的统计效率是相对于用m0来说，每用m个训练样本所获得的训练进度。<br>This quantity can be framed in terms of the gradient noise scale φt .<br>统计效率可以被计算为：</p><p>$$<br>EFFICIENCY_t(m) = r_tm_0/m = (\phi_t+m_0)/(\phi_t+m)<br>$$</p><h2 id="Pollux架构与设计"><a href="#Pollux架构与设计" class="headerlink" title="Pollux架构与设计"></a>Pollux架构与设计</h2><ol><li>PolluxAgent对每个任务所给定的GPU资源分配结果，找到能够最大化有效吞吐量的batch size和梯度累积步数。Pollux使用用户指定的learning rate调整规则来根据batch size调整learning rate。</li><li>PolluxSched根据集群中每个任务的有效吞吐量，为每一个任务找到合适的GPU资源分配方案。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/Pollux.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：Pollux的Co-adaptive scheduling架构</div></center><br><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>作者在16个节点、没个节点有四块NVIDIA T4 GPU的集群上进行了实验评估，使用的job trace包含微软公开的Philly集群job trace中最繁忙的8个小时内的160个job。Pollux可以将训练所需时间缩短37-50%。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/2.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表1：实验结果总结</div></center><br><p>通过图3中（A）处可以看出，在集群中的资源较紧张的时候，Pollux会使用更大的batch size以提升系统吞吐量，此时统计效率较低。在此之后，通过（B）可以看出，Pollux会给任务减少GPU资源的分配，使用较小的batch size，从而提升统计效率。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/3.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：ImageNet训练任务中Pollux与baseline的实验结果比较</div></center><br><p>除了集群调度之外，Pollux还可以应用于云端弹性训练、超参数搜索等场景。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Pollux通过优化Goodput，协同调整集群和任务的参数。Pollux 基于对有用作业完成进度提出更有意义的衡量指标，来提升深度学习作业竞争资源的公平性，并揭示了在云环境下降低深度学习成本具有新机会。</p><hr><h1 id="代码阅读笔记"><a href="#代码阅读笔记" class="headerlink" title="代码阅读笔记"></a>代码阅读笔记</h1><h2 id="Pollux-AdaptDL-部署-该Section内容主要来自我的好伙伴xxt"><a href="#Pollux-AdaptDL-部署-该Section内容主要来自我的好伙伴xxt" class="headerlink" title="Pollux / AdaptDL 部署 (该Section内容主要来自我的好伙伴xxt)"></a>Pollux / AdaptDL 部署 (该Section内容主要来自我的好伙伴xxt)</h2><blockquote><p>文档：<a href="https://adaptdl.readthedocs.io/en/latest/index.html">https://adaptdl.readthedocs.io/en/latest/index.html</a></p></blockquote><h3 id="Step-0：Deploying-MicroK8s-for-AdaptDL"><a href="#Step-0：Deploying-MicroK8s-for-AdaptDL" class="headerlink" title="Step 0：Deploying MicroK8s for AdaptDL"></a>Step 0：Deploying MicroK8s for AdaptDL</h3><p>首先安装snap，之后用snap安装microk8s，这里大概率需要用到代理</p><pre><code>$ sudo snap set system proxy.http=&quot;代理地址&quot;$ sudo snap set system proxy.https=&quot;代理地址&quot;$ sudo snap install microk8s --classic --channel=1.18/stable</code></pre><p>microk8s装好后，依次按文档执行都没有什么问题</p><pre><code>$ sudo microk8s enable dns$ sudo microk8s enable gpu storage$ sudo microk8s enable helm$ sudo microk8s helm init --stable-repo-url=https://charts.helm.sh/stable$ sudo helm repo add stable https://charts.helm.sh/stable   </code></pre><p>接下来运行</p><pre><code>$ sudo microk8s.kubectl get nodesNAME       STATUS   ROLES    AGE   VERSIONxinjin-1   Ready    &lt;none&gt;   20d   v1.18.20     </code></pre><p>之后按文档可以为了省去sudo，以及用kubectl代替microk8s.kubectl，需要执行如下命令。</p><pre><code>$ sudo usermod -a -G microk8s $USER$ mkdir -p $HOME/.kube$ sudo microk8s kubectl config view --raw &gt; $HOME/.kube/config$ sudo chown -f -R $USER ~/.kube    </code></pre><p>这些由于服务器上的$HOME路径改动过，需执行以下命令后生效</p><pre><code>sudo dpkg-reconfigure apparmorsudo rm -f /etc/apparmor.d/cache/* /var/cache/apparmor/snap.*sudo reboot</code></pre><h3 id="Step-1：Installing-the-AdaptDL-Scheduler"><a href="#Step-1：Installing-the-AdaptDL-Scheduler" class="headerlink" title="Step 1：Installing the AdaptDL Scheduler"></a>Step 1：Installing the AdaptDL Scheduler</h3><p>按文档运行</p><pre><code>$ helm install adaptdl adaptdl-sched --repo https://github.com/petuum/adaptdl/raw/helm-repo --namespace adaptdl --create-namespace --set docker-registry.enabled=true</code></pre><p>随后执行</p><pre><code>$ kubectl get pods -n adaptdl</code></pre><p>发现STATUS并非Running，而是ContainerCreating，查看日志发现是缺少一些镜像，可以手动拉取，可以参考:<a href="https://www.cnbugs.com/post-3276.html">https://www.cnbugs.com/post-3276.html</a>   </p><p>补全pause和tiller镜像后，问题自动解决，状态变为STATUS。</p><pre><code>$ kubectl get pods -n adaptdlNAME                                     READY   STATUS    RESTARTS   AGEadaptdl-adaptdl-sched-67c7bf5b59-597bj   3/3     Running   71         15dadaptdl-registry-864758d854-757s6        1/1     Running   23         15dadaptdl-validator-66ff95d99c-wsnkd       1/1     Running   23         15d</code></pre><h3 id="Step-2-Submitting-a-Simple-Job"><a href="#Step-2-Submitting-a-Simple-Job" class="headerlink" title="Step 2: Submitting a Simple Job"></a>Step 2: Submitting a Simple Job</h3><p>接下来尝试用命令行工具运行adaptdl job。</p><p>首先，运行<code>python3 -m pip install adaptdl-cli</code>，接着创建文件夹hello_world并将以下内容复制到<code>hello_world/hello_world.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> adaptdl.env</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello, world!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(adaptdl.env.share_path(), <span class="string">&quot;foo.txt&quot;</span>), <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&quot;Hello, world!&quot;</span>)</span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>接着，创建<code>hello_world/Dockerfile</code>，内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FROM python:3.7-slim</span><br><span class="line">RUN python3 -m pip install adaptdl</span><br><span class="line"></span><br><span class="line">COPY hello_world.py &#x2F;root&#x2F;hello_world.py</span><br><span class="line"></span><br><span class="line">ENV PYTHONUNBUFFERED&#x3D;true</span><br></pre></td></tr></table></figure><p>因为在提交步骤中出现</p><pre><code>python3: error while loading shared libraries: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory</code></pre><p>经排查发现是docker中缺少动态链接地址的原因，在Dockerfile中加上了<code>ENV LD_LIBRARY_PATH=:/usr/local/lib</code>   后恢复正常.</p><p>最后，拷贝以下内容至<code>hello_world/adaptdljob.yaml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: adaptdl.petuum.com&#x2F;v1</span><br><span class="line">kind: AdaptDLJob</span><br><span class="line">metadata:</span><br><span class="line">  generateName: hello-world-</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: main</span><br><span class="line">        command:</span><br><span class="line">        - python3</span><br><span class="line">        - &#x2F;root&#x2F;hello_world.py</span><br></pre></td></tr></table></figure><p>运行<code>adaptdl submit hello_world</code>后，任务提交成功，可以通过文档后续的命令查看输出并观察执行情况。</p><p>如果提交后出现<code>ModuleNotFoundError</code>，可以尝试运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install mitmproxy==6.0.2</span><br><span class="line">pip install markupsafe==2.0.1</span><br></pre></td></tr></table></figure><h2 id="Pollux-Testbed-Benchmark复现"><a href="#Pollux-Testbed-Benchmark复现" class="headerlink" title="Pollux Testbed Benchmark复现"></a>Pollux Testbed Benchmark复现</h2><p>因为我没有用aws的机器，所以复现的过程极其坎坷，在此记录一下。</p><h3 id="环境配置与调度器的安装"><a href="#环境配置与调度器的安装" class="headerlink" title="环境配置与调度器的安装"></a>环境配置与调度器的安装</h3><p>最开始以为按照<a href="https://adaptdl.readthedocs.io/en/latest/index.html">文档</a>配好调度器就可以了，但是调度的时候遇到PVC claim的问题。</p><p>然后发现了<code>benchmark</code>目录下的<code>main.tf</code>文件，应该是master和worker都需要按照文件里面的remote-exec配置好环境。所以卸载了所有安装的东西重新配环境。</p><p>在执行一些<code>kubectl apply -f</code>的时候遇到了很多：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unable to retrieve Cluster Version or Type: resource does not exist: XXX(default) with error: the server </span><br></pre></td></tr></table></figure><p>经搜索发现可能是kubernetes的版本太高了导致的，把kubernetes的版本降低到v1.21.0就解决了这些问题了。</p><p>配置好环境后执行<code>python run_workload.py pollux workloads/workload-6.csv</code>，遇到问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The push refers to repository [localhost:32000/pollux]</span><br><span class="line">Get <span class="string">&quot;http://localhost:32000/v2/&quot;</span>: dial tcp 127.0.0.1:32000: connect: connection refused</span><br></pre></td></tr></table></figure><p>经查看，端口32000是scheduler使用的，所以应该还是要先安装上scheduler才能跑benchmark。目前我还不知道如何用源码安装scheduler，所以暂时用了helm安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo helm repo add stable https://charts.helm.sh/stable</span><br><span class="line">sudo helm install adaptdl adaptdl-sched --repo https://github.com/petuum/adaptdl/raw/helm-repo --namespace default --<span class="built_in">set</span> docker-registry.enabled=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>在执行了安装scheduler的命令后，发现scheduler不能正常启动，都是pending状态：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/4.png" width="80%"></center><p>这是因为当前master节点带有taint（可以通过<code>kubectl describe node &lt;node&gt;</code>查看taint），这些tain后带有NoSchedule的标记，所以调度器找不到可以调度的资源了。我当时看到了两个taint，一个taint是说当前节点为master，另一个是Disk  pressure。可以用<code>kubectl taint nodes --all &lt;taint&gt;-</code>命令来去掉所有taint。但是有disk pressure的话后续还是会失败，清理磁盘到<code>/</code>目录的使用率在80%以下就没问题了。</p><p>调度器终于是running的状态之后，发现还是有失败的pod：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod copy-7pc4z</span><br><span class="line">Warning  FailedScheduling  115s  default-scheduler  0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims.</span><br><span class="line">$ kubectl get pvc <span class="comment"># the status of the PVC is pending</span></span><br><span class="line">$ kubectl describe pvc pollux</span><br><span class="line">Normal  Provisioning          3m35s (x27 over 92m)  rook-ceph.cephfs.csi.ceph.com_csi-cephfsplugin-provisioner-64d4468bbf-rwrfl_9d4dda0e-3e39-47e5-a5b5-b66b886b22d8  External provisioner is provisioning volume <span class="keyword">for</span> claim <span class="string">&quot;default/pollux&quot;</span></span><br><span class="line">  Normal  ExternalProvisioning  3m1s (x361 over 93m)  persistentvolume-controller                                                                                       waiting <span class="keyword">for</span> a volume to be created, either by external provisioner <span class="string">&quot;rook-ceph.cephfs.csi.ceph.com&quot;</span> or manually created by system administrator</span><br></pre></td></tr></table></figure><p>这居然是因为开源的artifact里面没有为PVC创建相应的PV！我只好把所有代码里涉及到PVC的目录全部改成hardcode的目录（<code>/mnt</code>），删掉创建PVC的pod和已经创建了的PVC，这样PVC就没有问题了。</p><p>然后发现提交了的adaptdljobs一直在pending，没有跑这些job的pod，但是可以通过kubectl get adaptdljobs看到这些job。kube describe没有报错信息。只能去查看scheduler pod的log。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs adaptdl-adaptdl-sched-cbc794b8f-br79v -c allocator </span><br></pre></td></tr></table></figure><p>发现log里有好多的信息，而且node上居然没有GPU这种资源类型：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/5.png" width="100%"></center><p>可能是GPU没有被scheduler发现。但是之前已经安装过nvidia的k8s plugin了。这种情况下只能去kube-system namespace下看看pod有什么问题。这个namespace下的所有pod都是正常运行的状态，所以要去看这些pod的log。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs nvidia-device-plugin-daemonset-n7f7q -n kube-system</span><br><span class="line">2022/05/27 07:25:44 Loading NVML</span><br><span class="line">2022/05/27 07:25:44 Failed to initialize NVML: could not load NVML library.</span><br><span class="line">2022/05/27 07:25:44 If this is a GPU node, did you <span class="built_in">set</span> the docker default runtime to `nvidia`?</span><br><span class="line">2022/05/27 07:25:44 You can check the prerequisites at: https://github.com/NVIDIA/k8s-device-plugin<span class="comment">#prerequisites</span></span><br><span class="line">2022/05/27 07:25:44 You can learn how to <span class="built_in">set</span> the runtime at: https://github.com/NVIDIA/k8s-device-plugin<span class="comment">#quick-start</span></span><br></pre></td></tr></table></figure><p>按照最后一行的link修改了<code>/etc/docker/daemon.json</code>文件并重启了docker service（<code>systemctl restart docker</code>），job终于是running状态了！</p><p>后来，在全新的环境里配置kubernetes的时候遇到问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">$ sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --v=5</span><br><span class="line">I0615 07:02:56.149065   39512 initconfiguration.go:115] detected and using CRI socket: /var/run/dockershim.sock</span><br><span class="line">I0615 07:02:56.149317   39512 interface.go:431] Looking <span class="keyword">for</span> default routes with IPv4 addresses</span><br><span class="line">I0615 07:02:56.149328   39512 interface.go:436] Default route transits interface <span class="string">&quot;eth0&quot;</span></span><br><span class="line">I0615 07:02:56.149899   39512 interface.go:208] Interface eth0 is up</span><br><span class="line">I0615 07:02:56.149966   39512 interface.go:256] Interface <span class="string">&quot;eth0&quot;</span> has 2 addresses :[10.5.0.7/24 fe80::20d:3aff:fe49:1b40/64].</span><br><span class="line">I0615 07:02:56.149982   39512 interface.go:223] Checking addr  10.5.0.7/24.</span><br><span class="line">I0615 07:02:56.149987   39512 interface.go:230] IP found 10.5.0.7</span><br><span class="line">I0615 07:02:56.150002   39512 interface.go:262] Found valid IPv4 address 10.5.0.7 <span class="keyword">for</span> interface <span class="string">&quot;eth0&quot;</span>.</span><br><span class="line">I0615 07:02:56.150007   39512 interface.go:442] Found active IP 10.5.0.7</span><br><span class="line">I0615 07:02:56.245405   39512 version.go:185] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.txt</span><br><span class="line">I0615 07:02:56.587172   39512 version.go:254] remote version is much newer: v1.24.1; falling back to: stable-1.21</span><br><span class="line">I0615 07:02:56.587215   39512 version.go:185] fetching Kubernetes version from URL: https://dl.k8s.io/release/stable-1.21.txt</span><br><span class="line">[init] Using Kubernetes version: v1.21.13</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">I0615 07:02:56.929034   39512 checks.go:582] validating Kubernetes and kubeadm version</span><br><span class="line">I0615 07:02:56.929090   39512 checks.go:167] validating <span class="keyword">if</span> the firewall is enabled and active</span><br><span class="line">I0615 07:02:56.937774   39512 checks.go:202] validating availability of port 6443</span><br><span class="line">I0615 07:02:56.937886   39512 checks.go:202] validating availability of port 10259</span><br><span class="line">I0615 07:02:56.937905   39512 checks.go:202] validating availability of port 10257</span><br><span class="line">I0615 07:02:56.937926   39512 checks.go:287] validating the existence of file /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">I0615 07:02:56.937939   39512 checks.go:287] validating the existence of file /etc/kubernetes/manifests/kube-controller-manager.yaml</span><br><span class="line">I0615 07:02:56.937949   39512 checks.go:287] validating the existence of file /etc/kubernetes/manifests/kube-scheduler.yaml</span><br><span class="line">I0615 07:02:56.937954   39512 checks.go:287] validating the existence of file /etc/kubernetes/manifests/etcd.yaml</span><br><span class="line">I0615 07:02:56.937964   39512 checks.go:437] validating <span class="keyword">if</span> the connectivity <span class="built_in">type</span> is via proxy or direct</span><br><span class="line">I0615 07:02:56.937981   39512 checks.go:476] validating http connectivity to first IP address <span class="keyword">in</span> the CIDR</span><br><span class="line">I0615 07:02:56.937999   39512 checks.go:476] validating http connectivity to first IP address <span class="keyword">in</span> the CIDR</span><br><span class="line">I0615 07:02:56.938010   39512 checks.go:103] validating the container runtime</span><br><span class="line">I0615 07:02:57.019188   39512 checks.go:129] validating <span class="keyword">if</span> the <span class="string">&quot;docker&quot;</span> service is enabled and active</span><br><span class="line">    [WARNING IsDockerSystemdCheck]: detected <span class="string">&quot;cgroupfs&quot;</span> as the Docker cgroup driver. The recommended driver is <span class="string">&quot;systemd&quot;</span>. Please follow the guide at https://kubernetes.io/docs/setup/cri/</span><br><span class="line">I0615 07:02:57.117386   39512 checks.go:336] validating the contents of file /proc/sys/net/bridge/bridge-nf-call-iptables</span><br><span class="line">I0615 07:02:57.117456   39512 checks.go:336] validating the contents of file /proc/sys/net/ipv4/ip_forward</span><br><span class="line">I0615 07:02:57.117489   39512 checks.go:654] validating whether swap is enabled or not</span><br><span class="line">I0615 07:02:57.117530   39512 checks.go:377] validating the presence of executable conntrack</span><br><span class="line">I0615 07:02:57.117575   39512 checks.go:377] validating the presence of executable ip</span><br><span class="line">I0615 07:02:57.117600   39512 checks.go:377] validating the presence of executable iptables</span><br><span class="line">I0615 07:02:57.117625   39512 checks.go:377] validating the presence of executable mount</span><br><span class="line">I0615 07:02:57.117650   39512 checks.go:377] validating the presence of executable nsenter</span><br><span class="line">I0615 07:02:57.117670   39512 checks.go:377] validating the presence of executable ebtables</span><br><span class="line">I0615 07:02:57.117691   39512 checks.go:377] validating the presence of executable ethtool</span><br><span class="line">I0615 07:02:57.117712   39512 checks.go:377] validating the presence of executable socat</span><br><span class="line">    [WARNING FileExisting-socat]: socat not found <span class="keyword">in</span> system path</span><br><span class="line">I0615 07:02:57.117753   39512 checks.go:377] validating the presence of executable tc</span><br><span class="line">I0615 07:02:57.117774   39512 checks.go:377] validating the presence of executable touch</span><br><span class="line">I0615 07:02:57.117795   39512 checks.go:525] running all checks</span><br><span class="line">I0615 07:02:57.209961   39512 checks.go:408] checking whether the given node name is valid and reachable using net.LookupHost</span><br><span class="line">I0615 07:02:57.210512   39512 checks.go:623] validating kubelet version</span><br><span class="line">I0615 07:02:57.270497   39512 checks.go:129] validating <span class="keyword">if</span> the <span class="string">&quot;kubelet&quot;</span> service is enabled and active</span><br><span class="line">I0615 07:02:57.279877   39512 checks.go:202] validating availability of port 10250</span><br><span class="line">I0615 07:02:57.279947   39512 checks.go:202] validating availability of port 2379</span><br><span class="line">I0615 07:02:57.279966   39512 checks.go:202] validating availability of port 2380</span><br><span class="line">I0615 07:02:57.279985   39512 checks.go:250] validating the existence and emptiness of directory /var/lib/etcd</span><br><span class="line">[preflight] Some fatal errors occurred:</span><br><span class="line">    [ERROR FileExisting-conntrack]: conntrack not found <span class="keyword">in</span> system path</span><br><span class="line">[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</span><br><span class="line">error execution phase preflight</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:235</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:421</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:207</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdInit.func1</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/init.go:152</span><br><span class="line">k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:850</span><br><span class="line">k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:958</span><br><span class="line">k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/spf13/cobra/command.go:895</span><br><span class="line">k8s.io/kubernetes/cmd/kubeadm/app.Run</span><br><span class="line">    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50</span><br><span class="line">main.main</span><br><span class="line">    _output/dockerized/go/src/k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25</span><br><span class="line">runtime.main</span><br><span class="line">    /usr/<span class="built_in">local</span>/go/src/runtime/proc.go:225</span><br><span class="line">runtime.goexit</span><br><span class="line">    /usr/<span class="built_in">local</span>/go/src/runtime/asm_amd64.s:1371</span><br></pre></td></tr></table></figure><p>是因为没有安装conntrack：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install conntrack</span><br></pre></td></tr></table></figure><h3 id="作业的运行"><a href="#作业的运行" class="headerlink" title="作业的运行"></a>作业的运行</h3><p>虽然job是running状态，但是job的一些行为十分诡异，需要注意解决。</p><h4 id="问题1-OSError-Errno-30-Read-only-file-system-‘-mnt-tensorboard’"><a href="#问题1-OSError-Errno-30-Read-only-file-system-‘-mnt-tensorboard’" class="headerlink" title="问题1 OSError: [Errno 30] Read-only file system: ‘/mnt/tensorboard’"></a>问题1 OSError: [Errno 30] Read-only file system: ‘/mnt/tensorboard’</h4><p>在job还在运行的时候，具体的job的log可以通过kubectl logs <pod>看到。<br>通过log发现：<code>OSError: [Errno 30] Read-only file system: &#39;/mnt/tensorboard/&#39;</code><br>无论是在docker外面还是里面，检查<code>/mnt</code>下的目录的权限，都是所有人可读写的，甚至这个目录存在的时候，会出现在里面写文件失败的情况，也是Read-only file system这种报错。<br>最后发现是每一个job的adaptdljob.yaml里有这样的一段：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">volumeMounts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">    <span class="attr">mountPath:</span> <span class="string">/mnt</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>把true改成false，删掉daemonsets，删掉build好的container镜像，然后重新运行脚本build镜像就没有这个报错了。</p><h4 id="问题2-把模型放到GPU内存里过程缓慢，需要10min，且训练卡死在forward阶段"><a href="#问题2-把模型放到GPU内存里过程缓慢，需要10min，且训练卡死在forward阶段" class="headerlink" title="问题2 把模型放到GPU内存里过程缓慢，需要10min，且训练卡死在forward阶段"></a>问题2 把模型放到GPU内存里过程缓慢，需要10min，且训练卡死在forward阶段</h4><p>经过profile发现<code>net = net.to(device)</code>需要10min之久，且训练会卡死在第一个<code>outputs = net(inputs)</code>。GPU的内存有一定的使用，但是计算上的利用率一直是0%。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Pollux/6.png" width="80%"></center>经过百度发现可能是CUDA、cuDNN、python等版本不匹配的问题。然后我发现在container里面，`torch.version.cuda`的CUDA版本是10.X，但是`nvidia-smi`得到的CUDA版本是11.X，而在container外面，两种方式得到的CUDA版本都是11.X，所以问题一定出在了container的环境上。把每一种job的Dockerfile第一行的镜像版本都改成`nvcr.io/nvidia/pytorch:21.10-py3`之后，还是删掉daemonsets，删掉build好的container镜像，然后重新运行脚本build镜像，一些都变得正常了。<p>后来在全新的环境里，又遇到了全新的问题，<a href="https://github.com/petuum/adaptdl/issues/124">https://github.com/petuum/adaptdl/issues/124</a></p><h2 id="Simulator代码阅读"><a href="#Simulator代码阅读" class="headerlink" title="Simulator代码阅读"></a><a href="https://github.com/petuum/adaptdl/tree/osdi21-artifact">Simulator代码阅读</a></h2><ul><li><code>simulator/traces/model/placements.csv</code>文件的含义，可以通过<code>simulator/application.py</code>看到<ul><li>第一列的字符串，有几个字符，就相当于有几个node</li><li>每个数字代表这个node上有几块GPU在用</li></ul></li></ul><hr><p><strong>原文作者：</strong>Aurick Qiao, Sang Keun Choe, Suhas Jayaram Subramanya, Willie Neiswanger, Qirong Ho, Hao Zhang, Gregory R. Ganger, Eric P. Xing<br><strong>原文链接：</strong><a href="https://arxiv.org/pdf/2008.12260v2.pdf">https://arxiv.org/pdf/2008.12260v2.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/petuum/adaptdl">https://github.com/petuum/adaptdl</a><br><strong>参考文献：</strong>[1] 计算机系统软件顶会OSDI 2021最佳论文出炉，邢波团队研究入选, <a href="https://zhuanlan.zhihu.com/p/390040932">https://zhuanlan.zhihu.com/p/390040932</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 集群调度 </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> OSDI </tag>
            
            <tag> 弹性训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AFS：分布式深度学习中的弹性资源共享</title>
      <link href="2021/04/13/AFS/"/>
      <url>2021/04/13/AFS/</url>
      
        <content type="html"><![CDATA[<h1 id="Elastic-Resource-Sharing-for-Distributed-Deep-Learning"><a href="#Elastic-Resource-Sharing-for-Distributed-Deep-Learning" class="headerlink" title="Elastic Resource Sharing for Distributed Deep Learning"></a>Elastic Resource Sharing for Distributed Deep Learning</h1><p>这篇文章是KAIST的박경수教授指导的工作，发表于NSDI 2021。作者提到，资源的分配和调度策略对于深度学习训练任务（DLT jobs）的平均任务完成时间（JCT， job complete time）有很大的影响。“最短剩余时间优先”等传统的方法在这方面表现得很差（只盲目地优先短的任务太粗粒度了）。这篇文章发现，对于加速DLT任务的执行来说，：（1）资源效率（resource efficiency，即一块GPU的边际吞吐量，如果一个任务使用的GPU数量变多，相应资源效率会下降）比段任务优先更重要；（2）如果对已有任务采用贪心算法会使得JCT增加。根据这些发现，作者提出了AFS来平衡资源效率和短任务的优先，与此同时限制资源分配中的不现实的优化。作者同时建立了一个透明化处理对模型的并行训练，并高效处理共享资源调整的DLT系统CoDDL。</p><h2 id="研究背景和动机"><a href="#研究背景和动机" class="headerlink" title="研究背景和动机"></a>研究背景和动机</h2><h3 id="问题与挑战"><a href="#问题与挑战" class="headerlink" title="问题与挑战"></a>问题与挑战</h3><p>减少集群中任务的JCT很重要。本文的目标：最小化JCT。</p><p>现有工作的局限性：</p><ul><li>Tiresias和Themis等调度器可以提高“结束早的”任务的优先级，却忽略了长时间运行的DL任务</li><li>Tiresias和Gandiva等调度器的算法不支持弹性资源分配（elastic training）</li></ul><p>本文提出了一种可以减小集群中<strong>平均JCT</strong>的动态调度方法，该方法中每个DL任务使用的GPU数量会不断调整。</p><p>挑战：</p><ul><li><p>算法层面：需要同时考虑到任务时长和resource efficiency -&gt; 设计新的调度算法</p></li><li><p>系统层面：现有auto-scaling和GPU间通信的API的overhead太大 -&gt; 实现了CoDDL这个系统来减小overhead</p></li></ul><h2 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h2><p>本文的弹性资源共享希望可以平衡短任务和高效任务之间的优先权。寻找最优分配策略看起来是一个NP完全问题。此外，未来作业的到达在调度时是未知的，可能会严重影响之前的资源重新分配决策。本文首先通过对简化问题的严格分析并观察。</p><h3 id="算法概述"><a href="#算法概述" class="headerlink" title="算法概述"></a>算法概述</h3><ul><li><p>问题的定义</p><ul><li>每个DLT job $j_k$在任一时刻被提交到一个有$M$块GPU的集群 </li><li>集群中的每个job都采用bulk-synchronous-parallel (BSP)模式训练</li><li>一共有$n$个job，对于每个job集群需要两次重新分配：开始和结束。每次重新分配都是一个event。</li><li>找到n维向量$R_u$={$r_{1,u}$, $r_{2,u}$, …,$r_{n,u}$}, $1 ≤ u ≤ 2n$，$r_{k,u}$是在第$u$个event之后分配给$j_k$的GPU数量</li></ul></li><li><p>方法</p><ul><li>找到最优的R_u需要对还没有到达的future job的知识（详见附录A）。</li><li>因此本文希望找到一个可以减少平均JCT的启发式策略。</li><li>不断地使用贪心策略依赖于这样一个假设：已完成任务释放的资源只会被已有任务使用，这在一个新的任务到来时会显得“过于乐观”。</li><li>这说明，调度器假设集群中所有正在执行的任务都只会不断地增加它所使用的资源数量，而不会减少；这和事实严重不符。</li></ul></li><li><p><strong>AFS假设：假设集群中每一个任务在未来的资源使用会保持不变</strong>，然后在此基础上找到最优的方案。这很简单，而且接近实际的集群环境（大多数情况下资源竞争的程度不会有特别大的变化）。</p></li></ul><p><strong>其中，AFS在为任务调整使用的GPU数量的时候是strong sclaing，即global batch size不变。</strong></p><h3 id="通过分析两个任务的情况得到的启示"><a href="#通过分析两个任务的情况得到的启示" class="headerlink" title="通过分析两个任务的情况得到的启示"></a>通过分析两个任务的情况得到的启示</h3><h4 id="两个任务，一块GPU"><a href="#两个任务，一块GPU" class="headerlink" title="两个任务，一块GPU"></a>两个任务，一块GPU</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/AFS/1.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：timeline of jobs示例</div></center><br><p>此时，若任务a更短，则($r_a,1$, r_a,2, r_b,a, r_b,2) = (1, 0, 0, 1)；否则，(r_a,1, r_a,2, r_b,a, r_b,2) = (0, 1, 1, 0)</p><h4 id="两个任务，M块GPU-M-gt-1-，待分配GPU为1块"><a href="#两个任务，M块GPU-M-gt-1-，待分配GPU为1块" class="headerlink" title="两个任务，M块GPU (M &gt; 1)，待分配GPU为1块"></a>两个任务，M块GPU (M &gt; 1)，待分配GPU为1块</h4><p>假设任务a比b先完成。假设已经有M-1块GPU分配给了两个任务如何决定剩下的一块GPU要分给谁？</p><p>用$w_k$来表示任务k所需要的训练iteration数量，$p_k,u$表示任务k分配有$r_k,u$块GPU时的吞吐率。可以用这些符号表示出t1和t2:</p><p>$$<br>t_1 = \frac{w_a} {p_{a, 1}} , t_2 = \frac{w_b - p_{b, 1}t_1}{p_{b, 2}}<br>$$</p><p>将这一块GPU分配给不同的任务，最终的任务完成时间不同。同样可以用这些符号表达出不同情况下的任务结束时间。需要找出可以使得完成时间最早的GPU分配方案。</p><h4 id="两个任务，M块GPU-M-gt-1-，待分配GPU多于1块"><a href="#两个任务，M块GPU-M-gt-1-，待分配GPU多于1块" class="headerlink" title="两个任务，M块GPU (M &gt; 1)，待分配GPU多于1块"></a>两个任务，M块GPU (M &gt; 1)，待分配GPU多于1块</h4><p>反复执行以上步骤，直到分配完成</p><h4 id="处理后续的任务"><a href="#处理后续的任务" class="headerlink" title="处理后续的任务"></a>处理后续的任务</h4><p>通过正文和附录里对多种情况的分析，说明（1）在未来有新的任务加入时，可能会使得之前最优的资源分配不再是最优选择；（2）如果新加入的任务使得资源竞争更加激烈，把更多的GPU分配给所需时间长但是更高效的任务是更好的选择。</p><h4 id="Apathetic-Future-Share"><a href="#Apathetic-Future-Share" class="headerlink" title="Apathetic Future Share"></a>Apathetic Future Share</h4><p>AFS假设：任何当前已有的任务在未来得到的资源分配都和现在的相同，即使这个过程中有的任务结束并释放了资源。基于这个假设，之前用于计算GPU分配的公式变得非常简单：</p><p>$$<br>\frac{p_{b,1}’-p_{b,1}} {p_{b,1}’} &gt; \frac{p_{a,1}’-p_{a,1}} {p_{a,1}’}<br>$$</p><p>其中$p_{k,u}’$表示比$p_{k,u}$多用一块GPU的吞吐率。</p><p>AFS假设在一些情况下并不是最优的，但是作者认为这些情况在实际集群中不常见。</p><h3 id="多任务的AFS算法"><a href="#多任务的AFS算法" class="headerlink" title="多任务的AFS算法"></a>多任务的AFS算法</h3><h4 id="从2个任务扩展到n个任务"><a href="#从2个任务扩展到n个任务" class="headerlink" title="从2个任务扩展到n个任务"></a>从2个任务扩展到n个任务</h4><p>AFS-L：n个任务量量比较优先级，对M块GPU一次分配，时间复杂度为O(M·n)。</p><p>AFS-L算法分为两个阶段：（1）假设所有任务都在单GPU上运行，如果M比任务的数量少，算法停止，否则（2）通过衡量每个任务所需时长和资源效率，决定如何分配剩下的GPU。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/AFS/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">算法1：AFS-L 资源共享</div></center><br><h4 id="处理任务时长未知的情况"><a href="#处理任务时长未知的情况" class="headerlink" title="处理任务时长未知的情况"></a>处理任务时长未知的情况</h4><p>AFS-L算法要求已知任务所需时长。作者把AFS-L算法中找出优先级最高的任务的计算方法，修改为衡量之前提到的简化后的公式。如果通过这个公式也找不到更好的方案，最随机分配GPU。修改后的整个算法被称为AFS-P。在AFS-P中，如果任务的数量多于M，则通过记录下来的每个任务历史的资源分配情况，决定哪些任务可以优先得到GPU。</p><h2 id="CoDDL系统"><a href="#CoDDL系统" class="headerlink" title="CoDDL系统"></a>CoDDL系统</h2><p>CoDDL系统在文中的介绍并不详细，该系统基于团队在OSDI‘18的一篇poster：Efficient Resource Sharing for Distributed Deep Learning。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/AFS/3.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：CoDDL系统架构</div></center><br><p>用户只需提交单机代码，该框架就可以运行数据并行的训练。CoDDL实现了自己的通信栈，在scaling时将无需通信的GPU op与新加入GPU的初始化overlap。</p><p>作者没有提通信上的overlap；据说做一次scale out只需4ms。但是无论在本篇论文中还是之前的poster中，都没有介绍overhead为什么可以这么低。</p><p>作者声称share shrinking中只需发信号给通信栈然后停掉相应kernel，这是zero cost的，只需4ms。</p><p>有许多reconfiguration同时到来的时候，每个worker可以“取消”正在进行初始化的reconfiguration。</p><p>CoDDL有自己的容错机制，而且支持job migration。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>对AFS算法从模拟和实际集群实验两方面进行评估。使用了有16台机器、每台机器4块GTX 1080 GPU的集群进行实验。workload基于微软137天的实际的trace。根据trace计算每个模型训练所需的iteration数，并根据此来进行后面对JCT的模拟。</p><p>对算法的评估分为两组：一组的算法中，任务时长已知，另一组的算法中任务时长未知。对于实际集群中的实验，都从平均JCT、最长完工时间、GPU利用率等等多个方面进行了评估。AFS在各个方面的表现和其他的调度算法相比，都合理且优秀。</p><p>作者还评估了CoDDL share re-adjustment的效率。没有和其他论文的方法进行比较，而是和没有reconfiguration cancelling的方法，以及既没有op overlap也没有reconfiguration cancelling的方法进行了比较，证明这两方面的优化可以让任务更快结束。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇文章的项目开始于2017年，到2021年才发表。AFS可以将集群中任务的平均JCT提升2.2倍至3.1倍。该方法只考虑了数据并行。该算法没有考虑用户对使用的GPU数量有自己的需求，如果用户要求一个任务使用大量的GPU，但是AFS算法认为这样不利于平均JCT，是否就无法满足用户的要求进行gang schedule了呢？另外，本文实验中仅考虑了训练一个模型所需的iteration数，没有考虑在改变了训练所用GPU数量之后，训练了这些iteration后模型的准确率是否达到了标准。</p><p>其他问题：不支持模型并行；没有考虑GPU之间的topology。</p><hr><p><strong>原文作者：</strong>Changho Hwang, Taehyun Kim（共同一作）, Sunghyun Kim, Jinwoo Shin, KyoungSoo Park<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/nsdi21-hwang.pdf">https://www.usenix.org/system/files/nsdi21-hwang.pdf</a><br><strong>CoDDL Poster：</strong><a href="https://chhwang.github.io/pubs/osdi18_poster_hwang.pdf">https://chhwang.github.io/pubs/osdi18_poster_hwang.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> NSDI </tag>
            
            <tag> 集群调度 </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rammer：如何通过全局视角编译深度学习计算</title>
      <link href="2021/04/09/Rammer/"/>
      <url>2021/04/09/Rammer/</url>
      
        <content type="html"><![CDATA[<h1 id="Rammer-Enabling-Holistic-Deep-Learning-Compiler-Optimizations-with-rTasks"><a href="#Rammer-Enabling-Holistic-Deep-Learning-Compiler-Optimizations-with-rTasks" class="headerlink" title="Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks"></a>Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks</h1><p>传统深度学习框架由于其自身的局限性，如今还远没有充分发挥出硬件的计算性能。微软亚洲研究院的研究员们在一些测试集上发现，现有的深度学习模型只能用到 GPU 2%到40%的性能。传统的深度学习框架通常通过分层调度来将一个深度学习模型调度到硬件设备（通常是 GPU 这样的协处理器）上进行计算。首先，在上层，深度学习模型通常会被抽象为由算子（Operator）和依赖关系构建而成的数据流图（DFG），深度学习框架主要负责将每个算子按照正确的依赖关系依次调度到下层设备上；接着，在下层硬件设备上（如 GPU），会有一个硬件的调度器将每个算子根据其内部并行性调度到硬件内的并行计算核上。这样两层调度的模型尽管较为简洁明晰，但在实际的部署中，两个调度层互相不感知会导致较大的调度开销，以及较低的硬件利用率。</p><p>传统深度学习框架在实现全面神经网络优化上的核心障碍在于：首先，现有的基于数据流图的抽象无法表示算子内部的并行性，对于由深度学习框架控制的图调度器完全是黑盒；其次，硬件厂商往往将细粒度的任务调度隐藏在硬件中，对上层框架调度也是黑盒。</p><p>针对现有深度学习框架的局限，微软亚洲研究院和北京大学、上海科技大学合作提出了一种可以成倍甚至几十倍地提升深度学习计算速度的编译框架RAMMER。这篇文章发表于OSDI 2020。</p><h2 id="动机与挑战"><a href="#动机与挑战" class="headerlink" title="动机与挑战"></a>动机与挑战</h2><p>目前，对两个层面的并行是通过“两层”的调度来进行的。DFG （data flow graph） scheduler实现inter-op的并行：通过op之间的依赖关系“发射”准备就绪的op；在不同的硬件上，op被看作是库中的“黑盒”function。硬件上的scheduler实现op内的并行：它把op内部的计算映射到加速器上的并行执行单元（EUs）上。在shchedule的overhead可以忽略且intr-op的并行计算可以充分利用全部的EU的情况下，这样两层的架构可以运行得很好。</p><p>然而，通常情况下，GPU设备的利用率会很低，调度op的开销也会很大。另外，这种两层的架构忽视了inter和intra op并行之间微妙的相互作用。比如两个可以同时计算的op可能尽可能地占用更多的EU资源，但是每个EU的利用率并不高。这会导致其中一个op等待另一个op执行完，才能获得足够的资源。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Rammer/1.png" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：已有方法（a）忽视了inter和intra op并行之间的相互作用，十分不高效；而（b）是优化后的方案</div></center><br><p>因此，Rammer把inter-op和intra-op共同进行调度。有几点挑战：</p><ul><li><p>挑战1：op的intra-op并行是由硬件进行调度的，并没有给软件暴露出细粒度的intra-op并行计算方式</p><p>解决方法：定义了rTask作为在EU上计算的最小单元，并把细粒度的intra-op parallelism暴露出来</p></li><li><p>挑战2：GPU等加速器没有暴露intra-op调度的接口</p><p>解决方法：Rammer把硬件加速器抽象成虚拟的并行设备，把rTask调度到vEU上，每一个vEU再对应一个真实的加速器上的EU。</p></li><li><p>挑战3：细粒度的调度引入跟多的调度overhead</p><p>解决方法：多数DNN的计算图是“可预测的”，且op的performance是deterministic的。因此可以在编译阶段就生成执行方案</p></li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Rammer/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：（a）传统深度学习框架，（b）RAMMER 深度学习编译框架</div></center><h2 id="Wavefront-scheduling-policy"><a href="#Wavefront-scheduling-policy" class="headerlink" title="Wavefront scheduling policy"></a>Wavefront scheduling policy</h2><p>Rammer中为每一个op都实现了一种或多种kernel（具体的实现方式）。有的kernel计算很块（fast），有的kernel使用资源较少（resource-efficient）。Rammer默认使用fastest kernels；在资源比较紧张、可以占满所有EU的时候，使用resource-efficient kernels。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>Rammer可以提高GPU的利用率，但是在某些情况下，讲两层的op并行调度架构改为1层的效果并不明显。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Rammer/3.png" width="85%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：GPU利用率</div></center><br><p>Rammer可以减少调度时间。但是没有评估Rammer导致的compile time的增加。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Rammer/4.png" width="85%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：Scheduling Overhead</div></center><hr><p><strong>原文作者：</strong>Lingxiao Ma∗, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, Lidong Zhou<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/osdi20-ma.pdf">https://www.usenix.org/system/files/osdi20-ma.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/microsoft/nnfusion">https://github.com/microsoft/nnfusion</a><br><strong>参考文献：</strong>[1] OSDI 2020 | 微软亚洲研究院论文一览 <a href="https://mp.weixin.qq.com/s/vHeXvAeEPPiTls349yjkAQ">https://mp.weixin.qq.com/s/vHeXvAeEPPiTls349yjkAQ</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习系统 </tag>
            
            <tag> OSDI </tag>
            
            <tag> 推理系统 </tag>
            
            <tag> 深度学习编译器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BytePS：加速异构集群中分布式训练的统一架构</title>
      <link href="2021/03/15/BytePS/"/>
      <url>2021/03/15/BytePS/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Unified-Architecture-for-Accelerating-Distributed-DNN-Training-in-Heterogeneous-GPU-CPU-Clusters"><a href="#A-Unified-Architecture-for-Accelerating-Distributed-DNN-Training-in-Heterogeneous-GPU-CPU-Clusters" class="headerlink" title="A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters"></a>A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters</h1><p>本文是字节跳动、清华大学和Google合作的项目，提出一种分布式DNN训练的统一架构<strong>BytePS</strong>，并延续了字节跳动在RDMA方面的研究，利用了RDMA高速网络的特性对集群的通信和算力资源利用率进行优化。</p><h2 id="背景和动机"><a href="#背景和动机" class="headerlink" title="背景和动机"></a>背景和动机</h2><p>即使目前已经有了许多，数据并行的效率远远没有达到理想的情况。</p><p>本文希望解决的三个问题：</p><ul><li>多机之间通信并不充分</li><li>单机内部GPU之间的通信也有瓶颈</li><li>CPU的性能限制了训练的效率</li></ul><p>通过从字节跳动异构集群中收集的3个月的trace，可以看出：（1）CPU并没有被很好的利用；（2）很多服务器运行的是非分布式任务，这些机器的贷款并没有被很好的利用。这提供了一个很好的机会：异构集群中有剩余的CPU和带宽。本文的设计目标是去充分利用这些没有被利用起来的资源。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BytePS/1.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：集群中收集到的DNN训练相关数据</div></center><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BytePS/2.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：BytePS架构</div></center><h3 id="多机之间的通信"><a href="#多机之间的通信" class="headerlink" title="多机之间的通信"></a>多机之间的通信</h3><p>一方面，PS只利用GPU和CPU之间的带宽进行通信。如果CPU服务器的数量不充足，那么GPU服务器的带宽就没有被充分利用。另一方面，all-reduce只利用GPU服务器之间的连接，CPU服务器的带宽完全没有被利用。所以最好的策略是把二者结合起来，这样所有机器之间的连接都能被利用，并且能充分利用CPU资源。这带来一个问题：如何分割这些link workload？</p><p>用x和y分别代表CPU-GPU traffic所占百分比和GPU-GPU traffic所占百分比。可以计算出最佳的策略下x和y的值。其中，n和k分别是GPU和CPU服务器的数量。在理论上，这样的partition策略可以达到最短的通信时间。<br>$$<br>x = \frac{2k(n - 1)} {n^2+kn-2k}, y = \frac{n(n - k)} {n^2+kn-2k}<br>$$</p><h3 id="单机内部的通信"><a href="#单机内部的通信" class="headerlink" title="单机内部的通信"></a>单机内部的通信</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BytePS/3.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：只有PCIe拓扑结构时的优化方案</div></center><br><p>如图所示，当单机之间只有PCIe拓扑结构时，瓶颈在于CPU和PCIe switch之间的连接。本文希望能够尽可能减少这个连接之间通信。然而，像MPI或者NCCL等通信库直接对这8个GPU做all reduce。根据all-reduce算法可以计算出，直接做all reduce时，bottleneck link上有7M/4的通信量，M是每个GPU上的模型大小。这个通信量对于bottleneck link来说太大了。</p><p>BytePS通过<strong>CPU-assisted aggregation</strong>的方法来解决这个问题。首先，在同一个PCIe switch下的四个GPU之间进行一次local reduce-scatter。这样每块GPU上有1/4的aggregated gradient。然后每个GPU把它所拥有的四分之一copy到主存中。最后，CPU把两个PCIe switch下的gradient加和。现在在bottleneck link上的通信量只有M。理论上该方法比MPI/NCCL快24%。</p><p>对于有NVlink的GPU machine，可以用类似的方法分析瓶颈所在的连接，并设计方法减少瓶颈连接上的通信量。</p><p>对于不同的拓扑结构来说，最优的通信算法不同，并不存在一个对所有拓扑结构都适用的算法。</p><h3 id="解决CPU瓶颈"><a href="#解决CPU瓶颈" class="headerlink" title="解决CPU瓶颈"></a>解决CPU瓶颈</h3><p>现象：optimization可以分成两个阶段：梯度加和和参数更新。前者对CPU友好，后者对于CPU来说计算量过大。</p><p>parameter server把forward和backward放在GPU上，把整个optimization放在CPU上。本文提出<strong>Summation Service</strong>：把optimization中的参数更新放到GPU上计算。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BytePS/4.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：不同策略下component的放置</div></center><br>但是这样是否会有过多的context switch从而带来这方面的overhead，作者没有讨论。<h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>多种模型、框架、baseline。</p><p>多机之间通信：byteps的通信表现可以达到几乎最优；CPU服务器越多，BytePS的表现越好。</p><p>单机内部通信：对于有PCIe和NVlink的机器，都有很大的performance提升。</p><p>End-to end scalability：在各种情况下的throughput都有所提升，GPU数量越多，提升越大。</p><hr><p><strong>原文作者：</strong>Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, Chuanxiong Guo<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/osdi20-jiang.pdf">https://www.usenix.org/system/files/osdi20-jiang.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/alibaba/GPU-scheduler-for-deep-learning">https://github.com/alibaba/GPU-scheduler-for-deep-learning</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> SOSP </tag>
            
            <tag> 通信调度 </tag>
            
            <tag> 深度学习框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch Distributed：加速数据并行训练的经验</title>
      <link href="2021/03/08/DDP/"/>
      <url>2021/03/08/DDP/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training"><a href="#PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training" class="headerlink" title="PyTorch Distributed: Experiences on Accelerating Data Parallel Training"></a>PyTorch Distributed: Experiences on Accelerating Data Parallel Training</h1><p>本文介绍了PyTorch DDP模块的设计、实现和评估。提供一个通用的数据并行训练package有三方面考验：</p><ul><li><strong>数学上的等价：</strong>需要保证和本地训练一样的训练收益</li><li><strong>无需大量修改用户代码的以及截获性的API：</strong>方便用户运行数据并行训练，也要允许用户自定义通信和优化</li><li><strong>高效：</strong>较高的训练吞吐量</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>训练DNN模型的三步：</p><ol><li>Forward pass: 计算损失</li><li>Backward pass: 计算梯度</li><li>Optimizer step: 更新参数</li></ol><p>PyTorch DDP采用all-reduce通信。All-recude是一种同步的通信机制，因为all-reduce只有在所有进程都准备好之后才能开始通信。与之相反，PS中采用的是P2P的通信方式。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="梯度Reduction"><a href="#梯度Reduction" class="headerlink" title="梯度Reduction"></a>梯度Reduction</h3><p>在过去的版本中，梯度Reduction的计算方法一直在不断改进。</p><ol><li><p><strong>Naive solution：</strong>DDP控制所有的训练进程（1）从同一个模型状态开始训练；（2）在每个iteration用同样的梯度。<br>通过在每次local backward之后梯度同步，或者加hook就可以实现（2），但是有两个performance上的问题：<br>小tensor中collective communication效率非常低<br>把梯度计算和同步分开之后，就不能把它们overlap了</p></li><li><p><strong>Gradient bucketing：</strong>Collective communications在大tensor上通信效率更高。因此，可以通过gradient reduction将几个梯度打包成一个allreduce操作。然而，为了让通信和计算overlap，DDP不能把所有的梯度都打包到一个allreduce里。</p></li><li><p><strong>计算和通信overlap：</strong>在bucketing中，DDP只需要等一个bucket中的所有数据都准备好通信。需要注意（1）在所有进程中reduce的顺序一定要相同；（2）有时不同进程里backward中gradient计算顺序不同，或有的gradient被跳过，此时会导致hang。</p></li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/DDP/2.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：分布式梯度reduction</div></center><br><ol start="4"><li><strong>梯度累积：</strong>每n个iteration进行一次allreduce。</li></ol><h3 id="Collective-Communication"><a href="#Collective-Communication" class="headerlink" title="Collective Communication"></a>Collective Communication</h3><p>DDP建立在NCCL、Gloo和MPI等通信库之上，把通信库里的API打包到ProcessGroup API中。在DDP里，worker需要加入通信组才能进行相关通信。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><ul><li>Python前端：有很多可以configure的节点，例如process group等等；支持单机多卡的模型并行；支持模型buffer，rank 0负责存储buffer的内容，在forward前将rank 0上buffer的内容broadcast到其他设备上。</li><li>核心梯度reduction：确保同一bucket中的parameter都来自同一个device；通过一个count来判断当前backward到了第几层，从而在合适的时候allreduce；默认bucket size为25M；在CPU上创建位图来保存本地没有使用的参数信息，并通过一个额外的allreduce得到global bitmap。</li></ul><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><ul><li>比较了不同模型、使用不同backend、有无通信和训练的overlap，并将latency breakdown为不同的部分来分析。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/DDP/3.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：breakdown每个iteration的latency</div></center><br><ul><li>比较了不同的bucket size的影响。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/DDP/4.png" width="95%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：bucket size v.s. latency</div></center><br><ul><li><p>比较了使用不同数量GPU时的latency、不同同步步调的latency以及loss。</p></li><li><p>DDP通过使用多个round-robin（轮询调度）进程组从而充分利用带宽。实验比较了使用不同数量进程组对latency的影响。</p></li></ul><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>没有适用于所有情况的configuration，但是可以总结出一些规则来帮助找到最佳configuration：</p><ul><li><p>通信backend：多数情况下，NCCL比Gloo快</p></li><li><p>Bucket size：随着模型的增大而增大</p></li><li><p>Resource allocation：用NCCL时，建议把所有的worker都放到同一台机器上的同一个进程组中</p></li></ul><p>未来优化方向:</p><ul><li><p>梯度顺序预测：使用自动梯度计算hook记录backward的order，并相应地向bucket mapping中更新参数</p></li><li><p>Layer dropping：在forward的过程中随机drop掉几层（加速训练，避免过拟合）；与此同时相应修改parameter-to-bucket mapping，或从bucket level drop</p></li><li><p>梯度压缩： 只通信需要高精度的梯度</p></li></ul><hr><p><strong>原文作者：</strong>Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, Soumith Chintala†<br><strong>原文链接：</strong><a href="https://arxiv.org/pdf/2006.15704.pdf">https://arxiv.org/pdf/2006.15704.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a><br><strong>参考文献：</strong>[1] PyTorch Distributed: Experiences on Accelerating Data Parallel Training <a href="https://ruipeterpan.gitbook.io/paper-reading-notes/machine-learning-systems/index/pytorch-distributed-experiences-on-accelerating-data-parallel-training">https://ruipeterpan.gitbook.io/paper-reading-notes/machine-learning-systems/index/pytorch-distributed-experiences-on-accelerating-data-parallel-training</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> 深度学习框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elan: 面向深度学习的通用弹性训练</title>
      <link href="2021/03/07/Elan/"/>
      <url>2021/03/07/Elan/</url>
      
        <content type="html"><![CDATA[<h1 id="Elan-Towards-Generic-and-Efficient-Elastic-Training-for-Deep-Learning"><a href="#Elan-Towards-Generic-and-Efficient-Elastic-Training-for-Deep-Learning" class="headerlink" title="Elan: Towards Generic and Efficient Elastic Training for Deep Learning"></a>Elan: Towards Generic and Efficient Elastic Training for Deep Learning</h1><p>Elasticity可以带来很多好处：（1）有利于集群管理；（2）有许多动态调整batch size，或需要动态资源的优化算法。但是深度学习框架没有很好的支持elastic training，如果在固定数量的资源上运行这些算法，甚至会导致运行效率减慢。因此，需要有一个系统很好地支持elastic training。实现这样一个系统主要由两方面的挑战：（1）如何找到一个既能保证训练效率又能保证convergence的合适的batch size；（2）如何高效进行state replication。</p><p>本文由清华大学和商汤合作，获得了ICDCS 2020的最佳论文提名奖。本文仅适用于数据并行。</p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>Elan为每一个任务设置一个application master（AM）。AM负责为调度器提供资源调整服务，并为所有的worker进行资源调整。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/1.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Elan的资源调整过程（以sclae out为例）</div></center><br><ol><li>Request：调度器请求资源调整（scale in、scale out或migration），将请求发给AM并启动新的worker；</li><li>Report：新的worker在启动并完成初始化后，将自身状态告知AM；</li><li>Coordinate：已有的worker和AM进行协作调整；</li><li>State Replication：将训练状态复制给新的worker；</li><li>State Adjustment：例如重新划分数据、重构通信组、调整batch size等。</li></ol><h2 id="混合scaling机制"><a href="#混合scaling机制" class="headerlink" title="混合scaling机制"></a>混合scaling机制</h2><h3 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h3><p>数据并行中scaling out有两种常见策略：strong scaling out（total batch size固定）和weak scaling out（local batch size固定）。前者不能很好地保证训练效率，后者不能保证训练的准确率。本文提出一种将两种方式混合的机制，既能保证效率，也能保证准确率。</p><p>对于strong scaling out来说，随着worker数量增大，训练的throughput先上升后下降。对于一个scaling out的请求，Elan首先检查，在调整后的worker数量是否超过throughput最大时的worker数量。如果不超过，则无需调整batch size；否则，将global batch size扩大为两倍，然后再将目标worker数量和新的global batch size下的optimal worker数量进行比较，反复进行这个操作。global batch size扩大的倍数不能超过实际worker数量扩大的倍数。具体算法如下：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/2.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Parallelism Scaling算法</div></center><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>Elan采用progressive linear scaling 规则调整learning rate。根据SGD，learning rate应该随着batch size的增加成比例变化。但是learning rate突然增大会导致模型不converge。因此，Elan在T个iteration的时间里，逐步调整learning rate。</p><h2 id="无IO的并发状态复制"><a href="#无IO的并发状态复制" class="headerlink" title="无IO的并发状态复制"></a>无IO的并发状态复制</h2><h3 id="训练状态的特点"><a href="#训练状态的特点" class="headerlink" title="训练状态的特点"></a>训练状态的特点</h3><p>数据(加载)、模型、通信组和优化器以及一些运行时的信息（例如当前的epoch和iteration数）共同组成数据并行任务里的训练状态。</p><p>训练状态有两个特点：（1）训练状态分布在异构的设备中；（2）GPU中的状态比CPU中的状态大得多。</p><p>由于这两个特点，状态复制的机制必须满足：（1）很好地管理状态的异构性；（2）高效复制GPU上的状态。</p><h3 id="带宽的不同"><a href="#带宽的不同" class="headerlink" title="带宽的不同"></a>带宽的不同</h3><p>PCIe连接的GPU之间有三种通信方式：P2P、CPU共享内存 (SHM) 和网络（NET; 在本文中是56Gbps Infiniband）。</p><p>集群中GPU间的通信又有四种典型的通信方式： L1：PCIe switches；L2：PCIe host bridge；L3：socket-level link (e.g., QPI)；L4：network。对于通信来说，P2P is 只能用于L1，L2和L3可以用SHM，L4只能用NET。</p><h3 id="状态复制"><a href="#状态复制" class="headerlink" title="状态复制"></a>状态复制</h3><ol><li>对于所有的worker建立拓扑树，在复制状态的时候，选择和目标worker最近的worker作为source（P2P &gt; SHM &gt; NET）；</li><li>并发地进行多个replication以充分利用带宽。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/3.png" width="40%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：向A、B、C、D中添加E、F两个新worker</div></center><br><p>此外，CPU和GPU上的状态复制是同时进行的。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>资源调整的过程是shut-down free的，且将启动和初始化新机器的过程和训练overlap起来。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/4.png" width="40%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：使用Elan scale out的时间轴</div></center><br><p>在数据并行中，进行了资源的调整后，需要对数据集进行重新划分。传统的方法是把整个数据集划分为几个部分，剩余的、还未用于训练的数据是碎片化的。Elan采用了一种序列化的方式，这样，剩余的数据就是连续的了，数据集的状态仅仅用一个index来表示未使用的数据的位置就可以了。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/5.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：chunk-based和序列化的数据加载</div></center><br><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>本文从三个角度对Elan进行评估：资源调整的overhead、动态batch size中获益以及弹性训练调度的获益。</p><h3 id="资源调整"><a href="#资源调整" class="headerlink" title="资源调整"></a>资源调整</h3><p>使用了8台服务器，各8张卡。</p><p>两个baseline：shutdown-restart和Litz（CPU-GPU上下文切换）。</p><p>Overhead定义为：如果没有进行资源调整，维持elasticity所浪费的时间（对于Elan来说指的是第四步）。S&amp;R和Elan在这方面overhead是相同的。使用Elan进行资源调整的overhead小于训练总时长的3‰。对于Litz，由于系统的设计不同（？），很难直接比较overhead的大小。作者比较了Litz和Elan的throughput，发现Litz的throughput比Elan小，进而说明Litz的方法中，overhead对训练的影响更大。</p><p>另外， 作者比较了S&amp;R和Elan migration,、scaling in和scaling out所用的时间，Elan在各种情况下都优于S&amp;R。</p><h3 id="弹性训练"><a href="#弹性训练" class="headerlink" title="弹性训练"></a>弹性训练</h3><p>用AdaBatch训练ResNet50。两个baseline：固定batch size和固定worker数量。Elan的训练效率最高且不用想模型converge。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/6.png" width="40%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：不同configuration下的训练效率</div></center><h3 id="弹性调度"><a href="#弹性调度" class="headerlink" title="弹性调度"></a>弹性调度</h3><p>Baseline：FIFO 和 Backfill (BF)。</p><p>收集了商汤的trace，并为每个job定义了min_res和max_res。</p><p>调度策略：如果有空闲的资源，则为边际增益最大的job添加GPU（类似<a href="">AFS</a>）。</p><p>实验表明，弹性调度可以提高资源利用率，且是必要的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Elan/7.png" width="70%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图6：不同弹性训练系统下的平均表现</div></center><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文的工作非常全面，实验从多个角度证明了Elastic Training的优点。虽然说调度器理论上不应该任意调整用户定义好的batch size，但是除此以外挑不太出来什么明显的问题，不愧是best paper candidate。</p><hr><p><strong>原文作者：</strong>Lei Xie, Jidong Zhai, Baodong Wu, Yuanbo Wang, Xingcheng Zhang, Peng Sun, Shengen Yan<br><strong>原文链接：</strong><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9355755&amp;tag=1">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9355755&amp;tag=1</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> ICDCS </tag>
            
            <tag> 弹性训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在异构GPU集群中权衡效率与公平性</title>
      <link href="2021/03/07/Gandiva-fair/"/>
      <url>2021/03/07/Gandiva-fair/</url>
      
        <content type="html"><![CDATA[<h1 id="Balancing-Efficiency-and-Fairness-in-Heterogeneous-GPU-Clusters-for-Deep-Learning"><a href="#Balancing-Efficiency-and-Fairness-in-Heterogeneous-GPU-Clusters-for-Deep-Learning" class="headerlink" title="Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning"></a>Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning</h1><p>本文是Gandiva的后续工作，来自微软印度研究院，发表于EuroSys’20。类似于HiveD，本文认为，在共享集群中，DLT job的performance不能比在使用同样资源的私有集群上差；如果用户有不能充分利用的资源，没有被使用的那一部分资源要被分享给其他用户已提升集群的效率。集群中有不同性能的资源，为不同的job分配不同性能的设备是不公平的。然而已有的一部分工作即使在同构集群中也不能保证公平性，这些工作往往注重与减少JCT或者提升集群整体效率。本文提出了一个通过把为充分利用的计算资源分配给其他活跃用户来保证集群中GPU时间公平性的调度器Gandiva_fair。Gandiva_fair用“ticket”表示公平性。Gandiva_fair通过三点来同时保证公平性和效率：（1）通过中心调度器调度大型任务，每个server上的<strong>split scheduler</strong>负责调度该服务器上的小任务；（2）Gandiva_fair中的<strong>load balancer</strong>通过job迁移来把任务均匀地分布在集群中；（3）通过<strong>自动资源交易</strong>的方法解决异构性带来的效率问题。</p><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><h3 id="Split-Stride-Gang-scheduling"><a href="#Split-Stride-Gang-scheduling" class="headerlink" title="Split Stride Gang-scheduling"></a>Split Stride Gang-scheduling</h3><h4 id="小任务（单机任务）"><a href="#小任务（单机任务）" class="headerlink" title="小任务（单机任务）"></a>小任务（单机任务）</h4><p>给个job都有一个pass值，每运行一个时间单位，就运行当前pass值最小的job，pass值累加这个job的ticket数的倒数。如果某次调度的时候，剩余的GPU数量少于需要被调度的job所需GPU数量，那就先去调度下一个job，等到下一轮的时候，这个job就成为了pass最小的，它一定会被最先分配到GPU。</p><h4 id="大任务（多机任务）"><a href="#大任务（多机任务）" class="headerlink" title="大任务（多机任务）"></a>大任务（多机任务）</h4><p>对于多机任务来说，需要一次性分配好所有的GPU；对于小任务来说，每个server上单独调度小任务效率更高。因此，Gandiva_fair采用Split Stride scheduler。其中，每个server上的per-server scheduler用于调度单机任务，central scheduler用于调度多机任务和每个server上所有小任务组成的“集成任务”。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Gandiva_fair/1.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Split Stride Scheduler</div></center><h3 id="Load-Balance"><a href="#Load-Balance" class="headerlink" title="Load Balance"></a>Load Balance</h3><p>fair的定义：inter-user fairness，根据每个user的ticket的数量占总的ticket数量的比例，来分配GPU资源。local per-server scheduler可以根据ticket的比例调度每个小job的资源。在此基础上，如果能保证center scheduler对每个local per-server scheduler的资源调度是公平的，就能保证整体的公平性了。这只需每个server上的ticket数量尽量平均（每个node上的ticketLoadPerGPU尽量相等）（load balance）。</p><p>在有资源空闲，但与此同时也有等待中的任务的时候，Gandiva_fair会（1）将部分任务迁移到其他的server上；（2）考虑把任务“pack”起来（参考Gandiva）（pack不影响前面对ticketLoadPerGPU的计算）。</p><h3 id="处理异构性问题"><a href="#处理异构性问题" class="headerlink" title="处理异构性问题"></a>处理异构性问题</h3><p>仅针对调参的时候，一个用户提交大量相似的任务的情景。通过同一用户的不同job在不同类型的GPU上运行，记录效率，从而决定怎样为这个任务分配GPU是最优的。对于不同的用户，最开始给他们分配的GPU型号是相对公平的。但是因为他们的任务不同，Gandiva_fair可能会根据job的特点，在他们之间进行GPU的交易，从而使得每个用户的运行效率都有所提升。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Gandiva_fair/2.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">资源配置算法</div></center><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>Job migration：修改了PyTorch和TensorFlow框架的源码。借助了已有的process migration工具CRIU，使用了GPU和CPU之间的上下文切换和checkpoint，切换的overhead在百毫秒数量级。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>50台服务器，200块GPU。PyTorch、TensorFlow和Horovod三种框架。</p><p>首先，只用1台/两台机器和少量的任务，来验证公平性。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Gandiva_fair/3.png" width="90%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：单机/双机上用户的GPU吞吐量</div></center><br><p>在同构、异构集群无trading和异构集群有trading的情况下分别验证了公平性和效率的问题。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要解决了两件事：（1）通过调度算法和负载均衡，保证不同用户之间的公平性；（2）充分利用异构集群的特点来提高集群的效率。</p><hr><p><strong>原文作者：</strong>Shubham Chaudhary, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Srinidhi Viswanatha<br><strong>原文链接：</strong><a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/05/gandiva-fair-eurosys20.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2020/05/gandiva-fair-eurosys20.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 集群调度 </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> EuroSys </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Resources for Machine Learning System</title>
      <link href="2021/03/02/MLsystem/"/>
      <url>2021/03/02/MLsystem/</url>
      
        <content type="html"><![CDATA[<h1 id="Meta-Papers"><a href="#Meta-Papers" class="headerlink" title="Meta Papers"></a>Meta Papers</h1><ul><li><input disabled="" type="checkbox"> <strong>How to Read a Paper</strong> [<a href="https://dl.acm.org/doi/pdf/10.1145/1273445.1273458">PDF</a>]</li><li><input checked="" disabled="" type="checkbox"> <strong>Low-level Advice for Systems Research</strong> [<a href="https://lalith.in/2020/09/27/Low-Level-Advice-For-Systems-Research/">Original Link</a>]</li><li><input checked="" disabled="" type="checkbox"> <strong>A Berkeley View of Systems Challenges for AI</strong> [<a href="https://arxiv.org/pdf/1712.05855.pdf">PDF</a>]</li><li><input checked="" disabled="" type="checkbox"> <strong>MLSys: The New Frontier of Machine Learning Systems</strong> [<a href="https://arxiv.org/pdf/1904.03257.pdf">PDF</a>]</li><li><a href="https://diandiangu.github.io/2020/12/25/survey/"><strong>综述类文章汇总</strong></a></li></ul><h1 id="课程资源"><a href="#课程资源" class="headerlink" title="课程资源"></a>课程资源</h1><ul><li><strong>MSRA</strong> <a href="https://github.com/microsoft/ai-edu">微软人工智能教育与学习共建社区</a> <a href="https://github.com/microsoft/AI-System">人工智能系统</a>（中文）</li><li><strong>陈天奇</strong> <a href="http://dlsys-course.github.io/">CSE 599W: System for ML</a>（经典）</li><li><strong>UC Berkley</strong> <a href="https://ucbrise.github.io/cs294-ai-sys-sp19/">CS294: AI For Systems and Systems For AI</a> [<a href="https://github.com/ucbrise/cs294-ai-sys-sp19">GitHub</a>]</li><li><strong>Michigan</strong> <a href="https://github.com/mosharaf/eecs598">EECS 598: System for AL (W’21)</a></li></ul><h1 id="概念补课"><a href="#概念补课" class="headerlink" title="概念补课"></a>概念补课</h1><ul><li><input disabled="" type="checkbox"> 什么是“云”？什么是“云原生”？</li></ul><ul><li><input disabled="" type="checkbox"> 什么是“软件定义”？</li></ul><ul><li><input disabled="" type="checkbox"> 什么是“disaggregation”？</li></ul><h1 id="Book-List"><a href="#Book-List" class="headerlink" title="Book List"></a>Book List</h1><ul><li><strong>System for AI</strong> <a href="https://microsoft.github.io/AI-System/">微软人工智能系统课程</a> （中文）</li><li><strong>机器学习系统</strong> <a href="https://openmlsys.github.io/index.html">机器学习系统：设计与实现</a>（中文）</li><li><strong>AutoML</strong> <a href="https://www.automl.org/book/">AutoML: Methods, Systems, Challenges (first book on AutoML)</a></li></ul><h1 id="Paper-Reading-Lists"><a href="#Paper-Reading-Lists" class="headerlink" title="Paper Reading Lists"></a>Paper Reading Lists</h1><ul><li><a href="https://diandiangu.github.io/frameworks/">分布式训练框架</a></li><li><a href="https://diandiangu.github.io/training/">训练优化</a></li><li><a href="https://diandiangu.github.io/scheduling/">集群调度</a></li><li><a href="https://diandiangu.github.io/sharing/">GPU资源利用</a></li><li><a href="https://diandiangu.github.io/elasticity/">弹性训练</a></li><li><a href="https://diandiangu.github.io/others/">其他</a></li></ul><h1 id="心态调整与研究方法"><a href="#心态调整与研究方法" class="headerlink" title="心态调整与研究方法"></a>心态调整与研究方法</h1><ul><li><a href="https://meditator-hkx.github.io/Research/pguo-PhD-grind.pdf">The PhD Grind</a></li><li><a href="/attaches/system_research.pdf">一名系统研究者的攀登之路</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 集群调度 </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> 深度学习框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Surveys on ML/DL</title>
      <link href="2020/12/25/survey/"/>
      <url>2020/12/25/survey/</url>
      
        <content type="html"><![CDATA[<h2 id="分布式机器学习"><a href="#分布式机器学习" class="headerlink" title="分布式机器学习"></a>分布式机器学习</h2><ul><li><p><strong>A Quick Survey on Large Scale Distributed Deep Learning Systems</strong> (ICPADS’18) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8644613">PDF</a>]</p><ul><li>从算法角度、分布式系统角度和应用角度分析分布式深度学习系统</li></ul></li><li><p><strong>Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques, and Tools</strong> (ACM Comput. Surv. 2019 53(1)) [<a href="https://diandiangu.github.io/2020/07/20/DistributedInfrastructures/">阅读笔记</a>][<a href="https://www.researchgate.net/profile/Ruben_Mayer/publication/332033965_Scalable_Deep_Learning_on_Distributed_Infrastructures_Challenges_Techniques_and_Tools/links/5d90f42492851c33e9488837/Scalable-Deep-Learning-on-Distributed-Infrastructures-Challenges-Techniques-and-Tools.pdf">PDF</a>]</p><ul><li>汇集、分类和比较了来自不同社区的分布式基础设施方面的大量工作</li><li>对现有的开源DL框架和工具进行了概述和比较</li><li>强调并讨论了该领域的开放性研究挑战。</li></ul></li><li><p><strong>Demystifying Parallel and Distributed Deep Learning: An In-depth Concurrency Analysis</strong> (ACM Comput. Surv. 2019 53(4)) [<a href="https://dl.acm.org/doi/pdf/10.1145/3320060">PDF</a>]</p><ul><li>深度学习中的并行和分布式</li><li>用于DNN评估的并行策略及其实现</li><li>针对支持分布式环境的训练算法和系统的扩展</li><li>对这些训练算法和系统的扩展的并发性和平均并行性的分析</li></ul></li><li><p><strong>A Survey on Distributed Machine Learning</strong> (ACM Comput. Surv. 53(2)) [<a href="https://arxiv.org/pdf/1912.09789.pdf">PDF</a>]</p><ul><li>机器学习的系统挑战，以及如何采用来自高性能计算（HPC）的思想来加速和提高可伸缩性</li><li>分布式机器学习的体系结构</li><li>最广泛使用的系统和库的生态系统及其底层设计</li><li>分布式机器学习的主要挑战</li></ul></li><li><p><strong>Communication-Efficient Distributed Deep Learning: A Comprehensive Survey</strong> (ArXiv’20) [<a href="https://arxiv.org/pdf/2003.06307.pdf">PDF</a>]</p><ul><li>在系统级探究系统为了降低通信成本进行的的设计和实现</li><li>在算法层面比较不同算法的理论收敛边界和通信复杂度</li></ul></li></ul><h2 id="深度学习调度器"><a href="#深度学习调度器" class="headerlink" title="深度学习调度器"></a>深度学习调度器</h2><ul><li><strong>Deep Learning Workload Scheduling in GPU Datacenters: Taxonomy, Challenges and Vision</strong> (ArXiv’22) [<a href="https://arxiv.org/abs/2205.11913">PDF</a>][<a href="https://github.com/S-Lab-System-Group/Awesome-DL-Scheduling-Papers">Awesome list</a>]</li></ul><h2 id="机器学习框架"><a href="#机器学习框架" class="headerlink" title="机器学习框架"></a>机器学习框架</h2><ul><li><p><strong>A Survey on Deep Learning for Big Data</strong> (J. Big Data 2018 6: 60) [<a href="https://pdf.sciencedirectassets.com/272144/1-s2.0-S1566253517X00079/1-s2.0-S1566253517305328/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjED0aCXVzLWVhc3QtMSJHMEUCICQli7LuHSc8WT3T32ooDPRkaYxuR3PfQ5sVNrQPERZ9AiEA0K1fr/yVaOw3zxrgPxcziYAk9q7KiUstik/lbZmQ3wYqvQMI5f//////////ARADGgwwNTkwMDM1NDY4NjUiDIPQspmsaW4TfpP9qiqRA/XeAonahY3FUp7arBQKkZuo+9+PibBudwcOo/wDhv8Fwi0F+c1pLYADKs8CfMLMZi5FOHayr0YCNszIAkjQemqjV+2VlddNu76Sc+oGYzBs5+p2/WD+PHlo8CZbC6PcPRMx57v/d1XzwgUzjg16Gs9PL3X4jztfx62yNe8W7qDtEUTCf3TEtvNg4qZTl0McfQlyqiOKUmDa88Pc3TRxWGo4igVH4fuQNg42hMjArtF+07EzqyEso73onIOayOh+g/Y4CFeJVvf/g+vANM00lEe75ufdEet4YAZhLo9M3Q+WsubvnpI2KFBgzx1MLMv7OS5SLnH6n9zpYFOjJXt2JLUcFEVdre3FQaheLs4sPYZzjH1Z1QPfd6zprvZP/J+kP4i5gcj1+LJ4aMLK6uQh8mrr8F+lXBusdKpVOvURElESeVrj2WNYG8BCB5oQ3quCi1G992NeP6BrwX2eg/zs/MX6olmBhrLlnDPf9lENjDpC7lF3rS2KGr+gLrZTK3wExBk5hbaNvwvIUJ5HzxduQiRfMJTQlf8FOusBivIkhmQf3IOPFPnPehjVmof1A9fD/l33jV4W1iIg2COsjdWd53mD6lPm/3x0/gOEhRW8uXHRTjv0OctmZsaqsuM107mKC1auJy7ldWSlITNn6Gzdyq1VzkOomDY5CRF4hYXoR75E67pbnuZ8ypzqQePDShSfUGl7sN+YFzcMMJGP0LlCt57xQVad5/tdyrDRBAW9jmS0FyIUy8rpgHA5gPIF+a2RxsJwH5hhF1SgDkpGF/k8dJw5OyNdEk3vyqMnw6qOmmeUR3SfWgbm+Uiu/GA+jI4C295dtWms84FGldfbR5xFLqvzEp0Tzg==&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20201225T045839Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3W64ASUD/20201225/us-east-1/s3/aws4_request&X-Amz-Signature=2fe7482479451e03541d9eeda76643075f925d11e8f278c9135b851f0f21f2be&hash=58e9266d45270e5b47ed648751ad8d84117e83a0e1656af04a4bed641fb35060&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1566253517305328&tid=spdf-ab42b773-613a-437d-bad7-68376d675327&sid=29cd82ba5cbcc749ce184bc048d206988b38gxrqa&type=client">PDF</a>]</p><ul><li>回顾大数据特征学习的深度学习模型的研究进展</li><li>指出大数据深度学习所面临的挑战，并讨论未来的课题</li></ul></li><li><p><strong>Machine Learning and Deep Learning Frameworks and Libraries for Large-Scale Data Mining: a Survey</strong> (Artif. Intell. Rev. 2019 52(1)) [<a href="https://link.springer.com/article/10.1007/s10462-018-09679-z">PDF</a>]</p><ul><li>概述了ML和DL技术以及它们的发展和新趋势</li><li>动态链接语言与加速计算的关系</li><li>最新的ML和DL框架和库：机器学习框架和没有特殊硬件支持的库、具有GPU支持的深度学习框架和库以及支持MapReduce的机器学习和深度学习框架和库</li><li>总结趋势和发展方向</li></ul></li><li><p><strong>Various Frameworks and Libraries of Machine Learning and Deep Learning: A Survey</strong> (Archives of Computational Methods in Engineering 2019) [<a href="https://link.springer.com/article/10.1007/s11831-018-09312-w">PDF</a>]</p><ul><li>比较了18个常用的深度学习框架和库并介绍了大量的benchmarking数据</li><li>从模型设计能力、接口属性、部署能力、性能、框架设计和发展前景六个方面对8个主流深度学习框架进行了打分</li></ul></li><li><p><strong>A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning</strong> (IPDPS’19) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8821020">PDF</a>]</p><ul><li>白盒测试，能够公平地分析和比较不同的DL工作负载和算法</li></ul></li></ul><h2 id="机器学习测试"><a href="#机器学习测试" class="headerlink" title="机器学习测试"></a>机器学习测试</h2><ul><li><strong>Machine Learning Testing: Survey, Landscapes and Horizons</strong> (IEEE Transactions on Software Engineering 2020) [<a href="https://arxiv.org/pdf/1906.10742.pdf">PDF</a>]<ul><li>介绍机器学习测试并介绍已有的对各种方面进行测试的方法</li><li>指出了几去学习测试面临的挑战、存在的问题和有前途的研究方向</li></ul></li></ul><h2 id="Efficient-DNN"><a href="#Efficient-DNN" class="headerlink" title="Efficient DNN"></a>Efficient DNN</h2><ul><li><p><strong>Efficient Processing of Deep Neural Networks: A Tutorial and Survey</strong></p></li><li><p><strong>Energy-Aware Scheduling for Real-Time Systems: A Survey</strong></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 调研 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Empirical Studies on DL and Other Areas</title>
      <link href="2020/12/25/empirical/"/>
      <url>2020/12/25/empirical/</url>
      
        <content type="html"><![CDATA[<h2 id="深度学习领域的Empirical-Study"><a href="#深度学习领域的Empirical-Study" class="headerlink" title="深度学习领域的Empirical Study"></a>深度学习领域的Empirical Study</h2><ul><li><p><strong>An Empirical Study on TensorFlow Program Bugs</strong> (ISSTA’18) [<a href="http://sccpu2.cse.ust.hk/castle/materials/issta18main-p98-p.pdf#:~:text=An%20Empirical%20Study%20on%20TensorFlow%20Program%20Bugs%20ISSTA'18%2C,further%20divided%20into%20two%20sub-phases%3A%20training%20and%20testing.">PDF</a>]</p><p>  <strong>内容：</strong>探究使用TensorFlow框架的程序中bug产生的原因、检测bug的挑战、用户如何应对检测bug的挑战、定位bug的挑战、用户如何应对定位bug的挑战。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据。</p></li><li><p><strong>A Comprehensive Study on Deep Learning Bug Characteristics</strong> (ESEC/FSE’19)[<a href="https://dl.acm.org/doi/pdf/10.1145/3338906.3338955">PDF</a>]</p><p>  <strong>内容：</strong>探究使用5种流行的深度学习框架会遇到的bug类型、根本原因、产生的影响、Bug出现的阶段、共性以及bug的演变。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据。</p></li><li><p><strong>An Empirical Study on Program Failures of Deep Learning Jobs</strong> (ICSE’20 <strong>Distinguished Paper Award</strong>) [<a href="https://wencongxiao.github.io/res/icse20/icse20-main-199.pdf">PDF</a>]</p><p>  <strong>内容：</strong>深度学习任务failure的种类、根本原因和现有测试/诊断方法。</p><p>  <strong>方法：</strong>通过研究微软集群中实际遇到的fault，并对6位微软的开发者进行访问。</p></li><li><p><strong>Taxonomy of Real Faults in Deep Learning Systems</strong> (ICSE’20) [<a href="https://arxiv.org/pdf/1910.11015.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究在使用深度学习系统中会遇到的错误。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据，并通过20位科研工作者/从业人员进行label。</p></li><li><p><strong>Repairing Deep Neural Networks: Fix Patterns and Challenges</strong> (ICSE’20) [<a href="https://dl.acm.org/doi/pdf/10.1145/3377811.3380378">PDF</a>]</p><p>  <strong>内容：</strong>探究在深度神经网络中中bug的fix pattern和挑战。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据。</p></li><li><p><strong>An Empirical Study on Bugs inside TensorFlow</strong> (DASFAA’20) [<a href="http://www.cs.sjtu.edu.cn/~zhonghao/paper/tfbug.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究TensorFlow框架内部bug的症状、根本原因和位置。</p><p>  <strong>方法：</strong>挖掘2017-2019年之间github上的bug。</p></li><li><p><strong>A Comprehensive Study on Challenges in Deploying Deep Learning Based Software</strong> (ESEC/FSE’20) [<a href="https://chenzhenpeng18.github.io/papers/FSE20_1.pdf">PDF</a>]</p><p>  内容：探究部署深度学习应用的挑战。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li><li><p><strong>An Empirical Study on Deployment Faults of Deep Learning Based Mobile Applications</strong> (ICSE’21) [<a href="https://chenzhenpeng18.github.io/papers/ICSE21.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究在移动端部署深度学习应用时的错误的症状和解决办法。</p><p>  <strong>方法：</strong>挖掘Stack Overflow和github中的数据。</p></li></ul><h2 id="其他领域的相关工作"><a href="#其他领域的相关工作" class="headerlink" title="其他领域的相关工作"></a>其他领域的相关工作</h2><ul><li><p><strong>A Characteristic Study on Failures of Production Distributed Data-Parallel Programs</strong> (ICSE’13) [<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/icse13seip.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究分布式数据并行程序中数据和代码的缺陷的symptom、root cause、fix以及debugging practice。</p><p>  <strong>方法：</strong>挖掘Microsoft集群中的SCOPE job。</p></li><li><p><strong>Software Documentation Issues Unveiled</strong> (ICSE’19) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8811931">PDF</a>]</p><p>  <strong>内容：</strong>对文档中的问题进行分类。</p><p>  <strong>方法：</strong>挖掘emails, github issue, GitHub pull-requests和 Stack Overflow中的数据。</p></li><li><p><strong>Going Big: A Large-Scale Study on What Big Data Developers Ask</strong> (ESEC/FSE’19) [<a href="https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1615&context=hc_pubs">PDF</a>]</p><p>  <strong>内容：</strong>探究大数据开发者所关心的主题、主题分类、主题流行度、主题难度、主题流行度和难度之间的关联度。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li></ul><ul><li><p><strong>Understanding Privacy-Related Questions on Stack Overflow</strong> (CHI’20) [<a href="https://groups.inf.ed.ac.uk/tulips/papers/tahaei2020SO.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究SO用户会将什么话题与“隐私”问题关联，以及用户为什么要参与到隐私相关话题中。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li><li><p><strong>Understanding Build Issue Resolution in Practice: Symptoms and Fix Patterns</strong> (ESEC/FSE’20) [<a href="https://chenzhenpeng18.github.io/papers/FSE20_2.pdf">PDF</a>]</p><p>  <strong>内容：</strong>探究构建（build）中用户所关心的话题、遇到问题的症状及根本原因。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li><li><p><strong>Interpreting Cloud Computer Vision Pain-Points: A Mining Study of Stack Overflow</strong> (ICSE’20) [<a href="https://arxiv.org/pdf/2001.10130.pdf">PDF</a>]</p><p>  <strong>内容：</strong>分析探究Cloud CV领域研究者遇到的痛点。</p><p>  <strong>方法：</strong>挖掘Stack Overflow的数据。</p></li><li><p><strong>An Empirical Study on API Usage</strong> (TSE’19) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8186224">PDF</a>]</p></li><li><p><strong>An Empirical Study on API-Misuse Bugs in Open-Source C Programs</strong> (COMPSAC’19) [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8754426">PDF</a>]</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 软件工程 </tag>
            
            <tag> 实证研究 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HiveD：新的多租户GPU集群管理方案</title>
      <link href="2020/12/22/HiveD/"/>
      <url>2020/12/22/HiveD/</url>
      
        <content type="html"><![CDATA[<h1 id="HiveD-Sharing-a-GPU-Cluster-for-Deep-Learning-with-Guarantees"><a href="#HiveD-Sharing-a-GPU-Cluster-for-Deep-Learning-with-Guarantees" class="headerlink" title="HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees"></a>HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees</h1><p>随着深度学习训练需求的不断扩张，很多组织和机构都会选择自建多租户集群来共享昂贵的 GPU 资源。然而现有的GPU集群管理方案因为使用了GPU配额（Quota）机制，可能导致严重的共享异常（Sharing Anomaly）现象：某些租户的深度学习任务甚至比在私有集群中性能更差。为了从根本上解决该问题，微软亚洲研究院和微软（亚洲）互联网工程院、北京大学、香港大学合作提出了一个新的多租户GPU集群管理方案HiveD，通过新的资源抽象和调度框架从而100% 保证共享安全（Sharing Safety），同时不失一般性地和任何任务调度策略兼容。</p><h2 id="背景和动机"><a href="#背景和动机" class="headerlink" title="背景和动机"></a>背景和动机</h2><p><strong>多租户集群（multi-tenant cluster）：</strong>HiveD的目标场景是一个集群里有多个tenants（比如公司里不同的团队/部门），每个tenant会贡献一定的资源，这部分贡献的资源就是所谓的私有集群（private cluster）。要保证这些tenants有意愿去共享资源的前提就是，它在共享集群中的性能应该不差于单独拥有一个私有集群时的性能，否则tenant会prefer单独占有这些资源。这个概念在调度领域也叫sharing incentive。</p><p><strong>传统的对多租户GPU集群的管理方法：</strong>用户申请一定数量的GPU（quota）。为了提高训练速度，用户通常对一个深度学习任务有<strong>affinity requirement</strong>，即对资源分布的要求（例如，一个需要64张卡的任务需要跑在8台机器各8张卡上）。在多租户集群中没有满足要求的资源的时候，资源管理器可以选择排队等待或用更宽松的affinity requirement。</p><p><strong>共享异常：</strong>和内存管理中的外部碎片类似。有时候多租户集群中有足够的资源数量，但是这些资源很“碎片化”，无法满足用户的“affinity”的需求，这样的现象被称为“共享异常”。</p><p><strong>HiveD</strong>：通过一种调度的方法减少全局资源碎片，从而缓解共享异常的现象。但是，减少碎片可能会增大不同任务之间的影响，从而使得任务的训练速度变慢。HiveD不用quota来描述tenant的请求，而是用cell来描述VC（Virtual Cluster）的结构，并给每一个用户分配VC，这样的描述方法可以很好的描述出VC的结构。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>HiveD采用了两层的架构：第一层是Virtual Private Cluster虚拟私有集群，这一层为用户提供了一个私有集群的“假象”；第二层是实际的集群，从虚拟私有集群（VC）到实际的集群中的设备之间存在着映射关系，如下图所示。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/1.png" width="65%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：系统架构：两层设计</div></center><br><p>HiveD的这两个layer上各有两点设计/贡献：</p><ul><li><p>VC</p><ul><li>（和quota相比）提出了一种新的对资源抽象的方式——cell，使用这种资源抽象可以同时描述GPU资源的quota和affinity。</li><li>在VC层面，可以使用现有的深度学习调度器对资源进行调度。</li></ul></li><li><p>From Virtual to Physical</p><ul><li>提出了可以保证共享安全的dynamic cell allocation算法。</li><li>支持low-priority jobs 。</li></ul></li></ul><p>其中，用cell对资源进行抽象以及cell allocation算法可以保证HiveD中的共享安全。在VC层面对调度器的使用可以保证调度效率。对low-priority jobs的支持保证了整个集群中的资源利用率。</p><h3 id="带有cell的虚拟私有集群"><a href="#带有cell的虚拟私有集群" class="headerlink" title="带有cell的虚拟私有集群"></a>带有cell的虚拟私有集群</h3><p>为了对私有GPU集群进行描述，HiveD定义了一个多层cell结构的层次结构。某一层次上的cell，是具有相应拓扑连接的GPU的对应集合。在每个虚拟私有集群（VC）使用每个级别上的cell数来模拟/描述相应的私有集群。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/2.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：cell的5个层级</div></center><br><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/3.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：多级cell分配的例子</div></center><br><p>在图3这个例子中，一共有四个层级的cell结构，图中用不同颜色框出了租户A、B和C的一种可能的GPU分配情况，但这不是唯一可行的方案。</p><p>在cell层次结构中，k级的cell由一组（k−1）级的cell组成。这些（k−1）级的cell称为伙伴单元（buddy cells）；伙伴单元可以合并为下一个更高级别的cell。本文假设cell具有层次一致的可组合性：（i）所有k级cell在满足租户对k级cell的请求方面是等价的，并且（ii）所有k级dell都可以拆分成相同数量的（k−1）级的cell。</p><h3 id="Buddy-Cell-Allocation算法"><a href="#Buddy-Cell-Allocation算法" class="headerlink" title="Buddy Cell Allocation算法"></a>Buddy Cell Allocation算法</h3><p>Buddy Cell Allocation算法其实非常的简单直观：如果需要在VC中分配一个k级cell，算法从k级开始，首先检查是否有可用的k级cell，如果有可用的k级cell则分配一个。否则，算法将逐级上移，去检查(k+1)级或者更高级别的cell，直到有一个可用的l级cell，其中l&gt;k。然后，算法将递归地将一个空闲的l级cell拆分为多个较低级别的cell，直到k级cell可用为止。每次拆分都会在下一个较低级别生成一组buddy cells，这些cell将添加到该较低级别的空闲列表中。</p><p>如果有cell被释放的话，该算法会按照与分配cell相反的方式把空闲的cell尽量merge起来，这样保证空闲的cell中有尽量多的更高level的cell。作者通过数学归纳法证明了这一算法可以保证：如果租户原有的VC可以满足一个cell分配，使用该算法在多租户集群中这样的cell分配也一定可以被满足。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/4.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">算法1：Buddy Cell Allocation算法</div></center><br><p>该算法可以扩展到支持低优先级的作业（即opportunistic jobs），被低优先级的作业占有的cell可以被高优先级作业抢占。支持这种低优先级的job有助于提高GPU的整体利用率，同时又不损害为VC提供的共享安全保证。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>HiveD已经被继承到了基于Kubernetes的开源深度学习训练平台——OpenPAI。HiveD已经在微软已经使用了1年多的时间，用于管理超过1000块异构GPU，为科研和产业workload提供服务。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>作者首先在96块GPU的集群上进行试验：本文在分别使用了三种state-of-the-art的调度器应用于VC上，进行资源调度。通过比较可以看出，与使用Quota相比，HiveD可以有效消除共享异常；和原始的私有集群相比，任务的queuing delay有明显减小；与此同时，和直接在整个集群上使用调度器相比，任务完成时间（JCT）是差不多一样的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/5.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：租户prod-a的平均排队延迟</div></center><br><p>作者还从一个具有2232块GPU、11个租户的集群中手机了2个月的trace，基于trace用96块GPU对这样一个大规模集群上的运行情况进行模拟。在这种情况下，使用HiveD的queuing delay依旧是最短的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/HiveD/6.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：租户prod-e的排队延迟</div></center><br><p>除此以外，作者还对GPU affinity需求和共享安全的权衡等问题进行了的讨论，也通过模拟实验评估了buddy cell allocation算法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>HiveD通过一系列方法应对共享多租户GPU集群的挑战：（i）保障了用户的共享安全，（ii）提出一种新的资源抽象单元cell，用来来模拟虚拟私有集群，（iii）提出了buddy cell allocation算法，该算法被证明能够保证共享安全，并可以扩展到支持低优先级作业，以及（iv）设计一个灵活的体系结构，以结合最新技术共享安全和调度效率的调度程序。</p><p>总的来说，HiveD做的事情就是给各个tenant一个正在使用自己私有集群的假象，同时赋予tenants使用自己VC以外的资源来从共享集群中获益的能力。这两方面目标由安全性的定义、cell和VC的抽象以及buddy cell allocation以一个统一的方式实现了。</p><p>本文方法仍存在一定的问题。即使使用HiveD，集群中仍然可能会存在碎片。对于HiveD是否能真的像作者提到的那样“完全消除共享异常”，有待讨论。<del>另外cell或许并不能完全准确地描述affinity的需求（不能全面描述GPU之间的拓扑关系）。</del></p><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><ul><li><p><strong>Q：</strong>Tenant是什么？<br>  <strong>A：</strong>目标的场景就是说一个集群里有多个tenants，比如不同的团队/部门，这在真实场景中都是非常常见的。每个tenant会贡献一定的资源，这部分贡献的资源就是我们所谓的私有集群（private<br>cluster）。要保证这些tenants有意愿去共享资源的前提就是，它在共享集群中的性能应该不差于单独拥有一个私有集群时的性能，否则tenant会prefer单独占有这些资源。这个概念在调度领域也叫sharing<br>incentive。</p></li><li><p><strong>Q：</strong>当一个VC中的全部job都结束之后，HiveD是否会继续维护这个VC？<br>  <strong>A：</strong>一般来说，VC和job是无关的。如上面所说，VC本质上就是建模一个tenant原本的私有资源。它跟里面跑什么job没有必然联系，job结束了VC也还在。当然我们支持对VC的reconfiguration，未来可能也会有动态和自动的reconfiguration。</p></li><li><p><strong>Q：</strong>HiveD加入了VC这个新抽象以后，VC上的scheduler能知道它能用的GPU之间的拓扑结构之类的信息？<br>  <strong>A：</strong>每个VC分配的是cell，cell的层次结构里就包含了GPU的topo，GPU本身是leaf cell。Cell是一个tree 的层次结构，所以就算不在一个server里也是有topo的，比如图2 level-5这个cluster里的一个rack cell，里面包含了2个node cell，然后每个node cell下有2个CPU cell，每个CPU cell有两个PCIe switch cell，每个PCIe switch cell下有2个GPU cell，GPU cell是leaf cell。如果config的最高level的cell就是node level，没有上面的rack和cluster了，那几个node cell 之间就没有topo（比如不知道是一个rack下），这个取决于cell的配置里有没有。Cell是tree，所以scheduler里存的也是 tree，除了数量以外层次结构也都在，所以scheduler能知道任意两个cell在 tree 里的位置。</p></li></ul><hr><p><strong>原文作者：</strong>Hanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang, Fan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau, Yuqi Wang, Yifan Xiong, Bin Wang<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/osdi20-zhao_hanyu.pdf">https://www.usenix.org/system/files/osdi20-zhao_hanyu.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/microsoft/hivedscheduler">https://github.com/microsoft/hivedscheduler</a><br><strong>参考文献：</strong>[1] OSDI2020——SJTU-IPADS的“云见闻”（二） <a href="https://mp.weixin.qq.com/s/GzQSfBYhcU8XccnWV7epKg">https://mp.weixin.qq.com/s/GzQSfBYhcU8XccnWV7epKg</a><br>[2] OSDI 2020 | 微软亚洲研究院论文一览 <a href="https://mp.weixin.qq.com/s/vHeXvAeEPPiTls349yjkAQ">https://mp.weixin.qq.com/s/vHeXvAeEPPiTls349yjkAQ</a></p><p>特别感谢原文作者赵汉宇师兄和熊一帆师兄对HiveD设计的讲解！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 集群调度 </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> OSDI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络是分布式训练的瓶颈吗？</title>
      <link href="2020/12/22/sigcomm20jin/"/>
      <url>2020/12/22/sigcomm20jin/</url>
      
        <content type="html"><![CDATA[<h1 id="Is-Network-the-Bottleneck-of-Distributed-Training"><a href="#Is-Network-the-Bottleneck-of-Distributed-Training" class="headerlink" title="Is Network the Bottleneck of Distributed Training?"></a>Is Network the Bottleneck of Distributed Training?</h1><p>近期有许多工作致力于提高分布式训练的通信效率，但是很少有工作致力于系统地研究网络是否是瓶颈、以及网络在多大程度上限制了分布式训练的效率。在这篇文章中，作者度量并分析了分布式训练的网络表现。作者预期，度量结果会证实通信是阻碍分布式训练达到linear scale-out效果的原因。但是，作者发现实际上网络带宽利用率很低，如果网络带宽可以被充分利用，分布式训练的scaling factor可以接近于1。</p><p>另外，目前很多人提出压缩率超过了100的梯度压缩方法，作者发现在充分利用起来带宽的情况下，在100Gbps带宽下没有梯度压缩的必要。另一方面，10Gbps的低带宽下仅需要2-5倍的梯度压缩率就可以达到接近linear layout的效果。和梯度压缩这种应用层级的方法相比，网络层级的优化不需要对应用作出改变也不会有模型表现的损失。因此，作者提倡对分布式的网络社区做出真正的改变，以充分利用网络能力，达到linear scale-out。</p><p>本文由约翰斯·霍普金斯大学金鑫老师的团队发表于SIGCOMM’20，目前金鑫老师已就职于北京大学软件工程研究所。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>DNN模型越来越大，越来越深。然而，与快速增长的DNN模型需求相比，单芯片提供的计算能力仍然有限。因此，训练大型DNN模型不可避免地通过扩展（scale out）得到越来越多的分布。横向扩展的最优目标都是线性扩展（linear scalability）。也就是说，假设单个设备的吞吐量是T，那么具有n个设备的系统的吞吐量应该是nT。让具有n个设备的系统实际实现的吞吐量为Tn。我们将<strong>scaling factor</strong>定义为：<br>$$<br>scaling  factor = \frac{T_{n}} {nT}<br>$$<br>线性扩展要求任何n的比例因子都为1。</p><p>人们普遍认为，网络带宽是阻碍分布式训练提前扩展的瓶颈。因为计算阶段是并行的，因为每个线程都独立地处理自己的batch，所以n个worker的吞吐量是一个worker的n倍，因此只有通信阶段才能减慢训练过程。针对这一点，最近几年，机器学习和系统界对提高分布式训练的通信效率进行了大量的研究。这些工作主要在应用层完成，且假设网络已经尽了最大的通信效率。然而，很少有工作系统地理解网络是否是瓶颈以及在多大程度上是瓶颈。</p><p>本文首先分析了分布式训练的原理和方法：对几种具有代表性的DNN模型在AWS上的训练吞吐量进行了measurement study。实验结果证实了通信是阻止分布式训练线性扩展的因素。然而，作者发现，网络带宽其实并不是瓶颈，因为带宽的利用率很低。</p><p>基于记录结果，作者做了一个假设分析，在该分析中，作者控制网络带宽并对假设带宽被充分利用。分析结果表明，在充分利用网络的情况下，分布式训练可以达到99%以上的scaling factor。作者通过梯度压缩进一步扩展了假设分析。</p><p>本文的两个主要贡献点：</p><ul><li>系统地测量和分析了分布式训练的性能瓶颈。与一般的观点相反，作者揭示了主要问题不在于网络速度，而在于通信阶段的软件实现问题。</li><li>进行假设分析，以评估高性能网络传输对分布式训练带来的好处。作者认为真正的挑战是网络社区为分布式训练开发高性能的网络传输，以充分利用网络容量并实现线性扩展。</li></ul><h2 id="记录训练的performance"><a href="#记录训练的performance" class="headerlink" title="记录训练的performance"></a>记录训练的performance</h2><h3 id="Set-Up"><a href="#Set-Up" class="headerlink" title="Set Up"></a>Set Up</h3><ul><li><p><strong>硬件：</strong>Amazon EC2 p3dn.24xlarge instances with 8 GPUs (NVIDIA Tesla V100)。</p></li><li><p><strong>软件：</strong> Horovod 0.18.2, PyTorch 1.3.0, Torchvision 0.4.1, NCCL 2.4.8, cuDNN 6.6.0.64, and Open MPI 4.0.2. Horovod, NCCL, and Open MPI 采用 Linux kernel TCP通信。</p></li><li><p><strong>WorkLoad：</strong>ResNet50、 ResNet101和 VGG16三种模型，ImageNet数据集。</p></li></ul><h3 id="当前的scaling-factor"><a href="#当前的scaling-factor" class="headerlink" title="当前的scaling factor"></a>当前的scaling factor</h3><p>作者采用单个GPU上的吞吐量作为基准吞吐量T。实验中，改变服务器的数量。对于每种情况，测量服务器可以实现的总吞吐量，并计算scaling factor。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/1.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：不同数量的服务器下的scaling factor</div></center><br><p>图1显示了不同服务器数量下每个模型的scaling factor。结果表明，ResNet50、ResNet101和VGG16的scaling factor分别为75.05%、68.92%和55.99%，4台服务器分别为74.24%、66.28%和63.01%，8台服务器分别为71.6%、66.99%和59.8%。ResNet50实现了比ResNet101和VGG16更好的scaling factor，因为ResNet50模型相对比较小，通信负担轻。然而，对于这三种模型，Horovod在AWS上无法达到超过76%的比例因子。</p><p>这些结果证实了目前像Horovod这样的现成的分布式训练框架不能实现linear scaling，且与linear scaling存在着很大的差距。</p><h3 id="计算是瓶颈吗？"><a href="#计算是瓶颈吗？" class="headerlink" title="计算是瓶颈吗？"></a>计算是瓶颈吗？</h3><p>在计算部分，每个worker将一个batch的label好的图像输入神经网络模型，并局部计算梯度。<strong>如果一个worker完成一个batch的计算时间随着worker数量的增加而增加，那么计算就是分布式训练的瓶颈。</strong></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/2.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：不同数量的服务器下的计算时间</div></center><br><p>图2显示了具有不同worker数量的三个模型的计算时间（正向和反向传播）。可以看到计算时间几乎相同。单个GPU和多个GPU之间的时间差主要来自两个因素。首先，分布式训练中反向传递的运行时间不仅包括backward op，而且还包括所有的reduce操作，因为它们在GPU上是异步的，并且是重叠的。然而，对于单个GPU的情况，不存在所有的reduce操作。其次，Horovod在分布式训练过程中为模型中的每一层加了一个hook，这在单个GPU训练中是不存在的。然而，即使我们认为这些计算时间的gap是不可避免的，scaling factor仍应在90%左右，而不是测量出来的56%-75%，因为在分布式训练中测量的计算时间最多增加15%。因此，作者认为计算时间的gap不是分布式训练不能linear scaling的一个因素。</p><h3 id="网络是瓶颈吗？"><a href="#网络是瓶颈吗？" class="headerlink" title="网络是瓶颈吗？"></a>网络是瓶颈吗？</h3><p>由于计算所花费的时间与服务器数量无关，因此唯一的可能性是通信组件是系统扩展时的瓶颈。为了验证这种情况，作者首先测量不同网络带宽下的scaling factor。如图3所示，ResNet50的scaling factor随着网络带宽的增加而增加。在两台服务器的情况下，当带宽从1 Gbps增加到10 Gbps时，扩展因子从13%增加到68%。这是可以理解的，因为带宽越高，worker之间交换相同数量的数据所需的时间就越短。基于all-reduce算法，由于要交换的数据更多，所以随着worker数量的增加，scaling factor会降低。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/3.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：ResNet50模型的scaling factor随网络带宽的变化</div></center><br><p>大家普遍认为网络太慢时发送梯度太慢从而限制训练效率。然而，与之相反的事，图3显示网络在25 Gbps后趋于稳定。<strong>这意味着系统不能从更快的网络中获益。</strong>为了验证这一点，作者通过记录实时网络吞吐量来测量服务器的网络利用率。图4表明，服务器确实在低带宽（例如1 Gbps）下充分利用网络，但在高带宽（例如100 Gbps）下，它们只使用很小一部分带宽。这意味着仅仅增加带宽以使网络更快，对于提高某一点后的sclaing factor是没有用的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/4.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：网络带宽利用率</div></center><br><p>在高带宽下利用率低的一种可能性是CPU是瓶颈，因为实验在TCP上运行Horovod，而且以100Gbps这样的高速运行TCP是CPU密集型的。然而，分布式训练的计算大多是由GPU完成的，而且大多数GPU都配备了足够多的CPU。通过实验，作者发现在分布式训练时，CPU的利用率很低，因此CPU不是充分利用100Gbps网络带宽的瓶颈。</p><p><strong>总之，实验结果证实了通信是瓶颈。但与普遍的看法不同，这并不是因为网络发送数据太慢。根本原因在于网络传输的实现不佳，导致不能充分利用可用带宽。</strong></p><h2 id="假设分析"><a href="#假设分析" class="headerlink" title="假设分析"></a>假设分析</h2><h3 id="如果带宽被充分利用，会发生什么？"><a href="#如果带宽被充分利用，会发生什么？" class="headerlink" title="如果带宽被充分利用，会发生什么？"></a>如果带宽被充分利用，会发生什么？</h3><p>作者首先分析，如果网络得到充分利用，可以达到什么样的scaling factor。为了进行假设分析，作者通过hook记录了模型不同层的梯度计算时间。</p><p>作者进行了两个模拟进程：对backward的模拟，和对all-reduce的模拟。两个进程通过一个message queue来通信。backward进程通过log记录的数据模拟backward计算。在某一层的backward计算完成后，它不会立刻向all-reduce进程发送请求，而是把几层的梯度放到buffer里，这里才用了Horovod fusion buffer的strategy。buffer满了或者time out之后才进行all reduce。数据传输的时间用(2S(N-1)/N)/bw来预估，S为all reduce size。向量相加的时间用(N-1)*AddEst(S/N)来预估，AddEst(x)通过在GPU上profile等方式预估。</p><p>overhead由 $t_overhead =t_sync −t_back$ 预估，scaling factor由 $f_sim =tbatch/(t_batch +t_overhead)$ 预估。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/5.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：不同带宽下模拟得到的scaling factor v.s. 测量得到的scaling factor</div></center><br><p>图5中模拟得到的scaling factor和实际测量得出的scaling factor的对比说明，网络在低带宽下得到了充分利用；在高带宽下（即在25 Gbps之后），两条线路开始明显偏离。</p><p>作者还使用假设分析来评估不同worker数量下的scaling factor。假设网络被充分利用，即使是64个GPU，这三个模型都可以达到接近100%的scaling factor。总的来说，假设分析证实了分布式训练可以从高网络带宽中获益，而且如果充分利用网络，scaling factor可以提高到接近100%。</p><h3 id="应用层级优化有多有用？"><a href="#应用层级优化有多有用？" class="headerlink" title="应用层级优化有多有用？"></a>应用层级优化有多有用？</h3><p>在这个subsection的实验保持其他模拟步骤与之前提到的相同中相同，但将梯度传输的时间成本除以梯度压缩比。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/sigcomm20jin/6.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图6：不同压缩率下模拟得到的scaling factor</div></center><br><p>梯度压缩可能会降低向量加法的成本，从而进一步提高模拟得到的scaling factor。但是，如图所示，模拟的结果说明我们可能不需要像一些过去的工作中所说的那样需要高压缩比。对于VGG16这样的模型来说，10×的压缩比足够大，在10Gbps网络中可以获得接近100%的scaling factor。100 Gbps的结果也表明压缩在高速网络中没有多大用处。因此，梯度压缩技术在低速网络中是有用的，但是在现代网络环境中不需要有很大的压缩比。</p><h2 id="讨论与未来工作"><a href="#讨论与未来工作" class="headerlink" title="讨论与未来工作"></a>讨论与未来工作</h2><p><strong>研究结果背后的基本原理</strong> 如果网络得到充分利用，scaling factor可以接近100%，这个发现是合理的。首先，很大的网络带宽下，传输ResNet50、ResNet101和VGG16的所有参数仅需7.8 ms、13.6 ms和42.2 ms。第二，在计算和通信之间有很大的overlap。最后一层的所有reduce可以在backward计算完最后一层的梯度后立即开始，而不必等待整个backward过程完成。所以，结合高效的通信和计算与通信的overlap，scaling factor可以达到接近100%</p><p><strong>应用层优化的权衡</strong> 假设分析表明，应用层中的梯度压缩只在低网络带宽下笔比较有效。作者认为，它对云上的分布式训练或配备GPU或TPU的本地集群没有特别大的用处。</p><p>作者还认为，在如今的分布式训练中，网络带宽不是主要问题，主要问题在于网络传输是否有很好的实现。</p><hr><p><strong>原文作者：</strong>Zhen Zhang, Chaokun Chang, Haibin Lin, Yida Wang, Raman Arora, Xin Jin<br><strong>原文链接：</strong><a href="https://xinjin.github.io/files/NetAI20_Training.pdf">https://xinjin.github.io/files/NetAI20_Training.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> SIGCOMM </tag>
            
            <tag> 网络通信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PipeSwitch：面向深度学习应用的高效多进程管理</title>
      <link href="2020/12/07/PipeSwitch/"/>
      <url>2020/12/07/PipeSwitch/</url>
      
        <content type="html"><![CDATA[<h1 id="PipeSwitch-Fast-Pipelined-Context-Switching-for-Deep-Learning-Applications"><a href="#PipeSwitch-Fast-Pipelined-Context-Switching-for-Deep-Learning-Applications" class="headerlink" title="PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications"></a>PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications</h1><p>深度学习（DL）任务包括需要高通量的训练任务和需要低延迟的推理任务。目前的主流做法是提供专门的GPU集群，分别用于训练和推理。由于需要满足严格的服务级别目标（SLO），GPU群集通常会根据峰值负载过度配置，而不同任务类型之间以及应用程序之间对GPU的共享有一定的限制。本文提出了一个PipeSwitch系统，它可以使一个推理程序的未使用周期由训练或其他推理应用程序填充。它允许多个DL应用程序与整个GPU内存共享同一个GPU，并只增加毫秒级的切换开销。使用PipeSwitch，GPU利用率可以显著提高，且不会牺牲SLO。本文通过引入流水线式的上下文切换来实现这一点。其核心思想是利用神经网络模型的层级结构及其逐层计算的模式，在PCIe上进行流水线模型传输，并在GPU中以模型感知分组的方式执行任务。本文还设计了统一的内存管理和Active-Standby Worker切换机制，以配合上下文切换的流水线并确保进程间的隔离。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>深度学习任务包括需要高通量的训练任务和需要低延迟的推理任务。目前的主流做法是提供专门的GPU集群，分别用于训练和推理。推理集群常常参考峰值负载而分配过多的资源，以满足严格的服务级别目标（SLO）。即使对于推理任务本身，在实际应用中，通常按每个GPU的粒度将资源分配到每个程序，以减少应用程序之间的干扰。</p><p>理想情况下，多个深度学习应用程序应该能够打包到同一个GPU服务器上，以便通过时间共享实现GPU利用率的最大化。这正是操作系统如何通过任务调度和文本转换来提高CPU利用率的。但是和CPU不同，GPU在任务之间切换时有很高的开销。为了避免这种交换开销，现有的解决方案是在空间上共享GPU内存。</p><p>然而，现有的多任务系统支持例如Nvidia MPS远谈不上高效（切换时间甚至达几秒，而很多推理任务的SLO可能是毫秒量级）。但这样的系统对于调度是有重要作用的。考虑这样一个场景：将训练和推理任务混合部署在同一个集群里。训练任务一般是时间较长，SLO不敏感；推理任务则是对SLO相当敏感，且可能具备明显的潮汐特性（例如一些在线推理到了晚上workload会比白天轻一些）。这两类任务的差异还是很显著的，因此很多公司倾向于将它们各自部署在单独的集群中，但就会导致资源利用率问题，比如训练任务无法在晚上利用推理集群空闲出来的GPU资源。如果我们既想要将训练和推理混合部署，同时又希望它们之间的切换能够高效呢？这就是PipeSwitch希望解决的问题。</p><h2 id="系统概览"><a href="#系统概览" class="headerlink" title="系统概览"></a>系统概览</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/PipeSwitch/1.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：PipeSwitch的架构</div></center><br><p>图1展示了PipeSwitch服务器的架构，包括四种不同的构件：</p><ul><li>控制器。控制器是中心部件。它从客户端接收任务，并控制内存守护进程和工作进程来执行任务。</li><li>Memory Daemon。Memory Daemon管理GPU内存和DNN模型。它将GPU内存分配给活动工作线程，并将模型从主机内存传输到GPU内存。</li><li>Active Worker。Active Worker是当前在GPU中执行任务的worker。这里的worker指的是在一个GPU上执行任务的进程。</li><li>Standby Worker。服务器有一个或多个Standby Worker。Standby Worker处于空闲状态，正在初始化新任务，或者正在为上一个任务清理其环境</li></ul><p>如果要启动一个新任务，控制器要么等待当前任务完成（例如推理任务），要么通过通知Active Worker停止（例如训练任务）来抢占它。同时，控制器通知空闲的Standby Worker为新任务初始化其环境。在Active Worker完成或停止当前任务后，控制器通知Memory Daemon和Standby Worker将任务加载到GPU，以通过流水线式的模型传输执行。Memory Daemon将内存分配给Standby Worker，并将新任务使用的模型从主存传输到GPU内存。Standby Worker将成为新的Active Worker来执行新任务，而原来的ActiveWorker将成为Standby Worker并为上一个任务清理环境。本文的主要目标是根据深度学习应用程序的特点设计一种方法，以最小化该过程中的任务切换开销。</p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="流水线式的模型传输"><a href="#流水线式的模型传输" class="headerlink" title="流水线式的模型传输"></a>流水线式的模型传输</h3><p>PipeSwitch利用了深度学习任务一个非常重要的特——DNN模型通常都包含多个layer，且计算都是按layer的顺序进行的，最前面的layer会在前向传播时首先被用到。所以在做context switch的时候，可以优先将靠前的layer所需要的状态信息加载进GPU中，同时GPU就先计算这些已经ready的layer，整个过程被流水线化了（如下图b所示）。而现有做法（图a）则是将整个DNN作为一个大黑盒，将它无脑地完整加载到GPU之后，再按layer进行计算。相比之下，显然PipeSwitch这种思路高效得多。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/PipeSwitch/2.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：以一个推理任务为例说明流水线式模型传输的过程</div></center><br><p>但是这样做存在两个问题，一个是多次调用PCIe传输数据开销较大；另一个是每次数据传输后都要进行数据同步，开销较大。本文的做法是grouping：将几个layer的数据组合成一个group来进行传输。</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p>PipeSwitch利用深度学习应用程序的两个特性来最小化GPU内存管理开销。深度学习任务在GPU内存中存储两种重要的数据：DNN模型（包括模型参数）和中间结果。首先，分配给DNN模型的内存量是固定的，并且在任务执行期间不会改变。并且存储参数所需的内存量保持不变。第二，中间结果以简单、规则的模式进行变化，这不会导致内存碎片。对于推理任务，中间结果是每层的输出，下一层使用这些输出。在计算下一层之后，就不再需要它们了，可以安全地释放它们。训练任务的不同之处在于，在前向传球中生成的中间结果不能立即释放，因为它们也被backward用来更新权重。然而，backward以与forward相反的顺序消耗中间结果，即中间结果是先进后出。内存分配和释放可以通过一个简单的堆栈式机制来处理，而不会造成内存碎片。一般的任务管理方式不考虑这些特点，且对于快速的上下文切换来说不够轻量级。</p><p><strong>减少内存分配开销。</strong>PipeSwitch使用一个内存守护进程来管理GPU内存。为了与现有系统兼容并进行最小的更改，内存守护程序在系统启动时使用cudaMaloc来获取GPU内存，然后在运行时将内存动态分配给工作线程，而不是替换GPU内存管理器。这消除了每个工作人员使用cudaMaloc获取大量内存来存储其模型和中间结果的开销。内存守护进程只需要将内存指针传递给worker，这是轻量级的。守护进程确保每次只有一个worker拥有GPU内存，以保证工作进程之间的内存隔离。每个工作线程使用内存池分配内存以存储其模型和中间结果，并在不再需要中间结果后将内存回收到池中。</p><p><strong>最小化内存占用并避免额外的内存拷贝。</strong>PipeSwitch将模型存储在内存守护进程中，这样服务器只需要在主存中保留每个模型的一个副本。因为内存守护进程还管理GPU内存，所以它直接将模型从主存传输到GPU内存以进行任务启动，这就消除了从内存守护进程到worker的额外内存拷贝。</p><p><strong>最小化IPC开销。</strong>神经网络模型的内存分配过程是确定性的。具体地说，给定相同的GPU内存区域和相同的模型，只要内存守护进程和worker使用相同的顺序为模型参数分配内存，那么参数的内存指针将是相同的。内存守护进程和worker很容易保持相同的顺序，因为神经网络模型是已知的，并且内存守护进程只需要使用与worker相同的顺序来传输模型。内存守护进程只使用一次GPU IPC初始化worker，然后使用廉价的CPU IPC通知worker哪个管道组已经被传输，这样可以将昂贵的GPU IPC的使用降至最低。</p><h3 id="Active-Standby-Worker之间的切换"><a href="#Active-Standby-Worker之间的切换" class="headerlink" title="Active-Standby Worker之间的切换"></a>Active-Standby Worker之间的切换</h3><p>本文设计的一个Active-Standby Worker切换机制，隐藏了任务清理和任务初始化的开销：<br>每个worker都是一个独立的进程，并且在第一次创建它时初始化它自己的GPU环境（即CUDA上下文）。这消除了将新任务分配给工作线程时的GPU环境初始化开销。PipeSwitch将“同步点”插入到训练任务中，这样排队中函数的数量是有限的，并且可以快速清除GPU上排队的异步CUDA函数。而推理任务不需要同步点，因为推理任务可以很快完成且不被抢占。另一个任务是释放GPU内存。PipeSwitch的清除过程的一个重要特性是它不修改内存的内容，而只清理元数据，即GPU内存指针。由于GPU内存由PipeSwitch管理，清理过程只删除指向张量数据的指针，而不是释放实际数据。因此，新任务将其模型同时传输到GPU内存是安全的。也就是说，我们可以将当前任务的任务清理和新任务的流水线模型传输并行化，以隐藏任务清理的开销。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>作者首先使用end-to-end实验来证明PipeSwitch带来的效果，然后通过实验说明每个组件的设计的有效性。最终效果是PipeSwitch能够实现不超34.6ms的额外切换开销（远低于Nvidia MPS）。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/PipeSwitch/3.jpeg" width="80%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表1：用不同方法进行GPU上下文切换的overhead</div></center><br><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文提出了PipeSwitch，一个能够为多个深度学习应用程序实现GPU高效细粒度分时的系统。PipeSwitch通过流水线式上下文切换来最小化深度应用程序GPU上的任务切换开销。流水线上下文交换包括三个关键技术：流水线式模型传输、统一内存管理和Active-Standby Worker切换。通过这些技术，PipeSwitch能够实现毫秒级的任务切换时间，使分时GPU上的深度应用程序能够满足严格的SLO。本文在各种DNN模型和GPU上进行了实验，验证了PipeSwitch的性能。PipeSwitch可以显著提高GPU的利用率，提高深度应用的灵活性。其实这种流水线的思路其实在去年SOSP上出现的PipeDream和ByteScheduler中也都有体现，也可以应用于与深度学习任务相关的其他方面的系统级优化。</p><hr><p><strong>原文作者：</strong>Zhihao Bai, Zhen Zhang, Yibo Zhu, Xin Jin<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/osdi20-bai.pdf">https://www.usenix.org/system/files/osdi20-bai.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/netx-repo/PipeSwitch">https://github.com/netx-repo/PipeSwitch</a><br><strong>参考文献：</strong>[1] OSDI 2020 有哪些值得关注的文章？<a href="https://www.zhihu.com/question/414538410/answer/1550912187">https://www.zhihu.com/question/414538410/answer/1550912187</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习系统 </tag>
            
            <tag> OSDI </tag>
            
            <tag> 上下文切换 </tag>
            
            <tag> 任务调度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ByteScheduler: 加速分布式训练的通信调度器</title>
      <link href="2020/12/07/BS/"/>
      <url>2020/12/07/BS/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Generic-Communication-Scheduler-for-Distributed-DNN-Training-Acceleration"><a href="#A-Generic-Communication-Scheduler-for-Distributed-DNN-Training-Acceleration" class="headerlink" title="A Generic Communication Scheduler for Distributed DNN Training Acceleration"></a>A Generic Communication Scheduler for Distributed DNN Training Acceleration</h1><p>本文发表于SOSP’19，由香港大学和字节跳动合作，提出了一种用于分布式DNN训练加速的通用通信调度算法ByteScheduler。ByteScheduler的设计基于这样的原则性分析：对tensor传输进行划分和重新排列可以在理论上获得最佳结果，并且在实际应用中可以获得良好的性能，即使在有调度开销的情况下也是如此。为了使ByteScheduler能够在各种DNN训练框架中通用地工作，本文引入了一个统一的抽象和一个依赖代理机制来实现通信调度，且不破坏框架引擎中原有的依赖关系。在此基础上，本文提出了一种贝叶斯优化方法，可以在不同的网络环境下，根据不同的训练模型自动调整tensor划分的大小和其他参数。ByteScheduler现在支持TensorFlow、Pythorch和MXNet，无需修改它们的源代码就可以很好地与参数服务器（PS）和all-reduce架构（使用TCP或RDMA）进行梯度同步。实验结果表明，ByteScheduler在所有实验系统配置和DNN模型下的训练速度提高了196%（原始速度的2.96倍）。</p><h2 id="研究背景和动机"><a href="#研究背景和动机" class="headerlink" title="研究背景和动机"></a>研究背景和动机</h2><p>深度神经网络（DNN）在计算机视觉、自然语言处理等各个领域都有十分广泛的应用。然而，由于训练数据集和DNN模型的大小都在逐渐增大，训练DNN的任务需要很长的时间和大量的计算资源。目前最常见的方式是通过数据并行加速DNN训练，但是由于通信的开销，其效果远不如“线性的加速”。</p><p>基于参数服务器（PS）的数据并行通信通常由三步组成：</p><ul><li>push：每个worker用本地的训练数据计算梯度并将梯度发送到PS；</li><li>update：PS将各个worker的梯度加和并更新参数；</li><li>pull：各个worker与PS进行参数同步。</li></ul><p>在DNN的分布式训练中，对tensor的计算和通信会形成一个依赖图，如图1所示：backward依赖于forward；push依赖于backward；pull依赖于push，forward依赖于pull。基于这样的依赖，深度学习框架的引擎会执行这样的一张有向无环图（DAG）。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/1.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：分布式训练中的层级计算和通信（以MXNet的参数服务器为例）</div></center><br>在默认的情况下，通信的操作按照“先进先出”（FIFO）的方式从模型的最后一层到第一层执行。例如，如图2所示，绿色的矩形表示push tensor，黄色的矩形表示pull tensor。从layer2到layer1，他们的计算和通信都是按照FIFO的顺序执行的。FIFO的顺序的主要问题在于它不能很好地overlap计算和通信。已有工作（例如P3和TicTac）对此的改进是通信调度，即将tensor划分成几个更小的部分并改变传输的顺序。在如图3所示的例子里，这样的方法和图2中FIFO的方法相比，训练速度提升了40%。<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/2.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：通过tensor划分和调度实现比FIFO更快的训练</div></center><br><p>P3和TicTac等已有的通信调度设计都是基于特定的框架来实现的，而且他们所做的优化都是基于一些实证结果的。然而，在实际应用中，分布式DNN训练的setup是多种多样的：不仅机器学习的框架有许多种，通信的架构（all-reduce，PS）和网络协议（TCP，RDMA）也是多种多样的。因此，一种通用的通信调度方法对于整个community和工业界都是很有价值的。通用的通信调度有三点要求：（1）对于各种的setting都可用；（2）对已有框架不做大量修改；（3）实现调度上的优化。</p><p>作者观察到，无论在什么样的实验setting下，DNN训练中的计算图几乎都是一样的。几乎所有的模型都有层级结构，计算都是一层一层执行的。尽管不同的框架有不同的特性，对于同一个模型来说，不同框架运行的计算图几乎都是一样的。而且不同的通信方式也不会改变模型的计算图。基于这样的特性，作者提出了ByteScheduler：一个通用的tensor通信调度框架。ByteScheduler从不同的实验设定中抽象出tensor调度的概念，并提出了一个无论在理论上还是实际应用中都有效的调度算法。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/3.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：用TensorFlow进行分布式训练时iteration之间的barrier（通过参数服务器通信）</div></center><br><p>其中一个挑战是一些框架例如TensorFlow和PyTorch在两个iteration之间引入了一个barrier（图3），这个barrier的存在会使得任何对push/pull或者all-reduce的调度很低效。另一个挑战是如何适应不同的运行时环境，需要平衡训练效果和训练速度。</p><h2 id="ByteScheduler的设计"><a href="#ByteScheduler的设计" class="headerlink" title="ByteScheduler的设计"></a>ByteScheduler的设计</h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>ByteScheduler在通信栈中位于engine层之上，API层之下。对于每一种机器学习框架，ByteScheduler都有一个把每一个通信的operation包成一个统一CommTask抽象的plugin。然后，ByteScheduler core会把CommTasks进行分割和调度。这样，ByteScheduler就可以在不同的框架之间都适用。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/4.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：ByteScheduler在通信栈中的架构</div></center><br><h3 id="对通信任务的统一抽象"><a href="#对通信任务的统一抽象" class="headerlink" title="对通信任务的统一抽象"></a>对通信任务的统一抽象</h3><p>CommTask是一个通信操作的抽象，例如push一个tensor，pull一个tensor或者all-reduce一个tensor。CommTask提供了四种不同的接口，分别为：</p><ul><li>partition(size)：划分CommTask为SubCommTasks；</li><li>notify_ready()：通知Core一个CommTask已准备就绪；</li><li>start()：开始执行一个CommTask的任务；</li><li>notify_finish()：通知Core一个CommTask已完成。</li></ul><h3 id="与框架Engine的交互"><a href="#与框架Engine的交互" class="headerlink" title="与框架Engine的交互"></a>与框架Engine的交互</h3><p>Denpendency Proxy是ByteScheduler中的一个操作（operation），它可以使ByteScheduler从框架中得到对通信调度的控制。ByteScheduler在每一个反向传播（back propagation）和通信操作之间加一个proxy，这个proxy做三件事情：（1）当一个反向传播计算结束之后，框架的engine会通过proxy通知ByteScheduler Core一个CommTask已经准备就绪，可以开始通信了。（2）当Core决定开始这个CommTask的时候，Core通过CommTask.start()来结束这个proxy（即这个proxy的生命周期是从backward结束开始、通信开始结束，它的使命就是在计算和通信之间作为控制他们执行顺序的桥梁）。（3）在CommTask结束之后，会使用另一种proxy，这个proxy只通过CommTask.notify_finish()发出信号。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/5.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：没有global barrier时，Core和框架engine之间的交互</div></center><br><p>而对于有barrier的机器学习框架，proxy在实现上有所不同，ByteScheduler会通过proxy来跨过barrier的限制。这个是通过“out-of-engine communication”实现的，即在engine之外进行实际的通信。ByteScheduler将原有的communication operation替换为一个异步的op，这个op会在后台开始通信。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/6.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图6：有global barrier时，Core和框架engine之间的交互</div></center><br><p>将通信改为异步通信之后，通信的op会在实际的通信执行完成之前就返回，这样可能导致下一个iteration的forward在完成通信之前就开始了计算，这违背了训练的dependency graph。为了保障每一层的计算和通信保持的正确的dependency，ByteScheduler增加了另外一种proxy，这个proxy可以block下一个iteration的forward propagation，直到通信结束。例如如图7所示，红色的线是框架原有的dependency，绿色的线是ByteScheduler的dependency。Global barrier仍然存在，但是这个global barrier不会影响通信。虽然proxy可以跨过这个barrier，但是proxy不会改变barrier所维持的通信和计算顺序的依赖。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/6.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图7：引入out-of-engine communication后的依赖图</div></center><br><h2 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h2><p>在通信的调度上，以前的工作采用stop-and-wait的方法，他们会在发送一个tensor之后等待通信完成，然后再发送下一个tensor。这样的方法不能充分利用网络带宽。</p><p>本文提出了Credit-based Preemption：它的工作原理和“滑动窗口”类似，credit size是窗口的大小，它允许在同一个滑动窗口内的多个tensor同时进行通信。Credit size对于这个方法来说是一个很重要的系统参数：credit size越大，可以同时通信的tensor越多，带宽的利用率就越高；但是由于通信栈是FIFO的，credit size越大，这个方法对窗口内tensor的通信顺序的调整就越不够及时。因此，找到最佳的partition size 和preemption size十分重要。</p><p>最佳的partition size 和preemption size会受到网络带宽、worker的数量、模型的结构、CPU和GPU的类型等等多种因素的影响。ByteScheduler会在运行时对最佳的参数选择进行搜索，本文使用了贝叶斯优化来自动调整参数大小。（贝叶斯优化在神经网络超参数自动化搜索中已有广泛的应用）</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>ByteScheduler实现了MXNet，Pytorch和Tensorflow三种框架，用户在使用时只需要在原有框架训练代码的基础上添加两行代码就可以了。</p><p>实验使用了16台机器的集群，每台机器有8张V100。</p><p>在实验中，作者将ByteScheduler和机器学习框架本身进行比较；另外也Linear scaling（机器学习框架本身的速度乘以机器的数量）做了比较。</p><p>在对训练的加速上，对于不同的模型和不同的框架，ByteScheduler都可以极大地提升框架原有的训练效率，在一些情况下ByteScheduler的表现和Linear Scaling相似，在GPU数量较多时加速会更明显。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/8.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图8：使用MXNet框架，PS，RDMA时不同模型的训练速度</div></center><br><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/9.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图9：使用VGG16模型时不同框架的训练速度</div></center><br><p>在不同的网络带宽环境下，ByteScheduler都可以提高训练速度，且对参数的自动调整对训练速度带来的提升十分明显。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/BS/9.png" width="75%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图10：使用MXNet框架，PS，RDMA时不同带宽下的训练速度</div></center><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文提出了一个加速分布式训练的、通用通信调度器ByteScheduler，主要有以下三点贡献：</p><ul><li>提出了tensor通信调度的统一抽象；</li><li>在几乎没有对现有的机器学习框架的修改的情况下，实现了对多种不同框架的支持；</li><li>提出了可以自动调整参数的tensor调度方法。</li></ul><p>尽管很多人表示ByteScheduler并没有论文里所带来的这么好的效果，且因为实现不够完善，用户体验不佳，但是不可否认的是ByteScheduler所提出的方法确实可以有效的解决许多问题，在工业界和学术界都很有价值，在分布式机器学习和通信调度方面有不小的影响力。</p><hr><p><strong>原文作者：</strong>Yanghua Peng, Yibo Zhu, Yangrui Chen, Yixin Bao, Bairen Yi, Chang Lan, Chuan Wu, Chuanxiong Guo<br><strong>原文链接：</strong><a href="https://dl.acm.org/doi/pdf/10.1145/3341301.3359642">https://dl.acm.org/doi/pdf/10.1145/3341301.3359642</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> SOSP </tag>
            
            <tag> 通信调度 </tag>
            
            <tag> 深度学习框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AntMan：面向深度学习的GPU集群动态弹性伸缩方法</title>
      <link href="2020/12/02/AntMan-0/"/>
      <url>2020/12/02/AntMan-0/</url>
      
        <content type="html"><![CDATA[<h1 id="AntMan-Dynamic-Scaling-on-GPU-Clusters-for-Deep-Learning"><a href="#AntMan-Dynamic-Scaling-on-GPU-Clusters-for-Deep-Learning" class="headerlink" title="AntMan: Dynamic Scaling on GPU Clusters for Deep Learning"></a>AntMan: Dynamic Scaling on GPU Clusters for Deep Learning</h1><p>在大规模GPU集群上高效地调度深度学习作业对作业性能、系统吞吐量和硬件利用率来说至关重要。随着深度学习的工作量变得越来越复杂，GPU集群上的调度变得越来越具有挑战性。本文介绍了一个面向深度学习的、可以实现集群资源弹性伸缩的系统AntMan，它与深度学习框架共同设计集群调度程序，并已在阿里巴巴生产中部署，用于管理数千个GPU上的数万个日常深度学习作业。AntMan利用深度学习训练的独特特性，在深度学习框架中引入了内存和计算的动态缩放机制。评估结果表明，在不影响公平性的前提下，AntMan在多租户集群中提高了42%的GPU内存利用率和34%的计算利用率，为大规模高效利用GPU提供了一种新的途径。</p><p>本文由阿里团队发表于 OSDI’21，是一作之一肖文聪博士任职阿里后开展的工作；项目负责人为贾扬清博士（阿里副总裁，pytorch、caffe等框架的主要贡献者）。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>在过去的几年里，我们见证了GPU上深度学习的巨大成功。大型公司通常会为深度学习工作负载构建多租户GPU集群。在阿里巴巴，我们观察到共享多租户深度学习集群中GPU硬件的利用率很低，同时许多作业等待资源需要很长的时间。深度学习集群的低利用率主要是由两个方面的原因引起的：一方面，训练任务不能充分利用所有的GPU资源；另一方面，深度学习任务的资源通常需要“组调度”（gang-scheduling），这导致许多任务为了获得足够的资源数量而需要等待很长时间。</p><p>在共享GPU上打包作业可以提高GPU的利用率，并使同一个集群总体上完成更多的作业。然而，这种方法很少有实际应用，因为这会使不同训练作业之间互相干扰，最终导致训练作业的速度减慢，同时也会造成因内存不足而导致的任务失败。</p><p>文章作者提出AntMan，可以在保证作业任务调度公平性以及作业任务训练速度的同时提升集群资源利用率。</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>文章作者从三个角度研究了生产集群中的资源使用情况：硬件、集群调度和作业行为。<br>•    <strong>正在使用中的GPU的利用率低。</strong> 通过对集群中GPU利用率的分析，作者发现只有20%的GPU运行的应用程序消耗了超过一半的GPU内存。就计算单元的使用而言，只有10%的GPU达到80%以上的利用率。这一统计数据表明GPU内存和计算单元没有得到充分利用，从而浪费了昂贵的硬件资源。<br>•    <strong>组调度中GPU资源空闲等待时间长。</strong> 为了利用海量数据训练深度学习，分布式多GPU训练是必不可少的。多GPU训练作业需要群调度，这意味着除非所有需要的GPU同时可用，否则作业不会开始训练。然而，在集群中，GPU资源很难同时得到满足。作业需要的资源越多，在空闲等待模式下由于部分资源预留而浪费的GPU周期就越多，集群持有空闲资源所需的成本就越高。<br>•    <strong>训练任务的动态资源需求。</strong> 作者发现深度学习训练作业在其生命周期中通常不能充分利用GPU资源。图1显示了在训练ESPnet模型时一段10分钟（1200∼1800秒）的记录。在训练阶段，ESPnet消耗3.6GB的GPU内存，动态GPU Streaming Multiprocessor利用率高达70%。在1400秒时开始evaluation阶段，主要在进行GPU上的解码（大约1400∼1600秒）和CPU上的合成（大约1600∼1700秒），其中解码阶段需要高达19GB的GPU内存。之后继续进行模型训练。这种作业内动态资源需求在生产深度学习pipeline中很常见，因此很难预测训练任务所需资源的数量。这种动态的资源需求与深度学习任务中的固定资源配置以及长时间运行时间相冲突。如果根据峰值使用情况以分配足够的资源会使得昂贵的硬件未得到充分利用。如果没有足够的资源，工作效率可能会受到限制，从而延迟作业完成时间。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/antman/1.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：在speech 数据集上ESPnet模型的GPU利用率</div></center><br>与此同时，作者发现了深度学习作业的两个特性：首先，只有一小部分内存用于存储深度学习模型，大部分所使用的内存在每一个mini batch中不断被分配和释放。另外，深度学习训练周期也很短，有80%的任务在600毫秒内就可以完成一个mini batch。根据这两个特性，设计了在共享GPU上调度深度学习任务的方法。<h2 id="方法设计"><a href="#方法设计" class="headerlink" title="方法设计"></a>方法设计</h2><h3 id="深度学习框架中的动态弹性伸缩"><a href="#深度学习框架中的动态弹性伸缩" class="headerlink" title="深度学习框架中的动态弹性伸缩"></a>深度学习框架中的动态弹性伸缩</h3><p>动态伸缩机制包括GPU内存和计算单元两个方面的细粒度动态控制。</p><p>图2说明了现有深度学习框架中的内存管理以及与AntMan的区别。现有深度学习框架中，缓存的GPU内存大小（红色虚线）的总大小随着深度框架中创建的张量而增加（图2a∼b）。一般来说，有些张量只在深度学习训练的某些阶段使用（例如，数据预处理、评估），在其他情况下这些张量是不被需要的。但是，这部分缓存的GPU内存没有释放（图2c）。在这种情况下，分布式缓存框架的性能得到了优化，从而丧失了内存的性能。</p><p>AntMan扩展GPU内存上限。它会主动检测已用内存，以收缩缓存内存，从而内省地调整GPU内存使用量，使之适合。这是通过在处理mini batch时监控应用程序性能和内存需求来实现的（图2d）。AntMan尽可能在GPU设备上分配张量，但是，如果GPU内存仍然不足，则可以在GPU之外使用主存分配张量（图2e）。有了这样的通用内存支持，作业甚至可以继续处理低于其实际GPU内存需求的进程。当GPU内存的上限增加时，张量可以自动分配回GPU（图2f）。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/antman/2.png">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：AntMan中动态弹性伸缩内存管理</div></center><br><p>动态计算单元管理是AntMan提出的一种控制深度学习训练作业GPU利用率的机制。通常情况下，如果把两个不同的训练任务打包在同一块GPU上，这两个训练任务会互相影响，导致训练速度变慢（图3a～b）。在AntMan中，GPU op的执行由一个新引入的模块GpuOpManager来控制。当一个GPU op准备好执行时，它会被添加到GpuOpManager中，而不是直接启动。GpuOpManager持续分析GPU op的执行时间，并在启动GPU op之前简单地分配空闲时隙（图3c）。这样就达到了控制两个任务训练快慢的目的。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/antman/3.png" width="50%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：在一个共享的GPU中运行两个作业而不受干扰的计算管理</div></center><br><h3 id="协同调度器"><a href="#协同调度器" class="headerlink" title="协同调度器"></a>协同调度器</h3><p>如图4所示，AntMan采用了层级结构，其中全局调度器负责作业调度。每个工作服务器都包含一个本地协调器，该协调器负责通过考虑来自深度学习框架的统计信息，使用动态资源伸缩的原语来管理作业的执行。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/antman/4.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：AntMan中协同调度的工作流</div></center><br><p>在AntMan中，作业被全局调度器分为resource-guarantee jobs和opportunistic jobs两种类型，并应用不同的调度策略。默认情况下，全局调度器将估计作业的排队时间，有长队列延迟的作业将作为opportunistic job自动执行。用户也可以为了避免排队时间过长自行将训练任务设置为opportunistic job。对于guarantee jobs来说，AntMan会保证他们有足够的资源。而对于opportunistic jobs来说，AntMan会尽量快地为其分配资源，但不保证所分配的资源是足够的。</p><p>而本地调度器从三个方面保证调度的公平性。第一，每块GPU上只能有一个resource-guarantee job，本地调度器会通过控制opportunistic jobs所使用的资源来尽量保证resource-guarantee jobs的资源使用。第二，本地调度器通过深度学习框架记录训练任务的一些日志，如果发现resource-guarantee job 增加了GPU内存的需求，即把一些张量存到主存里了，就会减少opportunistic job所使用的内存。Resource-guarantee job对GPU计算资源使用增加的情况也是类似的。第三，对于不同种类的深度学习模型，增加内存对他们performance的提升效果是不同的。AntMan优先把资源给那些只要有一点内存增加就有比较显著performance提升的训练任务。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>作者从三个方面对ANtMan进行了评估：首先评估了AntMan的动态弹性伸缩机制，以说明动态弹性伸缩机制在深度学习作业中是必要的且高效的；然后将AntMan和其他的集群调度器进行对比；最后通过AntMan在阿里巴巴集群中部署应用前后的集群使用情况说明AntMan可以提升GPU集群中训练任务的效率和表现。</p><p>作者将AntMan和其他几种在同一块GPU上训练两个不同任务的训练策略进行比较（表1），使用AntMan时不会因为内存不足而导致训练任务失败，且作业完成时间最短，验证了AntMan内存动态弹性伸缩的必要性。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/antman/5.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">表1：两个深度学习作业在不同调度器策略下执行的作业状态和作业完成时间</div></center><br> <p>如图5所示，作者将ResNet50训练任务和ESPnet训练任务放在同一块GPU上执行，并比较了在“打包模式”和动态计算资源调整模式下GPU计算资源利用率的表现。实验结果表明，AntMan的策略可以提高opportunistic job对GPU计算资源的利用，与opportunistic job单独使用一块GPU训练时相比，AntMan的方法可以做到在ResNet50模型训练任务维持57%的效率的情况下，几乎不会造成对opportunistic job训练效率的影响。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/antman/6.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图5：不同模式下的SM利用率</div></center><br><p>作者比较了在阿里巴巴集群上使用AntMan之前和之后训练任务的排队时长和集群中硬件的使用情况。比较结果表明，AntMan为这个集群中的深度学习训练任务提供了高达17.1%的额外GPU资源。硬件统计显示，AntMan的GPU内存使用率平均提高了42%，GPU利用率平均提高了34%。由于集群吞吐量的提高，作业请求延迟平均减少了2.05倍，尾部延迟显著减少了一个数量级以上。使用了AntMan之后，99%的任务在与其他任务协同工作时不受影响。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>文章提出了AntMan，一个部署在阿里巴巴GPU集群中的深度学习基础设施。AntMan可以在运行时对单个深度学习作业的GPU资源进行灵活的细粒度控制，并设计集群调度器和深度学习框架进行协同作业管理，使得GPU能够尽可能充分利用GPU，同时避免对其他作业任务的干扰。AntMan在不影响调度公平性的前提下，将阿里巴巴GPU集群的GPU内存利用率和计算单元利用率分别提高了42%和34%。</p><hr><p><strong>原文作者：</strong>Wencong Xiao, Shiru Ren（共同一作）, Yong Li, Yang Zhang, Pengyang Hou, Zhi Li, Yihui Feng, Wei Lin, Yangqing Jia<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/osdi20-xiao.pdf">https://www.usenix.org/system/files/osdi20-xiao.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/alibaba/GPU-scheduler-for-deep-learning">https://github.com/alibaba/GPU-scheduler-for-deep-learning</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 集群调度 </tag>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> OSDI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Facebook在边缘设备上的ML推理计算的现状</title>
      <link href="2020/10/13/EdgeInference-FB/"/>
      <url>2020/10/13/EdgeInference-FB/</url>
      
        <content type="html"><![CDATA[<h1 id="Machine-Learning-at-Facebook-Understanding-Inference-at-the-Edge"><a href="#Machine-Learning-at-Facebook-Understanding-Inference-at-the-Edge" class="headerlink" title="Machine Learning at Facebook: Understanding Inference at the Edge"></a>Machine Learning at Facebook: Understanding Inference at the Edge</h1><ol><li>Android设备上，AI边缘计算硬件碎片化严重。排名前50名的SoCs也仅仅占整个Android市场的65%，只有排名前225的SoCs才能占到95%的Android设备。</li><li>移动端CPUs多样化且非常旧，大约有一半的CPU都具有两个CPU集群（高性能核心集群与节能核心集群）。</li><li>移动端就算使用GPU进行AI边缘计算，但整体情况也不容乐观：大约只有11%的设备中GPU的推理速度是CPU的3倍，只有23%的设备GPU的推理速度是CPU的2倍，绝大部分的设备使用GPU推理速度提升并不明显，甚至还有下降。</li></ol><ul><li>移动SoCs的产品策略问题：中端SoC的CPU一般只比高端SoC慢10% - 20%，但是GPU却慢了2至4倍；</li><li>不同于PC，移动端SoC中CPU与GPU共用内存，两者之前会争夺稀缺的内存带宽。</li></ul><ol start="4"><li>运行在高通SoC上的Facebook程序，只有5%的设备具有“计算”DSP，市场中大多数的DSP没有集成“计算/矢量”DSP；</li><li>可编程性是GPU与移动协处理器运用的主要障碍：OpenCL库与驱动的不稳定性，导致不能大规模使用。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/EdgeInference-Facebook/0.png" width="85%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：Facebook移动端机器学习的执行流程</div></center><br><hr><p><strong>原文作者：</strong>Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, Tommer Leyvand, Hao Lu, Yang Lu, Lin Qiao, Brandon Reagen, Joe Spisak, Fei Sun, Andrew Tulloch, Peter Vajda, Xiaodong Wang, Yanghan Wang, Bram Wasti, Yiming Wu, Ran Xian, Sungjoo Yoo, Peizhao Zhang<br><strong>原文链接：</strong><a href="https://research.fb.com/wp-content/uploads/2018/12/Machine-Learning-at-Facebook-Understanding-Inference-at-the-Edge-v2.pdf">https://research.fb.com/wp-content/uploads/2018/12/Machine-Learning-at-Facebook-Understanding-Inference-at-the-Edge-v2.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习系统 </tag>
            
            <tag> 终端 </tag>
            
            <tag> 推理系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分析面向DNN训练的大型多租户GPU集群</title>
      <link href="2020/09/21/atc19jeon/"/>
      <url>2020/09/21/atc19jeon/</url>
      
        <content type="html"><![CDATA[<h1 id="Analysis-of-Large-Scale-Multi-Tenant-GPU-Clusters-for-DNN-Training-Workloads"><a href="#Analysis-of-Large-Scale-Multi-Tenant-GPU-Clusters-for-DNN-Training-Workloads" class="headerlink" title="Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads"></a>Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads</h1><p>通过分析微软Philly集群log来分析深度学习训练workload的特征。通过将调度程序日志与各个作业的日志关联起来，文章研究了三个不同的问题，这些问题影响多租户集群上DNN训练工作负载的集群利用率：（1）gang scheduling 和位置限制对排队的影响，（2） 局部性对GPU利用率的影响，以及（3）训练期间的failure。 基于在大规模运营中的经验，文章提供了有关DNN培训工作负载的下一代集群调度程序的设计指导建议。</p><p>文章使用的资源调度和集群管理服务：微软Philly.</p><p>深度学习工作负载workload对集群管理系统提出了许多新要求或约束：需要GPU来做大量浮点运算，但是和CPU不同，并没有适当的硬件支持细粒度共享。<br>本文研究了locality-aware scheduling 如何影响性能和利用率的两个主要方面。 1.等待局部性约束如何影响训练作业开始之前的排队延迟。2.locality-aware scheduling 如何影响分布式训练作业的GPU利用率。<br>文章还探究了训练任务失败的原因。</p><p>文章为下一代的GPU集群调度器提出了三个指导性建议：</p><ol><li>由于缺乏局域性会影响利用率和作业运行时间，并且由于DNN训练任务长期运行，因此调度程序应以queue的延迟为代价。</li><li>共享单个服务器的不同作业可能会相互干扰，从而不利地影响其训练时间。 因此，调度程序的目标应该是隔离专用服务器上的作业，同时实施诸如迁移以进行碎片化的技术，以支持需要更多GPU的作业的局限性。</li><li>应该尽早发现许多故障，而不是安排在更大的共享集群上。这可以通过在小型专用服务器池上安排每个传入的作业，甚至使用单个GPU应该能够捕获来自多GPU作业的简单编程和配置错误来实现。 此外，在运行时对故障进行在线分析可以使调度程序调整其重试策略，从而避免浪费的重新执行时间。</li></ol><p>文中通过对log分析得到的结论：<br>1.随着训练的进行，放松局部性以减轻分布式训练的排队延迟具有一定的合理性。除了公平份额排队延迟之外，对gang scheduling和局部性的需求还为机器学习作业带来了碎片化延迟。<br>2.在共享集群中，GPU并没有充分利用。文中介绍了分布和干扰如何影响利用率。</p><hr><p><strong>原文作者：</strong>Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee, Junjie Qian, Wencong Xiao, Fan Yang<br><strong>原文链接：</strong><a href="https://www.usenix.org/system/files/atc19-jeon.pdf">https://www.usenix.org/system/files/atc19-jeon.pdf</a><br><strong>公开数据集：</strong><a href="https://github.com/msr-fiddle/philly-traces">https://github.com/msr-fiddle/philly-traces</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> ATC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式设施上的可扩展深度学习：挑战、技术、和工具</title>
      <link href="2020/07/20/DistributedInfrastructures/"/>
      <url>2020/07/20/DistributedInfrastructures/</url>
      
        <content type="html"><![CDATA[<h1 id="Scalable-Deep-Learning-on-Distributed-Infrastructures-Challenges-Techniques-and-Tools"><a href="#Scalable-Deep-Learning-on-Distributed-Infrastructures-Challenges-Techniques-and-Tools" class="headerlink" title="Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques, and Tools"></a>Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques, and Tools</h1><p>为了让深度学习效果更好，深度学习系统的“可扩展性”需要提升。本文探索了分布式设备中深度学习的“可扩展性”的挑战、方法和工具，这包括用于深度学习的设备、并行深度学习训练的方法、多租户资源调度、训练和模型数据管理四个方面。<br>本文分析并比较了11个目前开源的深度学习框架和工具，并探究哪些方法有常见的实际应用。最后，本文提出深度学习系统中值得进一步探究的未来研究方向。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>深度学习在一些领域中表现优越，受到了越来越多的关注。深度学习的应用在各种领域都有应用潜质。很多公司开发了可以自动执行dl模型的强大的深度学习框架。<br>使得DL成功的一个因素是模型训练的三个维度上。1. 模型本身的大小和复杂度。（深度学习模型从最简单的模型逐渐向复杂演变）2. 训练数据量 3. 设备infrastructure高度并行的硬件，GPU等）<br>survey专注于处理DL使用的大型、分布式设备时遇到的挑战。管理大量的DL模型并用大量的数据训练这些模型很有挑战性。这涉及到并行的问题、资源调度与扩展、数据管理与可移植性等。这个领域发展迅速。与此同时，我们可以看到大量开源深度学习框架和编排系统出现。</p><h3 id="补充调查-Complementary-survey"><a href="#补充调查-Complementary-survey" class="headerlink" title="补充调查 Complementary survey"></a>补充调查 Complementary survey</h3><p>已有survey not focused on scalability and distributed systems for DL</p><h3 id="survey结构"><a href="#survey结构" class="headerlink" title="survey结构"></a>survey结构</h3><p>In Section 2, we introduce DL and provide the foundations for the further discussion of DL systems.<br>In Section 3, we discuss the challenges and techniques of scalable DL in detail. We cover four important aspects: Distributed infrastructures, parallelization of DL training, resource scheduling and data management.<br>In Section 4, we analyze and compare 11 open-source DL frameworks and tools that put scalable DL into practice.<br>Finally, in Section 5,we conclude this survey and provide an outlook on current trends and open problems in the field that deserve further research.</p><h3 id="分布式深度学习"><a href="#分布式深度学习" class="headerlink" title="分布式深度学习"></a>分布式深度学习</h3><p>用大量数据训练大模型通常需要有多个计算节点的分布式设施，每个节点中可能有多个GPU。这带来一系列的挑战：1.<br>避免由于通信瓶颈而停止使用costly GPU资源；2. 不同用户/进程之间共享计算、存储和网络资源，以减少开销并提供可拓展性。Figure 3</p><h4 id="硬件设施"><a href="#硬件设施" class="headerlink" title="硬件设施"></a>硬件设施</h4><ul><li><p><strong>深度学习硬件的组成</strong><br>多核CPU-&gt;GPU-&gt;FPGAs-&gt;……</p></li><li><p><strong>用于深度学习的大规模设备：</strong>有许多内部连接的硬件组成，构成一个warehouse-scale computer（仓库规模计算机）。</p><ul><li><p>Facebook：训练时既用GPU也用CPU，依靠CPU做inference，放弃使用RDMA/NCCL。</p></li><li><p>腾讯：也用GPU+CPU。他们的深度学习系统Mariana由三种框架组成，用于不同的设备/用况。</p></li><li><p>微软：用于DL的大型分布式系统Adam。分割模型时，模型的层可以放进L3缓存，以提升训练的performance。</p></li><li><p>谷歌：TPU，采用独特的通信协议</p></li><li><p>HPC：Infiniband有更高的吞吐率</p><p>总结来说：这些设备的异构性都很强，除了GPU服务器以外，还包含CPU。除此以外，也有专门用于深度学习的设备，尤其在谷歌和学术研究中。在HPC设备中，我们看到，为了使得性能更好，深度学习系统是针对设备的，例如NCCL，MPI等。</p></li></ul></li></ul><h4 id="并行的方法"><a href="#并行的方法" class="headerlink" title="并行的方法"></a>并行的方法</h4><ul><li><p><strong>数据并行</strong><br>挑战：不同worker之间parameter的同步。在3.3中进一步讨论这些挑战。数据并行主要的优势在于任何模型都可以用，对于计算量大但是参数少的op（如CNN）来说可以很好的scale（？？）。但是对于参数特别多的op来说，参数的同步会成为瓶颈。通过使用更大的batch size，这个问题可以被缓解，但这会导致模型convergence不佳。另外，当模型对于单个设备来说太大的时候，数据并行的模式不合适。在数据并行时，默认训练数据时相互独立的且同分布的。</p></li><li><p><strong>模型并行</strong><br>模型并行中的主要挑战是：如何划分模型。一个常用方法是用强化学习，从一个初始的划分方案开始逐步探究。Streaming rollout的方法只适用于RNN。<br>模型并行的主要优势是：减少内存占用，适用于超大模型。<br>模型并行的缺点：大量的通信。由于通信和同步的延迟，worker可能停止工作，因此增加模型并行的degree（worker数？）并不一定会提升训练速度。</p></li><li><p><strong>流水线并行：</strong>模型并行中结合了数据并行，例如piprdream，通过microbatches充分利用worker，与此同时爆出了模型并行的优点。</p></li><li><p><strong>混合并行（Hybrid）</strong><br>  前三者的混合。</p><p>  Mesh-TensorFlow：模型并行+数据并行</p><p>  卷积/池化适合数据并行（大量计算，少量param）；全脸阶层适合模型并行。</p><p>  除此以外，还有layer-wise并行/自动选择并行模式.</p></li></ul><h4 id="数据并行的优化"><a href="#数据并行的优化" class="headerlink" title="数据并行的优化"></a>数据并行的优化</h4><p>数据并行中，parameter的同步导致三个方面的挑战：1. 如何同步参数。使用中心化的方式还是去中心化的方式？2. 何时同步参数。是否应该没个batch都同步？3. 如何减少通信Overhead.</p><ul><li><p><strong>系统架构</strong><br>  主要挑战是需要一个可扩展的系统架构；另外系统需要易于配置（无需额外调整参数时performance就足够好了）；需要开发更底层的primitives（例如NCCL）。<br>  （1）中心化的。worker时不时地向parameter server发送参数更新。parameter server时数据并行中最重要的架构。一种常用的方法是：用哪个parameter shards，分布在不同的ps上，这些参数可以并行更新。</p><p>  P.S. 计算节点要训练时，向参数服务器查询所需参数（只需要一小部分）<br>  （2）去中心化。无ps，直接通过allreduce更新参数。此时，worker之间的拓扑结构十分重要。在全都连接起来的network中，通信的cost为O(n^2)，成为一个bottleneck。一个常用的减少overhead的方法是用ring all-reduce。</p><p>  P.S. ring all-reduce的通信成本是恒定的，与GPU数量无关，完全由系统中gpu之间最慢的连接决定。</p><p>  该算法分两个步骤进行:首先是scatter-reduce，然后是allgather。在scatter-reduce步骤中，GPU将交换数据，使每个GPU可得到最终结果的一个块。在allgather步骤中，gpu将交换这些块，以便所有gpu得到完整的最终结果。具体细节见：<a href="https://www.jianshu.com/p/8c0e7edbefb9">https://www.jianshu.com/p/8c0e7edbefb9</a></p><p>  “butterfly”等等提起的结构也被提出。</p><p>  非全部连接的network的缺点：the propagation of parameter updates to all workers needs more time, as there may be multiple hops between a pair of workers.（个人猜测：总时间下降，单个参数传输时间上升）</p><p>  除了topology以外，还有其他减少网络负载的方法。可以把需要交换的gradient分割，每次同步只发送gradient的一部分。</p><p>  去中心化方法与中心化方法比较：优势：免去了处理parameter server的麻烦；容错性更强（免去了checkpoint 权重超多的ps的麻烦）。</p></li></ul><p>在某些情况下，如果网络通信较慢，去中心化的架构比中心化的架构表现更好。<br>中心化和去中心化的学习在开元DL框架中都有广泛应用。有些框架（tf，mx）两种都支持。<br>（3）联邦学习。训练数据存储在用户的移动设备中，用户本地计算结果用于训练一个全局模型。保护隐私+减小带宽需求。<br>移动设备的带宽小，上传比下载慢很多，所以不可能重复向一个中心化的parameter server/去中心化的peer nodes上传并更新大模型的参数。为了缓解这个问题，有相关工作研究了参数采样和压缩。有人提出了减少数据上传的算法：在每一轮中，选择一些客户端，每一个被选中的客户端基于它所拥有的训练数据计算loss。为了能够converge，在客户端上的模型实例葱同样的随机初始化开始（训练）很重要。最终，一个中心化的服务器aggregate来自被选中的客户端的gradient。这个算法实际上与中心化的方法等价，但是在数据分布不同的情况下，中心化的方法表现得更好。<br>联邦学习仍然在发展中的早期，没有很多开源的深度学习框架支持联邦学习。近期出现了第一批联邦学习的工具：TensorFlow Federated是一个联邦学习实验的模拟器。PySyft是一个可以用PyTorch进行保护隐私的联邦学习的Python库。特别地，PySyft使用differential privacy methods保护敏感数据。</p><ul><li><p><strong>同步</strong><br>  何时再并行的worker之间同步参数受到了广泛的关注。主要的挑战是平衡convergence的速度和同步所需的cost。主要有三种方法：Synchronous（同步）, bounded asynchronous（有界异步）, and asynchronous training（异步训练）。<br>  （1）同步：每个iteration之后同步参数更新。好处是convergence很快，缺点是容易有straggler problem（速度收到最慢的worker的限制）。有许多工作致力于解决这里的straggler problem（例如最快的task可以取得最慢的task中已经算出来的部分更新）/改进创新同步的训练（针对联邦学习中的同步训练的）。同步的训练在许多的开源深度学习框架中都有应用。这样的方法尤其适合单机多卡的并行训练情况，这种情况下通信延迟很低，计算负载均衡，所以straggler problem不明显。</p><p>  （2）有界异步：由于深度学习的特性，少量的偏移和不确定性并不会对模型的准确率产生较大影响。在有界异步中，worker可以在没有实时更新的参数上进行训练，但是“没有实时更新的程度”是受到约束的。一定的staleness允许进行数学分析模型的收敛性证明。“界限”可以让worker在独立训练方面有更多的自由性，可以在一定程度上减少straggler problem的影响，并增加吞吐率。有许多该方法的相关工作。（例如异构worker中可以在Stale Synchronous Parallel 模型的基础上使用动态的学习率）。该方法在深度学习框架中没有广泛应用，因为观察到的相关延迟非常小，SSP的优势不够明显。但也有一些例外（相关应用）。</p><p>  （3）异步：每个worker对模型的更新完全独立。这使得数学分析模型的收敛性变得很难，但是可以完全避免straggler problem。有许多相关工作，例如Hogwild中worker不需要任何的锁就可以修改共享内存，允许worker任意修改其他worker的参数更新，只要只修改模型的一小部分，收敛效果可以很好。有许多基于Hogwild的工作。除此之外也有lock-bound的相关工作。异步训练在许多开园深度学习框架中都有应用。</p></li><li><p><strong>通信</strong><br>  主要的挑战是避免communication成为整个训练过程中的bottleneck。主要有三种提高通信效率的方法：降低模型准确率、压缩模型更新、改进通信调度。<br>  （1）降低模型的准确率：可以减少通信带宽，还可以减少模型大小（在GPU等资源受限的硬件上有用）。相关方法有：将模型的精度降低到16位等类似的降低精度的方法；除此以外，有工作结合两种optimize的方法来提高模型的准确率。模型量化的应用十分广泛。<br>  （2）压缩模型更新：无损失的压缩非常有限，通常都采用有损失的压缩方法。主要方法：梯度量化、梯度稀疏化（只传输重要的梯度）。在使用梯度量化的方法是，为了达到较高的准确率，有人提出error-feedback的方法：把上次量化梯度损失的梯度数值累加到后面的梯度上。梯度稀疏化：许多的梯度非常小（接近0），对训练没有什么贡献。CNTK和MXNet里有相关应用。<br>  （3）改进通信调度：数据并行中的通信模式通常是“突发式的”，尤其是在严格要求同步的系统中。为了不因为超过通信带宽而导致通信延迟，不同worker之间的通信需要被调度使得他们在时间上不重合。可以考虑到重要性或时效性有限传送一些特定的信息。有的方法优先传送对于模型收敛最重要的参数更新。有工作观察到在tf和pytorch等dl系统中，参数在徐念和推断过程中被随机地传送。这导致iteration time的方差很大，从而使得整个过程变慢。为了解决这个问题，TicTac强制设置一个数据传输的时间。复杂的通信调度方法还没有在开园深度学习框架中应用，可能是因为方法创新性较强。</p></li></ul><h4 id="调度和可伸展性"><a href="#调度和可伸展性" class="headerlink" title="调度和可伸展性"></a>调度和可伸展性</h4><p>深度学习中的调度问题指的是如何将训练的进程map到训练的设备节点中去。有三个方面：1. 单租户 2. 多租户 3. 模型架构与超参数搜索。</p><ul><li><p><strong>单租户</strong><br>  需要解决三个问题：哪个进程被分配到哪个资源上？被分配到同样资源上的进程的执行顺序？进程数和资源数是何时以及如何被适应的？<br>  在模型并行中，一个很重要的问题是如何划分模型。完成模型划分后，需要解决的问题是在哪里放置这些模型的部分，以及何时训练模型的哪一部分。不同模型部分之间的训练在调度上存在互相依赖。针对这一问题，有人提出了一些启发式的算法，但是都没有在深度学习框架中应用。特别地，模型划分和调度问题之间存在互相的依赖，这是有待被全面探索的。动态控制流的出现带来了其他挑战，这使得静态调度变得不可行。有工作做出了有先进scheduler的模型并行机器学习框架，能够根据依赖关系使得计算优先。Litz是一个可扩展的机器学习框架，scaling的开销很小（数据并行）。有的工作可以动态分配一些临时的资源。还有的工作实现了可以在运行时调整worker数量的去中心化的数据并行深度学习系统。</p></li><li><p><strong>多租户</strong><br>  YARN、Borg等resource scheduler不是针对深度学习任务的。针对深度学习的resource scheduler是有必要的。<br>Dolphine是一个中心化的可伸缩的数据并行机器学习框架。通过一个开销计算模型和持续的监视，parameter server的config和worker动态变化。有工作分析大型DL cluster系统的trace，根据分析结果提出了一些scheduling的建议。schedule的目标可以只是达到最高的模型准确率，也可以包括各租户之间的公平性等。<br>除此以外，还有【深度学习服务】的相关工作。IBM：DL as a service，深度学习的云端解决方案。</p></li><li><p><strong>模型架构和超参数搜索</strong><br>  这是深度学习领域中一个非常重要的问题。例如，给定一个深度学习任务，什么模型架构能够达到最高的模型准确率（CNN中，需要多少层，每层多大？）？什么样的超参数设置可以使得模型收敛速度最快？找到这样问题的答案很困难。典型的方法是不断尝试各种架构和超参数。另有考虑到其他方法。例如，可以把需要访问同样训练数据的训练任务batch到一起以节省网络负载，并在后续的执行中可以进一步优化。有的scheduler支持较早地停下在相应config下表现较差的训练任务，还可以预测准确率表现，还可以给预期准确率较高的任务分配更多的资源。有的系统为了单块GPU上多个训练任务而设计：一起执行多个模型，并一起做算子融合、共享I/O等优化。</p></li></ul><h4 id="数据管理"><a href="#数据管理" class="headerlink" title="数据管理"></a>数据管理</h4><p>既指对训练数据的管理，也指对训练完或训练中模型的管理。</p><ul><li><p><strong>训练数据</strong><br>  获取大量带标签的训练数据很难。一个方法是手动标记，但是需要大量时间精力。因此，有一些可以用可以方便获得的、标签不是很准确的（noisy）数据训练的方法。例如，有方法训练模型用于预测训练集的noise type（？）。这是一个很大的研究领域。<br>训练数据的预处理也很重要。包括数据正则化和数据增强（例如图片中物体的方向）。用失真的数据训练可以增强模型的鲁棒性。<br>用对象存储训练数据简单， 开销低，但是读数据的带宽远低于训练的吞吐率。有人为深度学习的栈单独增加文件系统以解决这个问题，把单个读请求转换为多个并发的请求，一次申请得到更大的总带宽。</p></li><li><p><strong>模型数据</strong><br>  包括对已训练模型的跟踪，存储和索引。<br>增强深度学习框架之间的互操作性：ONNX在不同深度学习框架之间交换模型数据。ModelDB和ModelHub可以通过SQL或其他查询模型</p></li></ul><h3 id="不同深度学习框架的对比"><a href="#不同深度学习框架的对比" class="headerlink" title="不同深度学习框架的对比"></a>不同深度学习框架的对比</h3><p><strong>Caffe：</strong>Berkley AI research团队和社区共同开发。具有命令行、PyThon和MATLAB API。model zoo是一个与训练好的模型的集合。在GPU上通过CUDA平台（使用cudnn库）实现简单的并行。不支持直接的分布式训练，但是Caffe的一些分支和扩展支持分布式训练。在文档中有关如何自定义操作的内容较少。Caffe不支持多节点部署，也无法实现自定义的并行方法。<br>其他框架详见表格。</p><h3 id="总结和展望"><a href="#总结和展望" class="headerlink" title="总结和展望"></a>总结和展望</h3><p>随着训练数据和深度学习模型越来越多，对训练和模型数据的管理的挑战越来越大。为了深度学习扩展性中新的瓶颈和限制可以被消除，需要更好的工具支持。另外，目前去中心化训练的发展，可能会改变深度学习设备和工具的设计。<br>未来的发展趋势：1. 目前可扩展的深度学习主要集中在并行和分布式的方面，数据管理和多租户调度方面需要更多的关注。2. DL serving（用深度学习做inference）也受到越来越多的关注。尽管和训练类似，但是解决方案完全不同。3. 隐私问题也十分重要。</p><hr><p><strong>原文作者：</strong>Ruben Mayer, Hhans-Arno Jaconson<br><strong>原文链接：</strong><a href="https://www.researchgate.net/profile/Ruben_Mayer/publication/332033965_Scalable_Deep_Learning_on_Distributed_Infrastructures_Challenges_Techniques_and_Tools/links/5d90f42492851c33e9488837/Scalable-Deep-Learning-on-Distributed-Infrastructures-Challenges-Techniques-and-Tools.pdf">https://www.researchgate.net/profile/Ruben_Mayer/publication/332033965_Scalable_Deep_Learning_on_Distributed_Infrastructures_Challenges_Techniques_and_Tools/links/5d90f42492851c33e9488837/Scalable-Deep-Learning-on-Distributed-Infrastructures-Challenges-Techniques-and-Tools.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习系统 </tag>
            
            <tag> 调研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>更多维度的深度神经网络并行策略</title>
      <link href="2020/07/20/FlexFlow/"/>
      <url>2020/07/20/FlexFlow/</url>
      
        <content type="html"><![CDATA[<h1 id="Beyond-Data-and-Model-Parallelism-for-Deep-Neural-Networks"><a href="#Beyond-Data-and-Model-Parallelism-for-Deep-Neural-Networks" class="headerlink" title="Beyond Data and Model Parallelism for Deep Neural Networks"></a>Beyond Data and Model Parallelism for Deep Neural Networks</h1><p>对训练深度神经网络的计算需求很高，分布式训练已经是常规操作。目前的深度学习系统通常用数据并行或模型并行。但是这些策略在并行程度上通常无法达到最优。这篇文章提出了一个复杂的深度神经网络的并行策略：SOAP。Sample，Operation，Attribute，Parameter。提出了FlexFlow：一个在SOAP维度为特定的并行机器随机搜索快速并行策略的深度学习框架。为了加速这个搜索的过程，FlexFlow用了一个创新性的、可以准确预测一个并行策略的表现、比原有方法更快速的执行模拟器。实验结果表明FlexFlow可以很大程度上增大训练的吞吐率。本文由斯坦福大学团队发表于MLSys 2019。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>模型越来越复杂，训练集越来越大，训练模型的计算需求也越来越大。因此分布式训练成为了常规操作。深度学习框架中对并行的应用很简单，最常用的是数据并行。数据并行对计算密集、参数较少的DNN很有效。另外一种常见的是模型并行。已有工作对池化卷积层用数据并行，对全联接层用模型并行。（利用experts’ knowledge）。但是仍然不是最优的。有工作做出了自动化并行策略。已有的自动化的框架只探索不同op之间的并行或者单个op的并行。其实两者结合可以得到一个更快的策略。</p><p>这篇文章提出FlexFlow，一个可以自动在更大的范围内找出快速并行策略的深度学习框架。为了形式化这一问题，我们首先定义了SOAP。Operation维度描述了一个DNN中不同的operation是如何并行的。另外，对于一个单独的DNN operation来说，sample和parameter维度描述训练样例和模型参数如何在不同设备之间分布。最终，attribute维度定义一个sample中不同的attribute是如何划分的。已有的系统都是在SOAP的子集中划分的。<br>在SOAP这个更大的范围内搜索的一个主要的挑战是快速评估候选的并行方案已找到一个高效的方案。已有的工作依赖于在硬件上执行一轮训练来评估不同方案的执行时间。在SOAP的范围内，这样的方法代价太高。</p><p>为了解决这样的问题，FlexFlow提出了一个创新性的执行模拟器，可以准确预测并行策略的表现，比profile真实的运行快了三个数量级。设计模拟器的挑战在于如何准确估计不同DNN op的执行时间（非线性，取决于硬件）。模拟器依赖于两个事实：（1）很多DNN模型只用少数几个不同的op（2）op的执行时间通常差异不大，很大程度上取决于输入数据。FlexFlow的模拟器对于每种数据大小，用一个op的计算时间来衡量同种类op的计算时间。然后，这些估算被用于预测各种各样的并行策略。另外，模拟器使用了一种delta simulate算法，这种算法基于对之前的模拟的更新对新的策略作出模拟。何以有的方法比，这样的方法有两个优势：更快、所需资源更少。<br>模拟器的预测准确率很高。FlexFlow的execution optimizer使用一种马尔可夫蒙特卡洛搜索算法探索SOAP的搜索空间，并给予对之前的候选策略的模拟表现选出候选策略。搜索过程结束后，optimizer返回最佳的策略。</p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><h3 id="程序接口"><a href="#程序接口" class="headerlink" title="程序接口"></a>程序接口</h3><p>与多数深度学习框架不同，FlexFlow用设备拓扑结构描述所有可用的应尽设备和他们之间的关联。拓扑结构中的“边”有带宽和延迟的标签。<br>FlexFlow可以自动为一个计算图和一个设备拓扑结构找到合适的并行策略。主要有两大优势：提供了易于编程的接口；可移植性（为不同的硬件自动选择搞笑的策略）。</p><h3 id="FlexFlow架构"><a href="#FlexFlow架构" class="headerlink" title="FlexFlow架构"></a>FlexFlow架构</h3><p>Execution optimizer为计算图和设备拓扑图选择高效的并行策略。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/FlexFlow/4.png" width="45%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：FlexFlow Overview</div></center><h3 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h3><p>执行模拟器假设每一个op的执行时间是可预测的且与input tensor的内容无关。所以该方法不适合执行时间data  dependent的模型。</p><h4 id="SOAP搜索空间"><a href="#SOAP搜索空间" class="headerlink" title="SOAP搜索空间"></a>SOAP搜索空间</h4><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/FlexFlow/1.png" width="55%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2：1D卷积的并行策略样例</div></center></br><p>对于一个op o_i， 用Pi表示它可以被分割的维度。Pi总会包括sample维度。如果一次分割需要把模型中的不同参数分割开，则就被称为parameter维度；否则，被称为attribute维度（例如在卷积中，把二维图片按照长宽分开）。o_i分割的config c_i定义了这个o_i在不同的设备之间是如何并行的。对于P_i中每一个可并行的维度，c_i包括一个正整数表示那个维度并行的“程度”。|c_i|是ci_中所有可并行维度的degree的乘积。我们对每一个维度采用同样大小的分割来保证分布式负载的良好的平衡。ci把oi分成|ci|个独立的任务，分别用ti:k表示。</p></br><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/FlexFlow/2.png" width="100%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图3：模拟并行策略的样例</div></center><br>策略S描述了一个application的可能的并行方案。S包括对于每一个oi的ci。<h2 id="执行模拟器-Execution-Simulator"><a href="#执行模拟器-Execution-Simulator" class="headerlink" title="执行模拟器 Execution Simulator"></a>执行模拟器 Execution Simulator</h2><p>输入：算子计算图G，设备拓扑结构D，并行策略S<br>输出：执行时间<br>simulator的重要假设：（1）每个task的执行时间都是可预测的，波动小，与input tensor的内容无关。（2）不同设备之间传输数据的时间为 数据大小/带宽。（3）每个设备按照FIFO的顺序执行任务（GPU就是这样的）。（4）每个设备在完成一个任务后，只要下一个任务的数据准备就绪就立刻开始执行下一个任务，overhead可忽略不计。<br>为了模拟一次执行，模拟器首先建立一个task graph，然后运行模拟算法。</p><h3 id="Task-Graph"><a href="#Task-Graph" class="headerlink" title="Task Graph"></a>Task Graph</h3><p>把设备之间的物理连接视为communication device，每一次数据传输被视为communication task。T = (T_N , T_E )。</p><h3 id="全模拟算法"><a href="#全模拟算法" class="headerlink" title="全模拟算法"></a>全模拟算法</h3><p>是后续delta模拟算法的baseline。<br>首先懂Dijkstra算法遍历。所有任务都被放到一个队列里，出队列的顺序是按照ready time的增序。该算法最终返回所有任务重最慢的一个执行完所需时间。</p><h3 id="delta模拟算法"><a href="#delta模拟算法" class="headerlink" title="delta模拟算法"></a>delta模拟算法</h3><p>使用一种MCMC搜索算法，每次只改变一个op的划分方式。这种情况下，前后两个策略的时间通常没有改变。delta simulation算法只重新模拟改变最终结果了的。对于同样的task graph，full和delta的模拟算法会给出同样的结果。</p><h2 id="执行优化器-Execution-Optimizer"><a href="#执行优化器-Execution-Optimizer" class="headerlink" title="执行优化器 Execution Optimizer"></a>执行优化器 Execution Optimizer</h2><p>输入：算子计算图G，设备拓扑结构D<br>输出：最有效的并行策略<br>问题抽象为最小化总执行时间。这个方法避免了平衡执行时间和通信时间二者的问题。<br>这是一个NP难的问题，但有方法可以简化。可能的策略数量是op数量的指数，所以不可能穷尽整个搜索空间。为了找到一个开销低的策略，采用开销最小化搜索。</p><h3 id="MCMC采样"><a href="#MCMC采样" class="headerlink" title="MCMC采样"></a>MCMC采样</h3><p>MCMC是从概率分布中获取样本的一个采样方法。公式1：把cost function转化为概率分布的常见方法。<br>MCMC开始与整个搜索空间的任意一点。然后产生一系列的点，这一系列的点达到 p(·)给出的分布。<br>使用某种算法生成马尔可夫链，维护一个当前的策略S和一个修改后的策略S<em>。若S</em>被接受，则S<em>替换S。这一过程无限重复，直到某一规定的时间被耗尽。公式2：S</em>被接受的标准（可能性？）。<br>若S<em>比S的cost更高，则S</em>也有可能被接受（？）。MCMC趋向于贪心搜索，更倾向于选择cost小的策略。</p><h3 id="搜索算法"><a href="#搜索算法" class="headerlink" title="搜索算法"></a>搜索算法</h3><p>在当前的策略中随机选一个op o_i，把ci替换成一个随机的config。<br>用已有策略/随机生成的策略作为candidate。对于每一个初始的策略，如果满足下面两个条件之一，搜索算法就会提出新的candidate：（1）当前处世策略的搜索时间budget被耗尽；（2）一般的搜索时间里都不能找出更好的策略。</p><h2 id="运行时环境"><a href="#运行时环境" class="headerlink" title="运行时环境"></a>运行时环境</h2><p>建立在Legion上。现有的框架基本上只支持数据并行。</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><ul><li>并行表现<ul><li>FlexFlow在每轮训练的吞吐率上表现优秀。</li><li>FlexFlow支持通信和计算的overlap以减少通信overhead。</li><li>比较不同的自动寻找并行策略的方法。</li></ul></li><li>评估模拟器：准确率+执行时间两个matrix。</li><li>搜索算法</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/FlexFlow/3.png" width="95%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图4：在4个P100GPU上训练Inception-v3的最佳策略</div></center><hr><p><strong>原文作者：</strong>Zhihao Jia, Matei Zaharia, Alex Aiken<br><strong>原文链接：</strong><a href="https://cs.stanford.edu/~zhihao/papers/sysml19a.pdf">https://cs.stanford.edu/~zhihao/papers/sysml19a.pdf</a><br><strong>项目代码：</strong><a href="https://github.com/flexflow/FlexFlow">https://github.com/flexflow/FlexFlow</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 分布式训练 </tag>
            
            <tag> 机器学习系统 </tag>
            
            <tag> MLSys </tag>
            
            <tag> 弹性训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lineage Stash：在关键路径之外的容错机制</title>
      <link href="2020/02/13/Lineage-Stash/"/>
      <url>2020/02/13/Lineage-Stash/</url>
      
        <content type="html"><![CDATA[<h1 id="Lineage-Stash-Fault-Tolerance-Off-the-Critical-Path"><a href="#Lineage-Stash-Fault-Tolerance-Off-the-Critical-Path" class="headerlink" title="Lineage Stash: Fault Tolerance Off the Critical Path"></a>Lineage Stash: Fault Tolerance Off the Critical Path</h1><p>目前主流的分布式计算框架通过全局快照和日志这两种方式之一来实现容错。全局快照的运行时开销小，因为是异步的，但是恢复很慢，因为需要把上一个快照之后的操作回滚再重来。使用日志的话，因为粒度比较细，所以恢复的时候比较快，只要跟着日志走就行了，然而平时的计算都需要等日志写完才能进行，也就是在关键路径上，所以运行时的开销很大。这篇文章提出了 lineage stash 的方法，能够减少日志方法的运行时开销，但又能实现高效的恢复。</p><p>大体上他们的方法是在把任务交给另一个节点处理的时候，顺便带上当前节点的日志，同时异步地把日志写到一个全局的存储上面。这样一来，如果有一个节点挂了，其他节点可以把这个节点的日志写到全局存储上面，这样这个节点重新上线开始恢复的时候就可以完整地得到之前自己的日志，而且在平时执行任务的时候也不用等到日志写到存储上面。</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><ul><li><p><strong>Stage：</strong>以shuffle为界，当在一个job任务中涉及shuffle操作时，会进行stage划分，产生一个或多个stage。partition个数即rdd的分区数，不同的数据源读进来的数据分区数默认不同，可以通过repartition进行重分区操作。</p></li><li><p><strong>目前常见框架应用的两种容错机制：</strong></p><ul><li><strong>global checkpointing，是基于logging的方法。</strong>该方法在运行时必须不断记录，造成运行时开销大。但是这样的记录可以减少recovery时replay的开销。</li><li><strong>基于lineage的方法</strong>，记录每一个stage的lineage（计算图）。出现failure时数据重构的消耗很大，但是在运行时开销小。</li></ul></li><li><p>本文提出的<strong>Lineage Stash方法：</strong>一个在运行时和恢复时开销都很小的causal logging方法，可用于细粒度data processing应用。</p></li></ul><h2 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h2><ul><li>目前记录lineage以batch为粒度，而传统的lineage-based方法以partition为粒度（包含多个batches），现在记录的力度小，记录量大，严重影响了延迟和吞吐量（为什么要设计一个更细粒度的记录呢？？）</li><li>lineage一定要在运行时更新，以保证后续恢复和之前的记录处理相同</li><li>设计刷新和恢复lineage的简单、稳定的方案</li></ul><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Lineage stash保证，如果一个处理过程fail了，从上一个checkpoint起，它收到的任何消息（message）会被以同样的顺序replayed。<br>lineage stash采用casual logging的方法做record和relay计算。casual logging会异步地向一个稳定的存储系统里不断地记录“不确定的事件”，在发生failure时，因为所有的“不确定事件”都被记录了并可以replay，所以这保证了global consistency。casual logging在实际应用中会遇到困难：不确定的事件种类繁多，且记录一定要全，这导致任务繁重且开销过大。好在通常data processing的应用中主要都是纯计算。主要的问题在于在data processing application中如何鉴别出并捕捉不确定性的源头。有三个要回答的问题：记录什么信息、如何记录信息、如何从这些记录中恢复。</p><h3 id="记录什么信息"><a href="#记录什么信息" class="headerlink" title="记录什么信息"></a>记录什么信息</h3><p>减少不必要的记录，例如无需重复记录相同的执行顺序；无需记录完全不确定的执行顺序等。<br>对于完全确定性的计算，可以记录计算图（lineage）而非原始数据。特别地，我们记录一个指向应用数据的指针（object）和对计算的简单描述（task）。一个object的数据图包括产生这个object的task，以及这个task的每一个参数的数据图。<br>对于不确定性的计算，就有必要记录计算顺序了。具体情况具体分析，记录的信息随机应变。</p><h3 id="如何记录信息"><a href="#如何记录信息" class="headerlink" title="如何记录信息"></a>如何记录信息</h3><p>传统方法：在task执行之前记录计算图的依赖关系<br>Lineage stash的方法：向下面的节点传递task输入数据的计算图。这样，执行任务的节点有所有重构输入的信息。<br>确定性的计算图的执行顺序无需被传递，因其确定性，这些计算图可以被准确无误地重建。每一个进程只需要记录它曾传递过的task。如果一个进程fail了，向这个进程传递过task的进程将重新传递tasks以帮助恢复进程状态。<br>不确定性的应用中，除了task以外，进程还需要向前传递它所见过的所有的计算图，因为task描述随着执行过程的进行会改变，仅仅传递task描述是不足够的。除了local stash以外，还有一个global linage storage。</p><h3 id="如何从记录中恢复"><a href="#如何从记录中恢复" class="headerlink" title="如何从记录中恢复"></a>如何从记录中恢复</h3><p>提取并重新执行计算图。</p><p>For each process p and task T that p has executed or submitted, T ’s lineage is in p’s local stash.<br>For each process p and task T that p has executed or submitted, T ’s uncommitted lineage is in p’s local stash.</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/Lineage-Stash/1.png" width="60%">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1：非确定性进程的恢复过程</div></center><br><p>Latency：和WriteFirst方法相比较，对于确定性的计算图，Lineage stash的方法latency更小；对于不确定的计算图，lineage stash的方法latency更大，如果限制该方法中lineage可向前传递的次数，latency变得稳定。</p><p>应用：在分布式训练，数据流处理等方面都有具有优势（发生Failure时恢复得更快）。</p><hr><p><strong>原文作者：</strong>Stephanie Wang, John Liagouris, Robert Nishihara, Philipp Moritz, Ujval Misra, Alexey Tumanov, Ion Stoica<br><strong>原文链接：</strong><a href="https://dl.acm.org/doi/pdf/10.1145/3341301.3359653">https://dl.acm.org/doi/pdf/10.1145/3341301.3359653</a><br><strong>参考文献：</strong>[1] SOSP 2019 有哪些值得关注的论文？ <a href="https://www.zhihu.com/question/336446443/answer/872624113">https://www.zhihu.com/question/336446443/answer/872624113</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> SOSP </tag>
            
            <tag> 容错 </tag>
            
            <tag> 分布式系统 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
